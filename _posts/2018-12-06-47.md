---
layout: post
title:  "BERT: Bidirectional Transformers for Language Understanding"
date: 2018-12-06 13:47:35 +0900
categories: NLP
tag: NLP
---


이번에는 많은 Task 에서 SotA(State of the Art)의 성능을 보이고 있는 BERT(**B**ert **E**ncoder **R**epresentations form **T**ransformers)에 대해서 알아보도록 하자. 이전에 소개된 ELMo, GPT에 이어 Pre-trained을 함으로써 성능을 올릴 수 있도록 만든 모델이다. 해당 모델은 Google에서 제시한 모델로 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) 논문에서 소개되었다. 이제 논문을 살펴보자.

****

### 1 Introduction

많은 Task 에서 Language model pre-training은 효과적이라는 것을 계속해서 입증해오고 있다. [**ELMo**](https://arxiv.org/pdf/1802.05365.pdf), OpenAI의 [**Generative Pre-trained Transformer(GPT)**](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 등 다양한 모델에서 pre-train을 함으로써 대부분의 NLP의 task의 성능이 향상됨을 보여왔다.

우선 여기에는 pre-train 된 언어 표현을 실제 task에 적용시키는 방법은 두 가지로 나뉜다. 하나는 *feature-base* 방법이고, 하나는 *fine-tuning* 방법이다.

우선 feature-based 방법의 경우 예시는 ELMo이며, 이 경우에는 ELMo를 통해 사전 학습한 feature를 사용하지만, task에 적합한 새로운 architecture를 추가로 사용해서 task를 해결한다. 즉 사전 학습을 통해 언어 표현 feature만 사용하고, 모델의 경우 별개의 개념인 것이다.

그와 다르게 fine-tuning 방법의 경우 예시는 OpenAI의 GPT이다. 이 방법의 경우 task에 맞게 새로운 task-specific한 parameter를 사용한다. 이 parameter를 각 task에 학습하는데, 이 때 사전 학습한 parameter를 사용해 fine-tuning 하는 방법이다.

이러한 두 방법 모두 같은 목적 함수를 사용해 사전 학습하지만, 두 방법 모두 단방향의 language model을 사용해서 언어 표현을 학습한다.

해당 논문에서는 기존의 이러한 기법들(ELMo, GPT)이 pre-train된 표현의 제대로된 성능을 저해하는 요소가 있다고 주장한다. 이러한 요소에 대해서 설명하면, 먼저 이러한 langugage model 이 단방향으로 학습한다는 점이다. 그리고 이러한 이유로 pre-train 과정에서 사용될 architecture를 선택할 수 있는 폭을 줄어든다는 점이다.

예를 들어 OpenAI의 GPT의 경우 텍스트에서 왼쪽에서 오른쪽으로만 참고할 수 있는 구조를 취하고 있다.

따라서 해당 논문에서는 fine-tuning based 방법으로 *BERT: Bidirectional Encoder Representations from Transformers* 를 사용함으로써 성능을 향상시킬 것이다. 해당 방법의 경우 기존의 방법이 단방향이였다면, "**masked language model(MLM)**"를 통해 사전학습을 함으로써 이러한 제약을 해결할 것이다.

Masked language model은 임의로 input token의 몇가지를 masking 처리하고, 다른 token들을 사용해서 masking된 token들을 예측하는 방향으로 학습한다. 기존의 왼쪽에서 오른쪽으로 단방향으로만 사전 학습하는 모델과는 다르게 MLM의 목적은 좌,우의 모든 문맥이 융화되도록 하는 것이다.

그리고 Masked language model과 더불어 "**next sentence prediction**" 라는 task를 사전 학습 과정에서 같이 사용할 것이다.

마지막으로 해당 논문이 기여하려고 하는 바는 다음과 같다.

* 해당 논문에서는 언어 표현을 위해 양방향으로 사전 학습하는 것의 중요성을 증명할 것이다. OpenAI의 GPT 처럼 단방향으로 사전 학습을 하지 않고 BERT는 masked langugage model을 사용해 언어 표현을 위한 양방향으로 사전 학습을 진행한다. 그리고 ELMo의 경우 좌에서 우, 우에서 좌 둘다 사용해서 양방향이라 표현할 수 있지만, 해당 논문에서는 좌에서 우, 우에서 좌로 각각 학습한 후 이를 concatenate 해서 사용했다는 점이 BERT와의 차이점이다.

* 또한 많은 engineering이 필요한 task-spesific한 architecture를 사용할 필요성을 사전 학습을 통해 줄여준다. BERT는 문장 단위와 단어 단위 모두에서 SotA의 성능을 보이는 최초의 fine-tuning 기반의 표현(representation) 모델이다.

* BERT를 사용함으로써 11개의 NLP task들에서 SotA의 성능을 향상시켰다. 그리고 BERT를 통해 양방향 특성이 가장 중요한 점이라는 것을 보여준다. 그리고 code와 pre-trained 모델의 경우 [goo.gl/language/bert](https://github.com/google-research/bert)에서 사용할 수 있다.


### 2 Related Work

먼저 사전 학습 모델의 역사에 대해서 간략하게 알아보도록 하자.

##### 2.1 Feature-based Approaches

우선 Feature based 한 Pre-trained 기법들을 Neural이 아닌 모델들도 많이 있었다. 그리고 Pre-trained 시 단순히 단어에 대한 임베딩만을 진행하는 것이 아니라, 문장 및 구절에 대해서도 임베딩 하기도 한다.

그 중에서도 ELMo는 이러한 전형적인 단어 임베딩 연구를 다른 차원으로 일반화 시켰다. ELMo 모델에서는 context-sensitive 특징을 모델을 통해 추출했다.

##### 2.2 Fine-tuning Approaches

최근의 Langugage model에서 tansfer learning의 트렌드는 실제 task에 맞는 지도 학습 모델을 적용하기 전에 동일한 모델을 사전 학습한 후 학습한 결과를 사용해서 fine-tuning하는 방법이다. 이러한 방법의 장점은 학습해야 하는 parameter의 수가 적다는 점이다. 더 적은 수의 parameter를 학습하면 된다.

##### 2.3 Transfer Learning from Supervised Data

비지도 학습의 사전 학습 방법은 무한정의 데이터를 사용할 수 있다는 장점이 있지만, 큰 데이터에 대해서 지도 학습 방법의 transfer가 더욱 효과적이라는 것이 입증되었다.

### 3 BERT

이제 본격적인 BERT의 세부적인 구현 내용에 대해서 알아보도록 하자. 순서는 우선 전체 모델의 architecture에 대해서 알아보고 input의 형태를 먼저 알아본다. 그리고 다음으로 pre-training task에 대해서 알아본 뒤 pre-training, fine-tuning의 세부적인 절차에 대해서 알아본다. 마지막으로 BERT와 OpenAI GPT의 차이점에 대해서 다뤄보도록 한다.

##### 3.1 Model Architecture

BERT의 모델 구조는 multi layer의 양방향 Transformer 인코더를 기반으로한 구조이다. Transformer에 대한 자세한 설명은 생략하도록 한다.([블로그 글 참고](https://reniew.github.io/43))

해당 모델의 트랜스 포머의 layer의 수(Transformer 블록의 수)는 $L$로 지칭하고, hidden size는 $H$로 지칭한다. 그리고 self-attention의 head 수는 $A$로 지칭한다. 마지막으로 Transformer의 feed-forward layer의 첫번째 layer의 unit 수는 $4H$로 지정한다.

해당 모델에서는 2개의 각각 다른 하이퍼 파라미터를 가지는 모델을 정의했다. 각 모델의 하이퍼 파라미터 값은 다음과 같다.

* $\text{BERT}_\text{BASE}$: $L=12, H=768, A=12, \text{Total Parameters} = 110M$
* $\text{BERT}_\text{LARGE}$: $L=24, H=1024, A=16, \text{Total Parameters} = 340M$

$\text{BERT}_\text{BASE}$는 비교를 위해 OpenAI의 GPT 모델과 같은 크기로 설정했다. BERT와 OpenAI의 GPT와 ELMo의 모델의 차이점은 아래의 그림을 참고하자.

![BERT](https://i.imgur.com/Rs2cg2I.jpg)

##### 3.2 Input Representation

이제 입력값의 형태에 대해서 알아보도록 하자. 여기서 입력값의 형태는 약간 모호할 수 있다. 하나의 문장이 입력값이 될 수도 있고 두개의 문장 쌍(e.g. [Question, Answer])이 입력값이 될 수도 있다. 주어진 token에 대해서 다음의 세가지 값을 더함으로써 입력값으로 사용한다.

* Token의 Embedding
* Segment의 Embedding
* Position의 Embedding

위의 세가지 값을 더하면 입력값이 된다. 입력값에 대한 부분은 아래의 그림을 참고하자.

![INpu](https://i.imgur.com/5Uvo5tl.jpg)

입력값에 대한 구체적인 특징들은 다음과 같다.

* 30,000개의 token vcabulary를 가지는 WordPiece 임베딩 값([Wu et al., 2016](https://arxiv.org/pdf/1609.08144.pdf))을 사용했다. 그리고 split word의 경우 ##으로 사용했다.

* 그리고 Positional 임베딩 값으로는 512 토큰 길이에 맞게 학습된 임베딩 값을 사용했다.

* 모든 문장의 첫 토큰은 special classification embedding값([CLS])을 넣어준다. 해당 토근에 대응하는 마지막 hidden state(Transformer의 출력값)는 분류 task에서 사용된다. 만약 분류 task가 아니라면 해당 벡터는 무시한다.

* 문장 쌍은 하나로 묶여서 하나의 문장으로 만들어지는데, 실제로 다른 이 문장 쌍을 두가지 방법으로 구별한다. 첫 번쨰는 두 문장 사이에 special token([SEP])를 넣어주고, 두 번째 방법은 학습된 문장 A 임베딩 값을 앞쪽 문장 모든 token에 더해주고 문장 B 임베딩 값을 뒤쪽 문장 모든 token에 더해준다.

* 단일 문장 입력값의 경우 문장 A 임베딩 값만을 사용한다.

##### 3.3 Pre-training Tasks

해당 모델에서는 전형적인 좌에서 우 혹은 우에서 좌로 가는 language model을 사용해서 BERT를 pre-train하지 않았다. 대신 BERT는 두개의 비지도 예측 task들을 통해 pre-train 했다. 이 Section에서 두개의 비지도 학습 task에 대해서 알아보도록 하자.

**3.3.1 Task #1: Masked LM**

직관적으로 양방향 모델이 기존의 좌->우 / 우->좌 모델보다 훨씬 더 좋다는 것은 합당하다. 하지만 불행이도 양방향 조건은 간접적으로 각 단어를 본인 스스로를 보도록 하기 때문에, 일반적인 조건부 language model은 오직 좌에서 우, 우에서 좌로만 학습할 수 있다.
따라서 양방향 학습을 위해서 여기서는 특정 확률만큼에 해당하는 입력 토큰들을 임의로 마스킹 처리한다. 그리고 마스킹 된 단어들을 예측하도록 한다. 이러한 기법을 "masked LM"이라 부른다.(MLM) denoising auto-encoders와는 대조적으로 여기서는 오직 masking 한 값만 비교를 해서 학습을 진행한다.

이러한 MLM 기법을 통해서 양방향 학습이 가능하도록 했지만, 이러한 기법의 큰 두가지 문제점이 남아있다. 첫 번째는 pre-train 과정에서는 "[MASK]" 토큰을 사용하지만, fine-tuning 한 후에는 사용하지 않아서 pre-train과 fine-tuning 사이의 간극이 생긴다는 점이다. 이러한 문제점을 해결하기 위해 학습 과정에서 모든 masking된 값을 [MASK] 토큰으로 대체하지 않았다. 자세하게 이러한 문제점을 해결하는 방법에 대해서 알아보도록 하자. 예를 들어 *"내 개는 크다"* 라는 문장이 있다고 하자. 이 때 *"크다"* 라는 단어를 masking 하려고 한다면 다음과 같이 진행된다.

* 항상 *"크다"* 라는 단어를 [MASK] 토큰으로 대체하는 것이 아니라 다음과 같이 진행한다.
* 전체 시간의 80%: *"크다"* 라는 단어를 "[MASK]" 토큰으로 대체한다. 즉 *"내 개는 크다"* $\rightarrow$ *"내 개는 [MASK]"*
* 전체 시간의 10%: *"크다"* 라는 단어를 임의의 다른 단어로 대체한다. 즉 *"내 개는 크다"* $\rightarrow$ *"내 개는 사과"*
* 전체 시간의 10%: Masking 된 단어, *"크다"* 를 그대로 둔다. 즉 *"내 개는 크다"* $\rightarrow$ *"내 개는 크다"*

위와 같이 masking 함으로써 pre-train과 fine-tuning 과의 간극을 줄여서 첫 번째 문제점을 해결한다. 두 번째 MLM의 문제점은 오직 15%의 token 만 각 batch 에서 예측된다는 점인데, 따라서 pre-train 값이 수렴하기 위해서는 더 많은 step을 학습해야 한다.

 ![step](https://i.imgur.com/4k6wu1i.jpg)

위 그림을 보면 MLM 기법을 사용한 BERT 모델이 단방향의 BERT 모델 보다 더 많은 step이 지나야 수렴한다는 것을 확인할 수 있다.

**3.3.2 Task #2: Next Sentence Prediction**

Question Answering, Natural Language Inference 등의 Task들은 두 문장 사이의 관계를 이해하는 것이 매우 중요하다. 문장 사이의 관계를 모델이 학습할 수 있도록 단일 Corpus로 구성된 두 개의 문장에 대해 문장이 관계있는지 없는지 이진 분류하는 next sentence prediction task를 통해 pre-train 한다. 구체적으로 두개의 A, B 문장을 선택하는데, 학습 과정에서 절반은 B 를 실제 A의 다음 문장으로 선택하고, 나머지는 임의의 다른 문장을 선택한다. 아래의 예를 보자.


$$
\begin{align*}
\text{Input} = &\text{[CLS] the man went to [MASK] store [SEP]}\\
&\text{he bought a gallon [MASK] milk [SEP]}\\
\text{Label} =&\text{IsNext}
\end{align*}
$$

$$
\begin{align*}
\text{Input} = &\text{[CLS] the man [MASK] to the store [SEP]}\\
&\text{penguin [MASK] are flight ##less birds [SEP]}\\
\text{Label} =&\text{NotNext}
\end{align*}
$$

NotNext 문장의 경우 완전히 임의로 선택했다. 해당 테스크에 대해서 최종적으로 pre-train된 모델의 경우 97~98%의 정확도를 보여준다. 이러한 task는 QA 와 NLI task에 대해서 아래와 같은 성능 향상을 보여줬다.

![nli](https://i.imgur.com/ukCeaMm.jpg)

##### 3.4 Pre-training Procedure

pre-traning Corpus를 위해 두개의 Corpus를 합쳐서 사용했다. BooksCorpus(800M words)와 Engish Wikipedia(2,500M words)를 합쳐 하나의 Corpus로 만들었다. Wikipedia corpus의 경우 list, tables, header를 모두 무시하고 text만을 추출했다.

각 학습 입력값을 만들어주기 위해서 두개의 문장을 뽑아서 하나의 문장으로 만들어 준다. 첫 번쨰 문장의 경우 A 임베딩 값을 가지고 두 번째 문장의 경우 B 임베딩 값을 가진다. B의 50%는 실제 A의 다음 문장을 사용하고 나머지는 임의의 문장을 사용한다. 이렇게 뽑아 "next sentence prediction" task에 사용한다. 그리고 두 문장을 합친 문장의 최대 길이는 512 토큰으로 제한한다.

학습 시 batch size의 경우 256 문장으로 한다. 따라서 한 batch에 총 256 sequence $\times$ 512 tokens 인 128,000개의 토큰이 학습된다. 또한 전체 step은 1,000,000번 으로 33억개의 단어 corpus에 대해서 대략 40 에폭정도 학습한다.

또한 학습은 Adam 을 사용하였으며 하이퍼 파라미터 값은 다음과 같다.

* Learning rate: $10^{-4}$
* $\beta_1 = 0.9$
* $\beta_1 = 0.999$
* 0.01 값으로 L2 가중치 감소 사용
* Learning rate의 경우 10,000 스탭마다 선형으로 감소하도록 함

그리고 드랍아웃의 경우 0.1 확률로 모든 레이어에 적용했다. 활성화 함수로는 gelu를 사용했다. 학습 loss의 경우 mean masked LM likelihood and mean next sentence prediction likelihood를 더한 값을 사용했다.

##### 3.5 Fine-tuning Procedure

문장의 classification task는 BERT의 fine-tuning이 직관적이고 간단한다. 입력값에 대해서 고정된 길이의 벡터를 추출해야 하는데, 해당 모델에서 첫 번째 입력값인 [CLS] 토큰에 의해 출력되는 마지막 hidden state값을 사용한다. 이 벡터를 $C\in\mathbb{R}^H$라 부른다. fine-tuning 과정에서 전체 파라미터는 그대로 유지되지만 마지막 classification을 위해 classification layer $W\in\mathbb{R}^{K\times H}$를 추가한다. 여기서 $K$는 classification 해야 되는 라벨의 개수를 의미한다. 해당 layer를 거친 벡터에 대해서 소프트맥스 함수를 적용시켜 확률 벡터인 $P\in\mathbb{R}^K$를 계산한다. 즉 아래의 수식과 같이 최종 확률 벡터가 계산된다.

$$
P = \text{softmax}(CW^T)
$$

BERT의 모든 parameter와 $W$는 실제 라벨과 비교한 log-probability 를 최대화 하며 학습된다. 사실 이러한 최종적인 fine-tuning과정과 학습 방법은 task에 따라 조금은 달라지는데 아래의 그림을 참고하자.

![bert_cl](https://i.imgur.com/g7361fl.jpg)

fine-tuning 과정에서 대부분의 모델의 hyper parameters는 pre-train 과정과 동일하게 진행하지만, batch size, learning rate, epoch 값은 변경한다.

그리고 대부분의 최적의 hyper parameters는 task마다 매우 다르다.

### 4 Conclusion

최근의 language model 에서의 transfer leaning의 효과가 입증되었다. 그리고 비지도 학습의 pre-training 이 대부분의 language understanding의 한 부분으로 통합되었다.

해당 논문을 통한 가장 큰 기여는 양방향의 구조를 통해 pre-train 모델이 대부분의 NLP 모델에 성공적으로 적용된다는 점이다. 대부분의 결과가 매우 성공적이지만, 일부 사람의 성능보다 떨어지기도 하기 때문에 더 많은 연구를 통해 BERT에 의해 잡지 못하는 언어적 현상을 잡도록 해야 한다.
