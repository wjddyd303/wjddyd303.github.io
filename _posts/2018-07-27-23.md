---
layout: post
title:  "Glove : Global Vectors for Word Representation"
date:   2018-07-27 13:47:35 +0900
categories: NLP
tag: NLP
---

지난 포스트에서는 단어 임베딩 기술인 Word2Vec에 대해서 알아보았다. 이번에는 또 다른 단어 임베딩 기술 중 하나인 Glove에 대해서 알아 보도록한다. 이번 포스트는 요약이 아닌 논문의 흐름을 따라 설명한다. ([Paper](https://nlp.stanford.edu/pubs/glove.pdf))


#### Glove

GLove란 Global Vectors for Word Represnetation의 약자로 미국의 Stanford대학에서 2014년 개발한 기술로 Word2Vec과 마찬가지로 단어 임베딩 과정에서 많이 쓰이는 기법이다.

Glove에 대한 설명을 하기 전 기존의 단어 임베딩 기술에 대해서 먼저 보자. 기존의 단어 임베딩의 경우 크게 두 가지 범주로 나눌 수 있다.

* **Matrix Factorization Method**
* **Shallow Window-Based Method**


두가지 분류에 대해서 하나씩 알아보자. 먼저 Matrix Factorization Method는 LSA, HAL와 같은 것들이 있다.

>**LSA** 에 대해서 간단히 설명을 하자면 전체 문장에서 단어들의 동시 등장정보(공기정보, Co-occurrence)를 이용한다. 전체 문장에 있는 모든 단어에 대해 동시등장정보들을 matrix로 만들어 준다. 하지만 여기에 큰 문제점이 있다. matrix의 크기가 매우 크고 sparse하다는 문제점인데 이를 해결하기 위한 기법이 SVD(Singular Value Decomposition)이다. 이 기법을 이용해 matrix의 차원을 줄이고 dense하게 만들어준다. 이러한 내용이 LSA의 전체적인 내용이다. 자세한 내용을 설명하기엔 전체적인 맥락에 벗어나므로 다음 기회에 설명하도록 한다.

이러한 Matrix Factorization Method는 global한 statical information을 잘 잡아내고 학습이 빠르다는 큰 장점은 있지만 단어 유사도 외의 문제에는 적용하기 어렵고 새로운 단어에 추가시키려면 처음부터 다시 해야한다는 단점이 있다.

그리고Shallow Window-Based Method의 경우는 이전 포스트에서 알아봤던 Word2Vec의 CBOW모델과 Skip-Gram 모델, 그리고 NNLM, HLBL등의 모델들은 뜻한다. Word2Vec에 대한 자세한 설명은 이전 포스트를 참고하자([Word2Vec(1)](https://reniew.github.io/21/), [Word2Vec(2)](https://reniew.github.io/22/))

Shallow Window-Based Method은 우선 성능이 매우 좋다는 것이 가장 큰 장점이다. 그리고 복잡한 패턴을 dense한 vector로 만들면서 잘잡아낸다는 장점이 있지만, 통계적인 자료를 활용하는데 비효율적이다라는 단점이 있다. 위 내용들을 요약하면 다음과 같다.

Counte based | Shallow Window-Based Method
-------------|-------------
LSA, HAL|NNLM, SkipGram, CBOW
학습이 빠르다, 전체의 통계정보를 활용한다. | 성능이 좋다, 패턴을 잘 잡아낸다.
단어 유사도 외의 문제에 적용이 어렵다. | 전체적인 통계정보를 활용하기 어렵다.

Glove는 자신의 모델을 설명할 때 위와 같은 기존의 단어 임베딩 기술들에 대한 문제점들을 해결하려 했다고 합니다. 대표적으로 Word2Vec과 LSA에 대한 단점들에 대해 얘기합니다. 해당 글을 소개하면 다음과 같습니다.

> While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure. Methods like skip-gram may do better on the analogy ask, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts.

즉 이전에 설명한 것과 같은 내용입니다. LSA의 경우 단어 분석 문제에는 적합하지 않고, Skip-gram과 같은 방법의 경우는 분석에는 적합하지만 global한 co-occurrence(동시등장) 정보와 같은 통계치를 활용하기에는 적합하지 않다는 것입니다.

따라서 Glove는 이러한 문제점들을 해결하는데 초점을 맞췄습니다. 먼저 동시 등장 정보를 활용하기 위해 Co-occurrence Matrix를 정의합니다.

#### Co-occurrence Matrix

Co-occurrence Matrix는 $X$라 부르고 행렬의 원소 $X_{ij}$는 단어 $i$의 context안에서 단어$j$가 등장한 횟수로 정의한다.

그리고 $X_i$는 단어 $i$의 context안에 등장한 단어들의 총 수라 하자. 수식은 다음과 같다.

$$
X_i=\sum_k X_{ik}
$$

이러한 행렬의 계산은 말뭉치가 커질 수록 계산량이 급증한다. 예를들어 10,000개의 말뭉치가 있다면 행렬은 총 1억개의 원소를 가지게 된다. 하지만 이 Matrix의 경우 여러번 계산하는 것이 아니라 모델 초반에 한번만 계산하면 된다는 점이 있다.

#### Objective function

확률 $P_{ij}$은 단어 $i$의 context안에서 단어 $j$가 등장할 확률이라 하고 다음과 같이 정의한다.

$$
P_{ij}=P(j|i)=\frac{X_{ij}}{X_j}
$$

정의한 확률에 대한 직관적인 이해를 위해 간단한 예를 들어 보자. 두 개의 별개의 단어를 정의하자. 단어 i를 'ice' 라고하고 단어 j를 'steam' 이라고 하자.

이제 두 단어 사이의 관계에 대해서 알아보려고 한다. 이 경우 우리는 다른 여러가지 단어 $k$ 들에 대한 동시 등장 확률(Co-occurrence Plobability, 위에서 정의한 $P$)의 비율을 확인하면 될 것이다.

먼저 ice에서는 관련되었지만 steam에는 관련없는 단어를 단어 $k$에 지정하자. 단어 $k$를 'solid'라고 한다. 이 경우 확률들의 비율인 $\frac{P_{ik}}{P_{jk}}$는 커질 것이다. 그리고 이번에는 ice에 관련 없고 steam에 관련있는 단어인 'gas'를 단어 $k$로 정하자. 이 경우 확률의 비율은 작아질 것이다. 만약 두 단어 모두에 연관있는 'water'의 경우나 두 단어 모두에 연관이 없는 'fashion'의 경우에는 비율이 1에 가깝게 결정될 것이다.
![glove](https://i.imgur.com/LKjp70Z.jpg)
이 결과를 통해 우리는 연관없는 단어를 구분하는 것이 쉽다는 것을 알 수 있다.
따라서 우리는 두 단어에 대한 직접적인 확률이 아니라, 다른 단어에 대한 각각의 확률에 대한 수식을 새우는 것을 목표로한다. 먼저 일반화된 모델을 제시한다.

$$
F(w_i,w_j,\tilde{w}_k)=\frac{P_{ik}}{P_{jk}}
$$

여기서 $w\in\mathbb{R}^d$는 단어 벡터이고 $\tilde{w}\in\mathbb{R}^d$는 separate context 단어 벡터이다.

위 수식 중 $F$는 확률에 대한 비율을 정보를 포함해야할 것이다. 그리고 그 정보는 단어 벡터 공간안의 정보여야할 것이므로 두 비율은 벡터들의 차이로 해석할 수 있을 것이다. 따라서 수식을 다음과 같이 수정한다.

$$
F(w_i-w_j,\tilde{w}_k)=\frac{P_{ik}}{P_{jk}}
$$

아직까지 정의한 식에서 애매한 부분이 있다. 함수의 인자가 되는 값들은 vector인데 수식의 우변에 있는 값들은 scalar값이다. 따라서 좌변의 인자값을 내적하는 것으로 수정한다.

$$
F((w_i-w_j)^T\tilde{w}_k)=\frac{P_{ik}}{P_{jk}}
$$

이제 동시 등장 행렬에 대해 중요한 것은 단어 $j$와 context 단어인 단어 $i$는 무작위로 선택되는 것이기 때문에 이 둘의 역활을 자유롭게 바꿀 수 있어야 한다. 따라서 아래의 조건을 만족해야 한다.

$$
w\leftrightarrow\tilde{w}
$$

이렇게 되면 행렬 $X$는 symetric 해야한다. 즉,

$$
X\leftrightarrow X^T
$$

최종적으로 마지막 모델은 각 단어에 라벨링하는 것에 따라 변하지 않아야 한다. 그러나 아직 위의 $F$함수는 그렇지 않기 떄문에 두 가지 조건을 만족하도록 해야한다. 첫 째로는 Homomorphism 조건이다. 즉, $F$가 Homomorphism해야 한다.

$$
\text{Homomorphism} ~F : (\mathbb{R},+)\rightarrow(\mathbb{R_{>0}},\times)
$$

위 조건을 만족시킨다면 함수 $F$는 다음의 수식을 만족시킨다.

$$
F((w_i-w_j)^T\tilde{w}_k)=\frac{F(w_i^T\tilde{w}_k)}{F(w_j^T\tilde{w}_k)}
$$

우변의 각 $F$는 원래 식과 같으므로 다음을 만족한다.

$$
F(w_i^T\tilde{w}_k)=P_{ik}=\frac{X_{ik}}{X_i}
$$

이제 함수 $F$를 정해야 한다. 위의 Homomorphism 조건을 만족시키는 함수들 중 exponential함수를 이용하면 된다. 따라서 $F=exp$로 정의한다. 이 경우 아래 수식을 만족할 것이다.

$$
\begin{align*}
&F(w_i^T\tilde{w}_k)=P_{ik}\\
&\Leftrightarrow exp(w_i^T\tilde{w}_k)=P_{ik}\\
&\Leftrightarrow w_i^T\tilde{w}_k=\log(P_{ik})=\log(X_{ik})-\log(X_i)
\end{align*}
$$

하지만 위의 마지막 수식을 보자 아직 $\log(X_i)$ 때문에 수식이 symmetric하지 않다. 하지만 위 식은 $k$에 대해 independent하기 떄문에 bias를 각각 더함으로써 이 문제를 해결할 수 있다.

$$
w_i^T\tilde{w}_k+b_i+\tilde{b}_k=\log(X_{ik})
$$

따라서 우리는 위의 식을 만족하도록 모델을 만들 것이다. 따라서 목적함수는 Least sqares regression 함수로한다.

$$
J=\sum_{i,j=1}^V(w_i^T\tilde{w}_j+b_i+\tilde{b}_j-\log{X_{ij}})^2
$$

아직 이 수식에는 큰 문제점이 있다. $X$ 행렬은 sparse한 매트릭스인데 log함수를 넣으면 행렬값이 0 이될경우 발산하기 때문에 계산이 불가능해진다. 따라서 행렬 $X$에 대해서 모든 원소에 1을 더해주면 이 문제는 해결할 수 있다.
하지만 이러한 해결방법은 또 다른 문제를 야기한다. 행렬 $X$의 경우 매우 sparse해서 보통 원래 원소값이 0인 경우가 75~95%정도 되는데 우리가 모든 원소에 1을 더함으로써 거의 안나오는 경우와 한번도 안나오는 경우를 같은 동시 등장 값을 가진다. 따라서 우리는 Weighting 함수를 추가한 목적함수를 만든다 Weighting 함수 $f$는 다음의 특징을 가지며, 수식은 아래와 같이 정의한다.

* $f(0)=0$ 이다. 행렬 $X_{ij}$값이 0이더라도 발산하지 않게끔 하기 위함이다.
* $f(x)$는 non-decreasing 해야한다. 드물게 등장하는 경우에 overweighted 되지 않게 하기 위함이다.
* $f(x)$는 $x$값이 클 경우 상대적으로 작은 값을 가져야 한다. 자주 등장하는 단어가 overweighted 되지 않도록 한다.

$$
f(x) =
\begin{cases}
(x/x_{\text{max}})^\alpha~~,\text{if}~x<x_{\text{max}}\\
1,~~otherwise
\end{cases}
$$

$x_{\text{max}}$와 \alpha값은 직접 지정해줘야 하는 hyperparameter이다. 논문에 따르면 $\alpha=3/4$와 $x_{\text{max}}=100$이 가장 좋은 결과를 만들었다고 한다.

이제 마지막으로 weighted least squares 함수를 정의하자.

$$
J=\sum_{i,j=1}^Vf(X_{ij})(w_i^T\tilde{w}_j+b_i+\tilde{b}_j-\log{X_{ij}})^2
$$

위의 목적함수를 학습시킨 후 나온 $w$벡터를 임베딩 벡터로 사용한다.

마지막으로 논문에서 여러 임베딩 기법들을 비교한 자료를 보며 포스트를 마치도록 한다.

![glove2](https://i.imgur.com/fkpbxsn.jpg)
