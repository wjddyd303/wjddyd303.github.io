---
layout: post
title:  "Seq2seq with attention: Neural Machine Translation by Jointly Learning to Align and Translate"
date:   2018-08-13 13:47:35 +0900
categories: NLP
tag: NLP
---

최근 NLP에서 많은 모델에서 사용되는 기술인 attention기법을 처음으로 도입한 논문인 [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)에 대해 알아보도록 한다. 최초로 attention을 도입한 이 논문은 Seqeunce to seqeunce with attetion으로도 유명하다.

정확히는 이 논문은 Seqeunce to sequence를 처음으로 도입한 조경현 교수님의 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(참고: [블로그 글](https://reniew.github.io/31/))에서의 문제점을 해결하기 위해 이어서 나온 논문이라 볼 수 있다.

이전 논문을 기억해보면 Encoder-Decoder구조는 RNN Encoder를 통해 가변길이의 sequence를 하나의 고정된 길이의 vector로 만든 후 다시 그 vector를 통해 가변길이의 output sequence로 만드는 구조였다. 하지만 여기서의 문제점은 중간의 하나의 vector로는 앞선 모든 sequence의 정보를 담기 어렵다는 것이였고, 따라서 이 논문에서의 attention 메커니즘을 통해 해결하려 한다.

이제 논문리뷰를 해보자.

---

### Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation

논문의 저자는 KyunHyun Cho, Yoshua Bengio, Dzmitry Bahdanau로 원 논문은 [링크](https://arxiv.org/pdf/1409.0473.pdf)를 참고하자.

#### Introduction

Neural machine translation(NMT)은 최근 기계번역 분야에서 부상된 분야이다. 기존의 통계적 기계 번역(SMT)와는 달리 NMT는 번역 성능을 올리기 위한 단일 신경망을 만드는 것을 목표로 한다. 최근 제안된 NMT 모델은 encoder-decoder 구조의 모델이다.

이 논문에서는 encoder-decoder 구조에서 중간의 fixed-size vector를 사용하는 것에 의해 병목현상이 일어날 것이라 추측했다. 실제로도 input sentence가 길어질수록 성능이 급격하게 떨어졌다. 따라서 이러한 문제를 해결하기 위해 이 구조를 확장시켜서 자동으로 문장에서 target 단어와 중요한 관계를 가지는 부분을 찾도록 할 것이다.

즉 align and translate를 같이 학습하는 encoder-decoder 모델을 만들 것이다. 매 time-step 마다 번역된 단어를 만들면서 source 문장에서의 관련된 정보를 가지는 것들의 위치를 찾는다.

이러한 접근법을 통해서 번역 성능을 매우 향상시킬 것이며, 긴 문장에도 높은 성능을 보여줄 것이다.

#### Background: Neural Machine translation

확률적인 관점에서 보면 번역은 아래의 수식의 $$y$$를 찾는 과정이다.

$$
\arg\max_{\mathbf{y}}p(\mathbf{y}\vert\mathbf{x})
$$

Nueral machine translation에서는 위 식의 확률을 최대화 하기 위해서 번역 데이터 쌍을 통해 파라미터를 학습한다. 최근에 많은 논문들이 이러한 Neural machine translation 모델들을 제안했는데, 이 모델들은 보통 두개의 부분으로 나뉜다. 첫 번째는 source 문장 $$\mathbf{x}$$를 encode하는 과정이고, 두 번째는 다시 target 문장 $$\mathbf{y}$$로 decode하는 과정이다.

이러한 구조를 가지는 모델들은 RNN을 base로 하고 있다. 따라서 RNN Encoder-Decoder라고도 불리는데, 이 구조를 조금 더 설명하자면, 우선 Encoder 과정에서는 input sequence에 대해서 encoding된 vector인 $$\mathbf{c}$$로 만든다.

$$
h_t=f(x_t,h_{t-1})
$$

$$
\mathbf{c}=q(\{h_1,...,h_{T_x}\})
$$

Decoder 과정은 주어진 context vector인 $$\mathbf{c}$$를 사용해서 다음 단어인 $$y_{t'}$$을 예측한다. 이러한 과정을 반복해서 전체 단어들인 $${y_1,...,y_{t'-1}}$$을 예측한다. 즉 decoder는 다음의 확률로 해석될 수 있다.

$$
p(\mathbf{y}) = \prod^T_{t=1}p(y_t\vert \{y_1,...,y_{t-1}\},c)
$$

여기서 확률은 non-linear함수인 $g$에 대해 다음과 같이 정의된다.

$$
p(y_t\vert\{y_1,...,y_{t-1},c)=g(y_{t-1},s_t,c)
$$

#### Learning to align and translate

앞서 말한 것처럼 align 과 translation을 같이 학습하는 모델을 만들 것이다. 따라서 이러한 새로운 모델에는 두가지의 새로운 구조가 있다. 하나는 Bidirection RNN encoder이고, 또 하나는 디코딩 과정에서 source sentence를 검색하는 decoder이다. 아래의 그림은 전체적인 구조를 그림으로 나타낸 것이다.


![s2swa](https://i.imgur.com/wOx53vt.jpg)

**Decoder: Genenral description**

이전 section에서 정의한 조건부 확률을 다시 새롭게 정의한다.

$$
p(y_i\vert y_1,...,y_{i-1},\mathbf{x})=g(y_{y-1},s_i,c_i)
$$

여기서 새롭게 나온 $$s_i$$는 RNN의 hidden state이다. 다음과 같이 계산된다.

$$
s_i=f(s_{i-1}, y_{i-1}, c_i)
$$

우선은 이 방법은 기존의 encoder-decoder 방식과는 다르다. 여기서 $$c_i$$는 encoder의 $$(h_1,...,h_{T_x})$$에 의해 계산된다. 여기서 각각의 $$h_i$$는 i번째 단어 주변을 좀 더 focus한 정보를 가지고 있다.(이 부분에 대한 설명은 다음 section에서 자세히 설명한다)

따라서 context vector $$c_i$$는 각 $$h_i$$들에 weight들을 각각 곱해서 계산된다.

$$
c_i=\sum^{T_x}_{j=1}\alpha_{ij}h_j
$$

여기서 각 $$h_j$$에 대한 가중치인 $$\alpha_{ij}$$는 다음과 softmax의 형태로 계산된다.

$$
\alpha_{ij}=\frac{\exp(e_{ij})}{\sum^{T_x}_{k=1}\exp(e_{ik})}
$$

그리고 또 여기의 $$e_{ij}$$는 다음과 같다.

$$
e_{ij}=a(s_{i-1}, h_j)
$$

$$e_{ij}$$은 alignment model이라 부른다. 이 값은 j번째 input 주변의 정보들이 얼마나 i번째 output과 적합한지에 대한 점수를 나타낸다. 이 점수는 RNN의 hidden state인 $$s_{i-1}$$과 j번 째 annotation인 $$h_j$$에 의해 계산된다.

alignment model인 a는 하나의 feedforward neural network로 만들었으며, 다른 네트워크와 같이 학습되도록 만들었다. 기존의 machine translation에서는 alignment를 latent variable로 봤었는데, 여기서는 하나의 network의 variable이 된다.

따라서 두개의 network가 같이 학습되는 것인데, 학습과정에서 cost의 gradient는 backpropagation을 통해 두 network에 같이 사용된다.

그리고 그 전의 annotation($$h$$)들을 weighted sum 하는 것은 annotation의 기대값을 구하는 것이라 생각하면 된다. 즉 $$\alpha_{ij}$$를 target word $$y_i$$와 source word $$x_j$$에 대한 확률이라 생각하면, i번째 context vector $$c_i$$는 annotation의 기대값이 된다.

확률 값인 $$\alpha_{ij}$$와 거기에 사용된 energy인 $$e_{ij}$$는 annotation $$h_j$$의 $$s_{j-1}$$에 대한 중요도를 나타낸다.

이러한 구조를 통해 decoder는 attention 메커니즘을 가진다. 즉 decoder가 source sentence에서 집중해야 할 부분을 결정할 수 있게 한다.

**Encoder: Bidirectional RNN for annotating sequences**

일반적인 RNN 구조에서는 input sequence에 대해 순방향으로 차례대로 계산을 하게 된다. 그러자 여기에서는 annotation이 앞선 단어에 대한 정보 뿐만 아니라 이후에 오는 정보도 포함할 수 있도록 양방향의 RNN(bidirection RNN, BiRNN)을 사용했다. 그리고 이 BiRNN은 이미 음성 인식분야에서 성공적인 성과를 보여줬다.

BiRNN은 순방향(forward)와 역방향(backward) RNN, 두개의 RNN으로 구성된다. 먼저 순방향 RNN $$\overset{\rightarrow}{f}$$은 sequence를 처음부터 순서대로 읽고 forward hidden state$$({\overset{\rightarrow}{f}}_1,...,{\overset{\rightarrow}{f}}_{T_x})$$를 계산한다. 역방향 RNN $$\overset{\leftarrow}{f}$$는 sequence를 역방향으로 마지막부터 처음까지 읽고 backward hidden state$$({\overset{\leftarrow}{f}}_1,...,{\overset{\leftarrow}{f}}_{T_x})$$를 계산한다.

이제 각 단어 $$x_j$$에 대해서 foward hidden state와 backward hidden state를 concatenate해서 annotation $$h_j$$를 구한다.

$$
h_j=\big[{\overset{\rightarrow}{f}}^T_j;{\overset{\leftarrow}{f}}_j^T\big]
$$

이 방법을 통해 annotation $$h_j$$는 j번쨰 단어 앞뒤의 정보를 모두 포함할 수 있게 된다.

#### Experiment Settings

실험을 위한 데이터셋은 WMT'14 English-French corpora를 사용했다. 데이터에 대한 설명은 조경현 교수님의 paper review 글을 참고하자([blog](https://reniew.github.io/31/)).

그리고 data selection 방법을 통해 데이터의 size를 줄였고, 앞선 데이터의 monolingual data는 사용하지 않았다.

실험을 위해 두가지 모델을 학습 시켰다. 하나는 기존은 RNN Encoder-Decoder모델(RNNencdec)이고 나머지 하나는 이 글에서 소개한 모델(RNNsearch)이다. 두 모델을 각각 두번 학습 시켰는데, 한번은 문장 길이를 30으로 제한한 것이고(RNNencdec-30, RNNsearch-30), 두 번째는 문장 길이를 50(RNNencdec-50, RNNsearch-50)으로 제한시켰다.

RNNencdec와 RNNsearch 모두 encoder,decoder는 각각 1000개의 hidden unit을 가진다.

학습과정에서 minibatch SGD 알고리즘을 사용했으며, Adadelta를 사용해 파라미터 update를 했다. 각 SGD update는 80 크기의 mini-batch로 업데이트 되었다.

모델을 학습 한 이후에는 beam search를 이용해 번역을 진행했다. 이 과정 또한 앞선 모델의 설명에서 확인 할 수 있다.

#### Quantitative Results


![qresult](https://i.imgur.com/AMZVlsm.jpg)

![qresult2](https://i.imgur.com/jeHbIcU.jpg)

우선은 위의 그래프를 보면 RNNencdec 모델의 경우 문장 길이가 길어 질 수록 BLUE 스코어가 확연히 떨어지는 것을 볼 수 있는데 RNNsearch-50 의 경우 문장의 길이가 길어 지더라도 지속적으로 높은 BLUE 스코어를 가진다.

#### Qualitative Analysis

![qual](https://i.imgur.com/cNPVifz.jpg)

위 그림은 각각의 단어에 대해 annotation $$\alpha_{ij}$$ 값을 grayscale로 나타낸 그림이다. 그림을 보면 각각 단어가 매칭되는 부분에서 annotation이 높은 값을 가지는 것을 알 수 있고, 정확히 대칭되는 위치가 아니라 앞 뒤의 위치에서도 매칭될 수 있는 모습을 볼 수 있다.

그리고 논문에는 긴 문장의 영어를 번역했을 때 나온 불어가 굉장히 잘 번역되었다고 하는데, 이 부분은 불어를 잘 모르기 때문에 생략한다.





---

긴글 읽어주셔서 감사합니다. 오역 및 잘못된 내용이 있을 수 있습니다. 잘못된 부분 혹은 이해가 잘 안되는 부분은 댓글 혹은 메일로 말씀해주시면 감사하겠습니다!
