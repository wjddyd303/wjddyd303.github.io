---
layout: post
title:  "Seq2seq (2): Sequence to Sequence Learning with Neural Networks"
date:   2018-08-13 13:47:35 +0900
categories: NLP
tag: NLP
---

최초의 Seq2Seq 개념을 도입한 논문에 대해서 알아보았다. 이번에는 Sequence to sequence 개념을 사용해 일반적으로 최초의 Neural Machine Translation 모델로 알려져 있는 Google의 [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)에 대해서 알아보자.

---

1. [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://reniew.github.io/31)
2. [Sequence to Sequence with Neural Network](https://reniew.github.io/35)

---


### Sequence to Sequence Learning with Neural Networks



#### Introduction

Deep Neural Network는 여러 분야에서 많은 성과를 보여줬다. 많은 분야에서 이미 증면된 DNN은 유연하고 강력함에도 불구하고, input과 target이 고정된 dimension의 vector인 경우의 문제에만 사용할 수 있었다. 이러한 점이 DNN의 심각한 한계였다. 많은 중요한 문제는 input과 target이 길이에 대한 정보가 미리 주어지지 않은 sequence이기 때문이다. 예를 들어 QA(Question Answering)문제도 주어진 가변길이의 sequence를 answer에 대한 가변길이 seqeunce로 바꾸는 문제이다.

따라서 이 논문에서는 LSTM을 활용해서 sequence에서 sequence로 가는 구조를 소개한다. 아래의 그림은 모델의 구조를 보여준다.

![s2s](https://i.imgur.com/KK3SZzU.jpg)

모델에 대해서 자세히 알아보자.

#### The Model

Recurrent Neural Network는 sequence의 일반적인 feedforward neural network이다. 주어진 input sequence $$(x_1,...,x_n)$$에 대해, RNN은 아래의 수식을 반복하면서 output sequence $$(y_1,...,y_T)$$를 계산한다.

$$
\begin{align*}
h_t&=\sigma(\mathbf{W}^{hx}x_t+\mathbf{W}^{hh}h_{t-1})\\
y_t&=\mathbf{w}^{yh}h_t
\end{align*}
$$

RNN을 통해 input sequence를 앞선 시간 정보를 포함한 output seqeunce으로 쉽게 mapping 할 수 있다. 하지만 input의 길이와 output길이가 다르고 두 길이가 간단한 관계로 이루어 지지 않은 경우에는 RNN을 적용하기 어렵다.

이를 해결할 간단한 전략은 input sequence를 RNN을 통해 고정된 길이의 vector로 만든 후에 다시 그 vector를 RNN을 통해 우리가 원하는 target sequence를 구하는 방법이다.([Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)와 유사한 방식이다) 단순한 RNN을 통해서도 이러한 전략을 구현할 수 있지만, 중요한 문제점은 RNN은 Long-term에 대한 정보를 포함하지 않는다는 것이다. 따라서 Long Short-Term Memory(LSTM)을 사용해서 이러한 long-term에 대한 정보를 잡아 낼 것이다.

여기서 LSTM의 최종적인 목표는 input sequence에 대한 output sequence의 조건부 확률 $$p(y_1,...,y_{T'}\vert x_1,...,x_T)$$를 구하는 것이다. 따라서 위의 전략대로 우선 주어진 input sequence를 통해 고정된 길이의 vector representation $$v$$를 먼저 구한다. 여기서 벡터 $$v$$는 첫 RNN의 마지막 hidden state의 값이 된다. 그리고 다시 LSTM을 통해 $$y_1,...,y_{T'}$$에 대한 확률을 게산한다. 즉 아래의 식과 같이 계산한다.

$$
p(y_1,...,y_{T'}\vert x_1,...,x_T)=\prod^{T'}_{t=1}p(y_t\vert v,y_1,...,y_{t-1})
$$

위 식에서 우항의 $$p(y_t\vert v,y_1,...,y_{t-1})$$은 전체 vocabulary의 단어를 통해 softmax값을 계산해서 구한다. 문장이 끝난 지점에는 "<EOS>"라는 토큰을 사용해 문장이 끝났다는 정보를 주고 그 지점부터 다시 output의 계산을 시작한다. 즉 위의 그림을 보면 먼저 LSTM은 "A", "B", "C", "<EOS>"를 먼저 계산한다 그리고 마지막의 hidden state값을 통해 "W", "X", "Y", "Z", "<EOS>" 에 대한 확률을 계산한다.

실제 모델은 위의 설명과 세가지 방법이 다르다. 첫 번째로는 input sequence와 output sequence에 대해 두개의 다른 LSTMs를 사용했다. 두 번째로는 deep한 LSTM의 성능이 더 뛰어난 것을 확인한 후 4개의 layer를 사용하는 LSTM을 사용했다. 세 번쨰로는 input sequence의 순서를 뒤집어서 사용했다. 예를 들면 문장 a, b, c를 d, e, f로 번역하는 대신 순서를 바꿔 c, b, a 를 LSTM을 통해 d, e, f 값이 나오도록 학습시켰다.(a->d, b->e, c->f 가 정확한 번역) 이러한 간단한 data transformation이 LSTM의 성능을 획기적으로 올려준다는 것을 확인 했다.

#### Experiments

WMT'14 English to French 기계번역 테스크에 이 모델을 적용시켰다. 그리고 다른 SMT 시스템을 참고하지 않고 직접 이 모델을 통해 번역했다.

데이터에 대해서 자세한 사항은 총 12M개의 sentence 중 일부를 학습시켰는데 여기에는 348M개의 프랑스어 단어와 304M개의 영어 단어가 포함되어있다. 그리고 두 언어에 대해서 각각 일정한 vocabulary를 사용했는데, 여기에는 160,000개의 가장 많이 사용되는 단어가 포함되어 있다. 그리고 vocabulary에 포함되지 않은 단어는 "UNK"라는 토큰으로 대체했다.

실험의 핵심은 크고 깊은 LSTM 모델을 많은 문장 쌍으로 학습시키는 것이다. 학습은 주어진 문장 $$S$$에 대한 정확히 번역된 문장 $$T$$의 log-확률값을 최대화 하는 것이다. 따라서 objective 함수는 다음과 같다.

$$
1/\vert S\vert\sum_{(T,S)\in S}\log p(T\vert S)
$$

여기서 $$S$$가 training set이 된다. 학습이 끝난 후에는 주어진 문장에 대해서 가장 높은 확률을 같은 문장 $$T$$를 찾는다.

$$
\hat{T}=\arg\max_T p(T\vert S)
$$

실제 예측 과정에서는 간단한 left-to-right beam search decoder를 사용했다. 즉 어떤 특정 $B$개의 문장을 정하고 각 timestep마다 다른 문장들을 추가한 후 위의 log-확률이 높은 B개를 제외하고 나머지는 모두 버린다. 그리고 "<EOS>" 토큰이 나오면  문장에 이 토큰을 더해 완성된 문장을 만든다. Decoder의 경우 근사시키는 방법인 반면 이 방법은 구현하기 매우 간단하다. beam size를 1로 했을 때도 매우 잘 동작했다.

#### Reversing the Source Sentence

앞서 말했듯이 LSTM에서 source sentence를 거꾸로 사용했을 때 더욱 좋은 결과를 만들어 냈다. 이러한 방법을 통해 perplexity는 5.8에서 4.7까지 떨어졌고, BLEU score는 25.9에서 30.6까지 올랐다.

이러한 현상에 대해서 정확한 이유는 아직 모르지만, input sentence를 뒤집음으로써 각 대응되는 단어끼리의 평균 길이는 변하지 않고 처음의 단어에 대해 대응되는 단어와 거리가 가까워지기 때문에 조금 더 효율이 올라갔을거라는 추측을한다.

#### Training detalis

학습과정에서 사용한 방법들의 자세한 설명은 다음과 같다.

* 4개의 layer
* 1000개의 cell
* 단어의 경우 1000 dimension으로 embedding
* 160,000개의 input vocabulary
* 80,000개의 output vocabulary
* 모든 파라미터는 (-0.08, 0.08)사이의 uniform distribution이루도록 초기화
* SGD 사용, learning rate = 0.7, 5에폭 후에는 0.5에폭마다 learning rate를 절반으로 줄임. 총 7.5에폭으로 학습
* 128 sequence 크기의 배치 사용
* Vanishing gradient는 발생하지 않았지만, exploding gradient의 발생 떄문에 gradient의 norm 값에 대해 constraint를 줌. 즉 매 배치에 대해서 평균 gradient의 L2 norm값이 5를 넘어가면 5값을 줌.
* 대부분의 문장은 20~30정도의 길이로 짧았지만 몇몇의 경우 100이상의 길이인 경우도 있었다. 따라서 128의 mini batch에는 주로 짧은 문장이 대부분이였고 긴문장 몇개가 포함되었다. 따라서 미니배치로 학습시 길이가 너무 달라 학습이 잘되지 않는 문제를 위해 미니배치안에서 길이를 고정시켰다.

#### Result

결과는 다음과 같다.

![result_s2s](https://i.imgur.com/FrZJLbd.jpg)

![result_s2](https://i.imgur.com/RhjACoH.jpg)


그리고 이 모델에서 중간의 input에서 output으로 가는 hidden state의 값인 고정된 벡터를 PCA를 통해 2차원 좌표상에 나타낸 결과는 다음과 같다.

![pca](https://i.imgur.com/7S4U55z.jpg)

결과를 보면, 문장 순서에 따라 값이 매우 달라지는 것을 볼 수 있고, 능/수동 은 크게 상관없는 것을 확인 할 수 있다.


---

긴글 읽어주셔서 감사합니다. 오역 및 잘못된 내용이 있을 수 있습니다. 잘못된 부분 혹은 이해가 잘 안되는 부분은 댓글 혹은 메일로 말씀해주시면 감사하겠습니다!
