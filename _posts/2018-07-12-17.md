---
layout: post
title:  "정보이론 : 엔트로피, KL-Divergence"
date:   2018-07-12 15:47:35 +0900
categories: InformationTheory
tag: Entropy
---

### 정보이론 : 엔트로피, KL-Divergence

<br>

지난 포스트에서 소개했던 것 처럼 정보이론이란 정보의 양을 측정하는 분야이다. 그리고 그 정보의 양을 표현하는 개념이 엔트로피(Entropy)이다. 그리고 정보를 전달할 때는 비트의 단위로 전달을 하게 된다.

셰넌은 그의 논문에서 정보를 '불확정성' 또는 '불확실성'으로 표현했다. 즉 어떤 사건에 대해 불확실성이 커질수록 정보량이 많아진다는 것이다.

직관적인 이해를 위해 설명을 하면, 어떠한 사건을 누군가에게 통신을 통해 전달해야 한다고 가정을 하자. 확률이 높은 사건이 설명하기 쉽겠는가, 확률이 희박한 사건을 설명하기 쉽겠는지에 대해 생각해보면 된다. 예를 들어 동전던지기를 설명하려면 앞면, 뒷면 두가지 경우중에 한가지가 나왔다는 것을 전달하면 되지만 주사위 던지기의 경우에는 1부터 6까지의 경우 중에서 하나가 나왔다는 것을 전달해야 한다. 이 두가지 경우중 전달해야 할 정보량이 많은 것은 당연이 후자일 것이다.
<br>
#### Self-Information
<br>
확률 $p$를 가지는 사건(메시지) $A$ 의 정보를 정의하는 것을 Self-Information(or surpisal)이라고 한다.
어떤 메시지 $m$에 대한 self-information은 다음과 같이 정의된다.
(여기서 log는 밑이 2인 log를 뜻 한다.)

$$
I(m)=log\bigg(\frac{1}{p(m)}\bigg)=-\log(p(m))
$$

정보량은 확률에대해 $log_2$를 씌운 값인데, 확률은 $0$~$1$값을 가지므로 음수값이 나오게된다. 그러나 정보량은 양수의 값을 가져야 하므로 -를 씌워줘서 양수로 만들어준다. 그리고 항상 로그의 밑이 2가 되는 것은 아니다. 경우에 따라 맞는 수를 사용하면 된다. 예를 들어 Discrete한 사건이 아니라 Continuous한 random variable에 대해서 정보량을 측정할 떄에는 보통 e를 밑으로 하는 log를 사용한기도 한다.
그리고 이 정보량인 $I(m)$의 단위는 비트(bit)이다.(*여기서 밑을 2로한 이유이기도 하다.*) 예를 들어서 확률 1/8을 가지는 사건에 대한 정보량은 $-log(\frac18)$로 3이된다. 즉 이 메시지를 전달하기 위해서는 3bit가 필요하다는 뜻이 된다.
<br>
#### Entropy
<br>
Self-Information은 하나의 메시지에 대한 자기 자신의 정보량을 나타냈다. 엔트로피란 어떤 다수의 메시지들(메시지 집합$M$)에 대해서 각각의 정보량을 평균한 값을 의미한다.
평균값을 계산하는 방법이 메시지 공간($M$)의 사건들이 Discrete한 경우와 Continuous한 경우 계산하는 방법이 다르므로 두 가지 수식을 나눠서 설명한다.

* 메시지 집합 $M$에 대한 엔트로피(Discrete)

$$
H(M)=E[I(M)]=\sum_{m\in M} p(m)I(m)=-\sum_{m\in M}p(m)\log p(m)
$$

여기서 $E[]$는 평균값(기대값)이다. 즉 평균을 계산하는 것이므로 각 변수와 그 변수에 대한 확률들을 더한 값이 엔트로피가 된다.

* 메시지 집합 $M$에 대한 엔트로피(Continuous)

$$
H(X)=E[I(X)]=\int p(x)I(x) dx = -\int p(x)\log(p(x))dx
$$

모든 변수 $x$에 대한 적분값으로 의미적으로는 기대값을 계산하는 것이므로 위의 식과 같다고 할 수 있다.
(여기서는 보통 $\log_2$대신 $\ln$을 사용한다.)

그리고 어떤 메시지 공간에 대해 엔트로피가 최대가 되는 경우 또한 중요하다. 공간 $M$에 대해서 각 사건들이 uniform distribution이 되는 경우 엔트로피가 최대가 된다. 즉 각 사건의 확률이 $p(m)=\frac1M$으로 동일하다면 엔트로피가 최대가 된다.
($E[M]=\log|M|$)
<br>
#### Joint Entropy
<br>
Discrte한 두 변수 $X,~Y$에 대한 entropy인 Joint Entropy는 다음과 같이 정의된다.

$$
H(X,Y)=E_{X,Y}=[-\log p(x,y)]=-\sum_{x,y}p(x,y)\log p(x,y)
$$

만약 $X,~Y$가 independent하면, Joint Entropy는 각각의 Entropy의 합이 된다.
Continuous한 경우의 Entropy도 Sigma 대신 Integral을 사용해서 계산하면 된다.

<br>
#### Cross Entropy
<br>
Cross Entropy는 Joint Entropy와 수식적으로 비슷해 혼동되는 경우가 있는데, 다른 개념이다.
두 확률 분포 $p$와 $q$에 대해서 분포$p$ 대신 $q$를 사용해 분포 $p$를 설명할때 필요한 정보량을 Cross Entropy라 한다.

수식을 보자.

$$
H(p,q) = E_p[-\log(q)] = -\sum_xp(x)\log q(x)
$$

정보를 나타내는 $log$값에 $p(x)$대신 $q(x)$를 사용한 것을 볼 수 있다. 즉 분포 $q(x)$를 활용해서 $p$를 설명하기 위해 $p$의 기대값을 구한 식이다.

<br>
#### Conditional Entropy
<br>
어떤 특정한 값을 같는 random variable $Y=y$에 대해 $X$의 conditional entropy는 다음과 같다.

$$
H(X|Y=y)=E_{X|Y=y}[-\log p(x,y)]=-\sum_{x\in X}p(x|y)\log p(x|y)
$$

conditional entropy는 다음의 식이 성립한다

$$
H(X|Y) = H(X,Y) - H(Y)
$$
<br>
#### Kullback-Leibler divergence(KL-Divergence)
<br>
KL-Divergence에 대한 수식을 설명하기 전에 개념의 Idea부터 얘기를 해보자.

우리가 어떤 확률 분포 $p(x)$를 가지고 있다. 이 확률 분포를 전송을 해서 정보를 전달을 해야하는데 각각의 변수$x$에 대해 $p(x)$를 모두 전달하기에는 정보량도 많고 전달할 수단이 부족하다고 가정하자. 이때 우리는 분포 $p(x)$를 전달하기보다 이미 정의된 다른 확률 변수$q(x)$를 대신 전달하려는 생각을 했다. 이렇게 되면 어떤 분포인지, 그리고 분포의 특정 몇가지 값들만 전달하면 되므로 정보량 자체가 엄청나게 줄고 그래서 전송 또한 가능해진다.

그러면 이제 새로운 문제에 봉착한다. 과연 어떤 분포 $q(x)$를 선택해서 보내야 하는가이다.
최대한 분포$p(x)$와 유사한 분포를 선택해야할 것이다. 이때 분포 $p$와 $q$의 유사한 정도를 계산하는 방법이 KL-Divergence이다.

KL-Divergence는 그 값이 작을 수록 두 분포가 유사하다는 것을 의미하고 값이 0이 되면 두 분포가 같은 분포라는 뜻이 된다.

KL-Divergence 수식을 위해서는 Cross Entropy를 사용한다. $p$를 $q$로 설명하는 정보량을 뜻하는 Cross Entropy에서 $p$가 자기자신을 설명하는 정보량인 $p$의 엔트로피의 차이가 KL-Divergence가 된다.
수식을 보자

$$
\begin{align*}
D_{KL}(p||q) &= H(p,q)-H(q)\\\\
&=-\sum_{x\in X} p(x)\log q(x)-(-\sum_{x\in X}p(x))\\\\
&=\sum_{x\in X}p(x)\log\frac{p(x)}{q(x)}
\end{align*}
$$

두 엔트로피의 차이를 계산함으로써 $p$대신 $q$를 사용했을 때의 정보량의 차이를 계산하는 식이다. 그리고 KL-Divergence를 거리함수로 생각하는 경우도 있는데 엄밀하게 말하면 거리함수가 아니다. 왜냐하면 p,q를 바꿨을 때 두 값이 같아야 하지만 계산해보면 다르기 때문에 엄밀하게 거리함수라 할 수는 없다.

이번 포스트에서 정보이론의 내용 중 정보량을 다루는 개념들에 대해서 알아보았다. 다음 포스트에서는 정보를 압축하는 방법들에 대해 다뤄보도록 한다.
