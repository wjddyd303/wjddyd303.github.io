---
layout: post
title:  "CS20(TensorFlow) Lecture Note (12): Machine Translation, Seqeunce-to-sequence and Attention"
date:   2018-09-07 13:47:35 +0900
categories: TensorFlow
tag: tensorflow
---

스탠포드의 TensorFlow 강의인 cs20 강의의 lecture note를 정리한 글입니다. 강의는 오픈되지 않아서 Lecture note, slide 위주로 정리된 글임을 참고 해주시길 바랍니다. 강의의 자세한 Syllabus 및 자료들을 아래 링크를 참고해 주세요.

[CS20: TensorFlow for Deep Learning Research](http://web.stanford.edu/class/cs20si/)


---

##### Post list

* [Lecture 1, 2: Overview & TensorFlow Operation](https://reniew.github.io/32)
* [Lecture 3: Linear and Logistic Regression](https://reniew.github.io/33)
* [Lecture 4: Eager execution and interface](https://reniew.github.io/34)
* [Lecture 5: word2vec + manage experiments](https://reniew.github.io/36)
* [Lecture 6, 7: Intro to ConvNet & ConvNet in TensorFlow](https://reniew.github.io/38)
* [Lecture 8: CNN(Style transfer), TFRecord ](https://reniew.github.io/39)
* [Lecture 10: Variational Auto Encoders(VAE)](https://reniew.github.io/40)
* [Lecture 11: RNNs in the TensorFlow](https://reniew.github.io/41)
* [Lecture 12: Machine Translation, Seqeunce-to-sequence and Attention](https://reniew.github.io/42)

---

### 12. Machine Translation, Seqeunce-to-sequence and Attention

이번 강의에서 알아볼 내용은 크게 다음과같이 세가지로 나눌 수 있다.

* 새로운 task: Machine Translation
* 새로운 neural netwrok architecture: Sequence to sequence
* 새로운 neural technique: Attention

위의 세가지는 각각 별개의 내용이 아니고 깊게 연관되어있다. Machine Translation을 위한 주로 사용하는 architecture가 seuqeunce to sequence이고, seqeunce to sequence를 사용할 때 같이 주로 사용하는 것이 attention 기술이다. 이제 하나씩 알아보도록 하자.

#### Machine Translation

Machine translation이란 말 그대로 기계를 통해 번역을 하는 문제다. 정확히 말하면 어떤 언어의 input sentence를 다른 언어의 output sentence로 만드는 task이다.

$$
\begin{matrix}
\text{x:	L'homme est né libre, et partout il est dans les fers}\\
\downarrow\\
\text{y: 	Man is born free, but everywhere he is in chains
}
\end{matrix}
$$

기계번역은 시대에 따라서 번역을 하는 방법이 계속해서 바껴왔는데, 어떤 방식으로 진행해 왔는지 알아보자.

**1950년대**

Machine Translation이 처음 나온것이 1950년이다. 시대적으로 기계번역이 나온 역사적 이유가있다. 이 시기는 냉전 시기로 소련과 미국은 항상 서로의 통신이나 기밀 문서들을 자동으로 번역해서 해석하기를 원했는데, 이를 위해 Machine Translation이 처음 도입된 것이다. 따라서 이당시의 machine translation은 주로 영어, 러시아어 번역이 대부분이였다.

그리고 이 당시의 번역은 주로 rule-based 방식이였다. 즉 두 개의 언어의 사전을 통해 각 단어가 대응되는 것을 찾아서 번역하는 가장 단순한 방식이다.

**1990 ~ 2000년대 초반**

기존의 rule-based 방식이 아닌 data를 통해서 번역하는 방식이 처음으로 사용되었다. 이때의 방식을 Statistical Machine Translation이라 부르는데, 데이터를 통해 확률 분포를 학습하는 방법이다.

데이터를 통해 확률을 가장 높이는 방법으로 번역한다. 즉 특정 언어 input sentnece $x$에 대해서 또 다른 언어 output sentence인  $y$로 번역할 때 다음의 확률이 가장 높은 sentence를 선택하게 된다.

$$
\arg\max_y P(x\vert y)P(y)
$$

그리고 실제로 사용할 때는 위의 수식을 bayes theorem을 사용해 다음과 같이 번형해서 두개의 부분을 계산하는 방식으로 수행한다.

$$
\propto\arg\max_yP(x\vert y)P(y)
$$

위 식을 보면 $P(x\vert y)$와 $P(y)$로 두개의 두분으로 나뉘는데, 앞부분은 Translation Model이라 부르고 parallel 데이터를 통해서 확률을 계산하고 뒷부분은 Language Model으로 monolingual 데이터를 통해서 계산한다.

여기서 parallel 데이터란, 문장들이 두개의 언어 모두로 표현되어 있는 데이터이다. 따라서 한 문장에 대해 두가지 언어표현을 통해 한 언어에 대한 다른 언어로 바꿧을 때의 확률을 계산할 수 있게 한다. 그리고 monolingual 데이터란 하나의 언어만 있는 데이터로 이 데이터를 통해 언어의 특성을 확률값으로 계산한다.

Statistical machine translation(SMT)를 통해 기존의 rule-based 방식에 비해 정확도를 매우 높였지만 그럼에도 불구하고 alignment 문제가 아직 남아있다. alignment 문제란 번역에서 두개의 언어는 각각 특성이 달라서 단어들의 순서나 품사의 순서가 다른데 이러한 부분을 SMT로는 이런 부분까지 파악하기 어렵다는 점이다. 아래는 alignment가 제대로 이뤄지지 않은 번역을 나타낸다.

$$
\begin{matrix}
\text{x: i go to school}\\
\downarrow\\
\text{y: 나는 간다 학교에}
\end{matrix}
$$

위와 같은 alignment 문제가 있음에도 불구하고 SMT는 불과 몇 년전까지만 하더라도 대부분 사용하는 방법이였고, 많은 연구가 이뤄졌다. SMT의 특징을 정리하면 다음과 같다.

* 좋은 성능을 내는 SMT system은 매우 복잡한 구조이다.
* 각 system은 각 부분부분으로 나눠서 sub-system들이 모여있는 형태다.
* 많은 feature engineering이 필요하다.
* 추가적인 많은 자료를 필요로 한다.
* 사람의 손이 빠지고는 좋은 성능을 기대하기 어렵다.

**2014년 ~**

2014년 Sequence to sequence라는 모델을 통해 Nerual Machine Translation이 도입되었다. 기존의 SMT모델에 비해 매우 간단함에도 불구하고 매우 높은 성능을 보여줬다. 그리고 현재의 대부분의 Machine translation은 NMT를 사용한다.

세계에서 가장 많이 사용하는 번역기인 google 번역기도 2014년 sequence to sequence가 나온 2년뒤인 2016년 기존의 Statistical machine translation에서 Neural machine translation으로 바꾸었다. 기존의 SMT가 오랜 기간에 거쳐서 실제 사용할 수 있게된것에 비해 NMT의 경우에는 2년만에 바로 사용하는 것을 보면 성능이 좋다는 것을 의미한다고 볼 수 있다.

그렇다면 NMT의 장점을 정리해서 보자. SMT에 비해 NMT는 다음의 장점이 있다.

* 높은 성능
  * 더욱 사람이 한 것 같은 번역
  * 문맥 이해를 잘한다
* 하나의 neural network만으로도 사용할 수 있다.
* 사람이 직접 engineering을 많이 하지 않아도 된다.

많은 장점이 있지만, 무조건 좋은 것은 아니다 NMT에도 다음과 같은 단점이 존재한다.

* 해석하기 어렵다. 즉, 정확하게 어떻게 번역되는지 확인이 어려워 debug또한 어렵다.
* 위와 비슷한 이유로 제어하기 어렵다는 단점이 있다.

따라서 무조건 NMT만 사용하는 것이 아니라 SMT도 결합해 사용하거나 SMT를 아직 사용하기도 한다. 그리고 NMT를 사용할 때 어려운 점은 다음과 같다.

* Out-of-vocabulary 문제
* 학습, 테스트 데이터의 domain이 다른 경우 성능이 떨어지는 문제
* 문장이 길어질 때 문맥을 이해하기 어려운 문제
* language-pair 데이터가 많지 않은 문제

그리고 뿐만 아니라 NMT를 사용할 경우 아래와 같이 편향된 결과가 나올 수 있다.

![kk](https://i.imgur.com/BO401r4.jpg)

Machine translation이 어떻게 발전해 왔는지 알아봤다. 최초의 방식부터 최근의 NMT 방식까지 다양한 모델들이 존재하는데, 그렇다면 machine translation 모델을 평가하는 것을 생각해보자. 사람이 직접하더라도 번역은 정답이 없는 문제이다. 하지만 우리는 모델을 평가해야 하는데 어떤 방식으로 평가할지를 정해야 한다. 따라서 평가 방식에 대해서 알아보자.

#### How to evaluate Machine translation

machine translation이 제대로 이뤄졌는지 확인하는 대표적인 지표는 'BLUE' 점수이다. 이 방법 외에도 다양한 방법이 있지만 아직까지는 대부분 BLUE 점수를 통해서 모델을 평가한다. 그렇다면 BLUE는 어떤 방식으로 모델을 평가하는지 알아보도록 하자.

**BLUE**

BLUE(Bilingual Evaluation Understudy)란 기계에 의해 번역된 문장과 사람이 작성한 문장을 비교해서 유사도를 측정해서 성능을 측정하는 지표이다. 다음의 3가지 측면으로 평가를 한다.

* n-gram precision: 1 ~ 4 gram으로 나눠 얼마나 맞는지 확인
* 문장 길이가에 대한 패널티를 곱한다.(짧은 문장에 패널티)
* 같은 단어에 대한 보정

위와 같은 계산 방식을 사용하며, 아직까지는 가장 많이 사용하는 지표이며, BLUE 점수가 절대적인것은 아니므로 다양한 지표로 평가를 해야한다.

#### Seqeunce to sequence

Sequence to sequence란 RNN을 사용해서 sequence를 다루는 모델이다. [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) 논문을 통해 처음 나왔으며 전체 내용에 대한 자세한 내용은 블로그 [post](https://reniew.github.io/31)를 참고하자.

전체적인 모델을 다음과 같다.

![en-de](https://i.imgur.com/z0tAytE.jpg)

input sequence를 받아서 하나의 벡터로 encoding하는 앞부분과 encoding된 vector를 활용해서 output sequence를 만드는 decoder로 구성되어 있다.

Machine translation에 적용해보면 input은 번역할 문장이 되고 outout은 번역이 완료된 다른 언어의 문장이 될 것이다.

이 모델을 통해 처음으로 NMT가 시작되었지만 문제가 하나 있다. 모델을 보면 input을 단 하나의 벡터로 만드는데 이 과정에서 bottleneck 현상이 발생한다. 그리고 번역의 경우 alignment가 중요한데 위와 같은 구조로는 input의 alignment정보를 decoder로 전달하기 어렵다는 문제가 생긴다. 따라서 이러한 문제를 해결하기 위해서 사용된 기술이 Attention 기법이다.

#### Attention

attention은 Sequence to sequence with attention이라 불리는 논문인 [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)에서 처음 제안된 기법이다. 이 논문에 대한 자세한 내용은 블로그 [post](https://reniew.github.io/37)를 참고하자.

Attention 기법은 input 문장의 alignment정보를 decoder에 전달하기 위해 사용된다. 기존의 모델을 생각하면 input에 대한 정보를 하나의 vector로 만들어 output sequence를 만들 때 사용했었다. attention이란 위의 encoding된 vector와 함께 alignment 정보를 같이 사용해서 계산한뒤 sequence의 각 값들을 계산한다. 아래의 그림을 참고하면 attention이 어떤 구조로 계산되는지 어느정도 이해할 수 있을 것이다.

![attention](https://i.imgur.com/HddESNP.gif)

output은 앞의 정보들을 계산해 가장 확률이 가장 높은 단어들이 나오는 구조인데, 이때 단순히 가장 높은 token을 선택하는 greedy한 방법을 사용하는 것이 아니라 Beam search라는 기법을 이용해서 output token을 선택한다. 마지막으로 beam search를 알아보자.

#### Beam Search

beam search란 정해둔 크기(beam size)의 개수만큼 높은 확률의 token을 계속해서 후보로 두고 전체 output sequence를 선택하는 방식이다. 아래의 그림을 보면 beam search를 직관적으로 이해할 수 있을 것이다.

![beam](https://i.imgur.com/TnXJIDl.gif)
