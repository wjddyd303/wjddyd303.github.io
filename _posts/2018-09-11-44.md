---
layout: post
title:  "ConvS2S: Convolutional Sequence to Sequence Learning"
date:   2018-09-11 15:47:35 +0900
categories: NLP
tag: NLP
---


이번에 소개할 논문은 Facebook에서 발표한 ConvS2S라 불리는 [Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf)이다. 이름부터 알 수 있듯이 sequence to sequence 모델을 convolutional neural network를 사용해서 만든 모델이다. 기존의 sequence to sequence 모델들은 대부분 RNN을 기반으로 나왔는데, CNN을 사용해서 sequence를 다루면서 높은 성능을 보여준 모델이라 많은 의미가 있다. RNN 대신 CNN으로 어떻게 모델을 구성하는지, 또 RNN대신 CNN을 사용하면 어떤 장점이 있는지 알아보자.

---

#### Introduction

Sequence to sequence learning은 기계번역, 음성인식, text 요약 등 많은 분야에서 성공적인 결과를 보여줬다. 이러한 모델 구성중 대부분이 encoder에서 bi-directional RNN을 사용하고 decoder에서도 RNN을 사용했다.

그에 반해 Convolutional neural network는 sequence를 다루는 모델에서는 별로 사용하지 않았다. sequence 모델에 CNN을 적용할 때를 RNN을 적용할 때와 비교해 보자. CNN을 적용하면 고정된 크기의 문맥만을 얻을 수 있다. 즉 우리가 정의한 kernel size로만 문맥을 파악하게 할 수 있는데, 다행이도 CNN은 몇개의 layer를 추가함으로써 context size를 늘리는 것이 쉽다. 따라서 모델의 maximum length of dependencies를 제어하기가 쉽다. 또한 RNN은 이전 step의 값이 있어야 계산을 할 수 있는데 반해 CNN은 그럴 필요가 없기 때문에 병렬화가 쉽다는 장점이 있다.

Multi-layer CNN을 생각해보자. 층층이 쌓여서 hierarchical한 구조를 만드는데 이는 lower layer에서는 단어 주변의 문맥을 파악하고 higher layer로 가면 먼 거리의 단어도 파악할 수 있게 된다.

그렇다면 시간을 고려해보자. CNN의 경우 n개의 단어를 문맥으로 파악하려면 드는 시간은 $O(\frac{n}{k})$이다. RNN의 경우를 생각해보면 linear하기 때문에 시간은 $O(n)$이 된다.

해당 논문에서는 sequence를 다루는 모델로 전체가 convolutional neural network로 구성된 모델을 소개한다. 다음의 세가지를 사용해서 모델을 구성할 것이다.

* Gated linear units (Dauphin et al., 2016)
* residual connections (He et al., 2015)
* attention

소개한 모델의 평가는 두가지 task로 진행한다. Machine translation과 Text summerization으로 진행한다.

#### Recurrent Sequence to Sequence Learning

기존의 recurrent한 sequence to sequence 모델을 생각해보자. input에 대해서 RNN을 통해 representation을 계산한다. 여기까지가 encoder이고, decoder에서는 구한 representation을 가지고 다시 RNN 모델을 사용해 output을 만든다. 그리고 decoder에서는 conditional input을 사용하기도 한다. attention을 도입한 모델에서는 conditional input으로 representation을 가중 평균한 값으로 사용한다.

그리고 가장 흔히 쓰이는 RNN 모델은 LSTM과 GRU이다. 둘다 Elman RNN을 응용해서 나온 모델로 long-term dependency를 잡기 위해 만들어 졌다. 그리고 최근에 가장 많이 쓰이는 모델은 bi-directional encoder 이다. RNN을 input에 대해 양방향으로 두개의 RNN을 만들어서 사용하는 것이다. 그리고 추가적인 기법으로는 shortcut과 residual connection을 많이 사용한다.

#### A Convolutional Architecture

sequence to seqeunce modeling을 fully convolutional architecture를 살펴보자.

**Position Embeddings**

우선 가장 먼저 input $\mathbf{x}$을 embedding vector $\mathbf{w}$로 만든다. 그리고 embedding한 dimension과 똑같이 각 token의 절대적인 위치에 대한 embedding vector $\mathbf{p}$를 만들어서 두 vector를 더해서 representation vector $\mathbf{e}$를 만든다.

$$
\begin{align*}
&\mathbf{x} = (x_1,...,x_m)\\
&\mathbf{w} = (w_1,...,w_m)\\
&\mathbf{p} = (p_1,...,p_m)\\
&\mathbf{e} = (w_1+p_1, ...,w_m+p_m)
\end{align*}
$$

그리고 이러한 position embedding 기법은 decoder에 의해 만들어진 ouput 값에서도 사용된다.

**Convolutional Block Structure**

encoder와 docoder 모두 simple block 구조를 비슷하게 사용한다. 이 block은 고정된 개수의 input을 연산한다. 여기서 $l$-th block을 decoder에서는 $\mathbf{h}^l=(h_1^l,...,h_m^l)$이라 표현하고 encoder에서는 $\mathbf{z}^l=(z_1^l,...,z_m^l)$라 표현한다. 그리고 해당 논문에서 block과 layer는 같은 의미로 사용된다.

각각의 block의 구성은 1d convolution + non-linearity로 구성된다. 하나의 예를 보자. decoder에서 하나의 block이 있다고 하자. 그리고 이 block의 kernel size가 5라고 하면 convolution 하나의 단일 결과인 $h_i^1$는 k개의 단어에 대한 정보를 포함하고 있다. 몇개의 block들을 위에 쌓음으로써 정보를 포함하는 단어의 개수를 늘릴 수 있다.

예를 들어 kernel size가 5인 6개의 blcok을 쌓았다고 생각하면 총 25개의 단어의 정보를 포함하고 있다.

그리고 각 convolution의 kernel의 파라미터는 $\mathbf{W}\in\mathbb{R}^{2d\times kd}$와 $\mathbf{b}_w\in\mathbb{R}^{2d}$이다. 위 파라미터는 단어 하나당 $d$-dimension vector인 단어 k개를 포함하는 matrix인 $\mathbf{X}\in \mathbb{R}^{k\times d}$를 input으로 계산된다.

kernel에 적용시킨 결과 output을 $\mathbf{Y}\in\mathbb{R}^{2d}$라 부른다. 이 vecotr의 dimension은 input의 dimension의 2배가 되었는데 이는 gated linear unit을 적용시키기 위함이다.

$$
\mathbf{Y}=\text{conv}(\mathbf{X}\mathbf{W})+\mathbf{b}
$$

$$
\text{where}~ \mathbf{X} = [\mathbf{e}_i,...,\mathbf{e}_{i+k-1}]\text{,  with  }\mathbf{e}_i = \mathbf{w}_i + \mathbf{p}_i
$$

그리고 이 모델에서는 convolution 결과에 non-linearity로 gated linear unit(GLU: [Dauphin et al., 2016](https://arxiv.org/pdf/1612.08083))를 사용했다. convolution을 output의 dimension을 생각해보면 input과는달리 2d였다. 이는 GLU를 사용하기위해 dimension을 2배로 만들어 준것인데, dimension이 2배가된 output vector를 2개로 나눠서(A,B) GLU에 적용시킨다. GLU의 식은 다음과 같다.

$$
v([A,B]) = A\otimes\sigma(B)
$$

여기서 $A, B$는 d차원의 vector가 된다. 그리고 $\otimes$는 element-wise multiplication이다. GLU를 통해 다시 output은 resize되어서 dimension이 input과 같아진다.

GLU에 대해서 좀 더 생각해보자. input으로 A,B 두 벡터가 들어가게 되는데, 하나는 값 그대로 들어가고 또 다른 하나는 sigmoid를 적용시켜서 들어간다. sigmoid를 적용시킨 값은 A를 문맥에 적용시키기 위해서 사용한 것이다.

> **개인적인 생각**
GLU에서 하나의 vector를 반으로 나눠서 적용시키는데 이는 거의 유사한 값이라고 생각하자. 이때 하나는 그 값 그대로 넣고 나머지 하나는 sigmoid를 적용시킨다. 따라서 한 벡터는 값을 의미하고 한 벡터는 그에 대한 확률 값이라 생각할 수 있다. 즉 input을 적용시키지 않고 비율을 각각 곱해준뒤 적용시킨다고 생각할 수 있다.

해당 논문에서는 GLU에서 sigmoid를 사용했는데, tanh를 사용하는 경우도 있다. 하지만 Dauphin의 말을 인용하면 language modeling에서는 tanh 보다 sigmoid를 적용하는 것이 성능이 좋다고 한다.

다음으로는 deep한 convolutional network를 만들기 위해서 residual connection을 사용했다.  residual connection을 사용한 수식은 다음과 같다.

$$
h_i^l = v(\mathbf{W}^l[h_{i-k/2}^{l-1},...,h_{i+k/2}^{l-1}]+b_w^l)+h_i^{l-1}
$$

수식에서 마지막 더해진 항이 residual connection 값이다.

encoder network에서 output값이 input값과 동일하게 하기 위해서 sequence의 양끝에 padding을 추가한다. 즉, 양 끝에 k-1 개의 0값을 추가한다. 그리고 output에서 마지막 k개의 원소를 제거해서 사용한다.

마지막으로 decoder의 output인 $h_i^L$에 weight를 곱한 후 bias를 더한 값에 softmax를 취해서 다음 token 값인 $y_{i+1}$을 얻어낸다.

$$
p(y_{i+1}\vert y_1,...,y_i,\mathbf{x})=\text{softmax}(W_oh_i^L+b_o)\in \mathbb{R}^T
$$

**Multi-step Attention**

해당 논문에서 decoder layer에 separate attention mechanism을 적용시켰다. attention 계산을 위해서 현재 decoder state vector에 이전 target element의 embedding값($g_i$)을 더해준다.

$$
d_i^l=W_d^lh_i^l+b_d^l+g_i
$$

decoder layer $l$의 attention 값인 $a_{ij}^l$은 현재 state $i$에 대한 source 원소인 $j$의 attention 값을 나타낸다. 계산은 decoder의 state summary 값인 $d_i^l$과 encoder의 output값인 $z_j^u$를 dot-product한 값을 softmax 취해주면 된다.

$$
a^l_{ij}=\frac{\exp(d_i^l\cdot z_j^u)}{\sum^m_{t=1}\exp(d_i^l\cdot z_t^u)}
$$

그리고 이 attention 값을 사용해서 decoder에 사용될 conditional input $c_i^j$를 계산한다. 이 때 attention 값을 encoder의 output 값에 가중평균하는데 encoder의 output만 사용하는 것이 아니라 input값이 embedding 값도 더해줘서 가중평균한다.

$$
c_i^l=\sum^m_{j=1}a_{ij}^l(z_j^u+e_j)
$$

이 attention 구조는 기존의 RNN에서 사용한 attention 과는 $z$뿐만아니라 embedding 값인 $e$를 사용했다는 점이 다르다. 여기서 사용한 attention을 생각해보면 encoder를 통해서 나온 값인 $z$를 key로 해석할 수 있고, 각 input의 embedding 값인 $e$를 value로 해석할 수 있다.

또 다른 해석은 $z$는 context에 대한 정보도 포함을 하고 있고, $e$는 하나의 token에 대한 정보를 담고 있으므로 두 정보를 모두 사용하는 것이다.

계산된 conditional input $c$는 간단하게 decoder layer의 output $h_i^l$에 더해서 사용한다.

attention은 multi hop구조로 적용된다. 즉 attention을 통해 나온 결과를 바로 결과로 만든는 것이 아니라 결과를 다시 또 attention을 거치게 하는 구조이다. 이러한 구조는 첫 attention을 통해 유용한 정보를 결정하고 이제 유용한 정보만을 사용해서 다시 attention을 적용하는 구조로 생각할 수 있다.

이때까지 나온 구조를 그림을 통해서 이해해보자. 우선 전체 그림은 다음과 같다.

![convs2s_archi1](https://i.imgur.com/cqA9lCO.jpg){: .center}

그림의 윗 부분이 encoder이고 아랫 부분이 decoder이다. 중간의 matrix는 attention 값들을 의미한다. encoder의 값들과 decoder의 값들을 사용해서 attention 값을 을 계산한다. encoder의 우측부분은 embedding vector 와 encoder output을 더하는 것을 의미한다. 이 값을 attention값과 가중 평균 해준다.

이제 encoder와 decoder를 나눠서 살펴보자. 이제부터의 그림은 stanford의 [seminar slide](https://nlp.stanford.edu/seminar/details/mauli.pdf)에서 나온 그림이다.

먼저 encoder 먼저 살펴보자.

![encoder](https://i.imgur.com/818EToJ.jpg){: .center}

convolution을 계산한 후 2개의 벡터로 나눠서 gated linear unit이 계산된다. 그리고 residual connection으로 연결된 모습을 볼 수 있다. 위 그림의 가장 상단의 결과가 $z$가 될 것이다.

이제 decoder의 그림을 보자.

![decoder](https://i.imgur.com/jVHBnSL.jpg){: .center}

우선 encoder와 마찬가지로 convolution이 계산되고 gated linear unit까지 계산되는 부분은 동일하다. 이제 그 결과를 encoder값과 같이 attention 값을 사용한다. 그리고 그 결과를 이전의 결과 값과 더해주고 마지막으로 residual connection까지 계산을 해주면 최종 출력이 나오는 모습이다.

하지만 해당 논문에서는 이런 single attention이 아닌 multi hop 구조를 사용했다고 했는데 이를 그림으로 표현하면 다음과 같을 것이다.

![multi_hop](https://i.imgur.com/D9BlwsO.jpg){: .center}

attention 까지 계산한 결과가 다시 input으로 들어가서 다시한번 attention을 계산하는 형태이다. 전체적인 multi hop 구조는 아래의 그림을 보면 좀 더 이해가 쉬울 것이다.

![fairseq](https://i.imgur.com/1VgwtYQ.gif){: .center}

전체적인 구조에 대한 설명은 여기까지이다. 이제 Normalization 방법과 initialize한 방법에 대해서 알아보자.

**Normalization Strategy**

해당 논문에서는 Batch Normalization은 사용하지 않고 weight nomalization을 사용했다. 논문의 말에 따르면 network의 전체적인 varience가 크게 변하지 않도록 initialize와 normalize에 신경을 썻다고 한다.

특히 residual block의 output과 attention을 scaling 해서 전체적인 variance를 크게 변하지 않도록 했다. residual block에는 $\sqrt{0.5}$를 곱해서 varience를 절반으로 줄였다.

그리고 attention 값인 conditional input $c_i^l$에는 전체 attention score가 균등 분포를 따른다고 가정하고 원래 크기에 맞춰주기 위해 $m\sqrt{1/m}$을 곱해서 scale up 했다.

그리고 multiple attention을 사용한 convolutional decoder에는 gradient 값도 사용한 attention 수만큼 scaling 했다.

**Initialization**

Residual connection과 같이 다른 layer의 값을 어떤 layer에 더하는 모델에는 초기값 설정이 매우 중요하다. 사용한 초기값 설정은 대부분 정규 분포를 따르도록 했으며, 평균 0 에 표준편차를 weight에 맞게끔 설정해줬다.

#### Experiment

**Datasets**

데이터셋은 간단하게 소개만 한다. 다음의 데이터셋들을 사용해서 실험을 했다.

* WMT'16 English-Romanian
* WMT'14 English-German
* WMT'14 English-French
* 그 외 몇가지 corpus

**Model Parameters and Optimization**

Parameter와 사용한 optimization에 대한 정보는 다음과 같다.

* Encoder 와 decoder에 512개의 hidden unit 사용.
* 모든 embedding은 512 dimension
* Nesterov's accelerated gradient 방법을 사용해서 학습
  * 하이퍼 파라미터인 모멘텀 값은 0.99로 하고 norm이 0.1을 넘지 않도록 만들었다.
* 학습률은 0.25로 설정했다.
  * 학습률은 학습이 경과하면서 계속 감소하도록 설정하고 그 값이 $10^{-4}$보다 작아지는 Epoch에서 학습을 멈추도록 설정
* 64 크기의 mini-batch 사용
* 가장 긴 sentnece의 길이는 GPU memory에 맞게 설정(64 mini batch에서 메모리가 허용하는 가장 긴 길이 사용)
* gradient는 mini-batch의 padding 값이 아닌 값들의 개수로 normalize함
* convolutional block의 input과 embedding에 dropout사용

#### Result

결과도 간단히 보고 넘어가자.

![13](https://norman3.github.io/papers/images/fairseq/f04.png)

![123](https://norman3.github.io/papers/images/fairseq/f05.png)

#### Conclusion

다른 모델들에 비해 상대적으로 최근에 나온 모델이고, 모델을 발표한 facebook에서 소스코드 또한 공개를 해서 성능 높은 모델을 접근성 높게 사용할 수 있다는 장점이 있다. 그리고 기존의 RNN이 대부분이였던 Sequence to sequence 모델에서 CNN만을 사용해서도 다른 모델보다 높은 성과를 보여줬다는 것은 새로운 모델도 계속해서 나올 것이라는 것을 예상해 볼 수 있다.

그리고 무었보다 이 모델의 sequence에 적용했을 때의 장점은 다른 RNN계열의 모델들 보다 병렬화가 쉽기 때문에 속도가 매우 빠르다는 것이 큰 장점이다.

---

오역 및 잘못된 내용이 있을 수 있습니다. 잘못된 부분 혹은 이해가 잘 안되는 부분은 댓글 혹은 메일로 말씀해주시면 감사하겠습니다!
