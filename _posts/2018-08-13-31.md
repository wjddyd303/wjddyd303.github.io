---
layout: post
title:  "Seq2seq (1): Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
date:   2018-08-13 13:47:35 +0900
categories: NLP
tag: NLP
---

최근의 기계 번역(Machine) 분야에서 주를 이루고 있는 Neural Machine Translation의 시초를 뽑으라고 하면 Sequence to Sequence 모델을 뽑을 것이다. Google이 발표한 논문인 'Sequence to Sequence with Neural Network'를 통해 본격적인 NML분야가 활발해졌다고 볼 수 있다.

따라서 이번 포스트에서는 sequence to sequence 모델에 대해서 알아 보도록 할 것이다. 앞서 말한 구글의 논문 이전에 최초로 sequence to sequence모델을 도입한 논문인 'Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation'([paper](https://arxiv.org/abs/1406.1078))에 대해 먼저 알아 본 후 다음 포스트를 통해 google의 'Sequence to Sequence with Neural Network'([paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf))에 대해 알아보도록 한다.

1. [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://reniew.github.io/31)
2. [Sequence to Sequence with Neural Network](https://reniew.github.io/35)

---

Sequence to sequence는 가장 많이 사용되는 기계번역 분야 뿐만 아니라 다양한 분야에서 사용된다. sequence를 사용하는 분야라면 어느 테스크에서든 사용할 수 있다. RNN을 기본으로한 모델인 이 모델은 뉴욕대의 조경현교수님께서 만드신 모델이다. 이 논문을 통해 seqeunce to sequence가 세상에 소개된 것 뿐만 아니라 새로운 RNN 구조인 GRU 또한 소개되어서 세상의 많은 관심을 받은 논문이다.

처음 sequence to sequence의 개념이 도입된 이 논문에서는 sequence to sequence라는 이름을 쓰지 않고 encoder-decoder로 표현되었다. 이제 논문을 보며 자세히 알아보도록 하자.


### Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation



#### Introduction

Neural network를 활용해서 Object detection분야와 speech recognition분야에서 많은 성과를 보여줬다. 뿐만 아니라 최근에는 NLP분야에서도 nerual network를 통해 많은 발전을 이뤄왔다. 그리고 기계 번역 분야에서 기존에 활용되던 Statical Machine Translation(SMT)분야에서의 Neural network의 접목을 통해 더 좋은 성능을 보여줬는데 이 논문에서도 SMT의 한 부분으로 Neural network를 사용하는 것을 보여줄 것이다.

앞으로 보여줄 모델을 RNN Encoder-Decoder라 부른다. 이 모델의 구성은 각각 encoder, decoder 역활을 하는 두개의 RNN으로 구성되어 있는데 encoder는 가변 길이의 sequence를 고정길이의 벡터로 만들고 decoder는 이렇게 seuquence를 vector로 표현한 것을 다시 가변 길이의 sequence로 바꿔준다. 이 두개의 네트워크는 주어진 sequence에 대해서 target sequence의 조건부 확률을 최대화 시키면서 학습된다. 그리고 부가적으로 메모리 사용량과 학습을 쉽게해줄 몇가지 정교한 hidden unit에 대해서도 소개할 것이다.

모델의 평가는 English-French 번역 결과에 대한 점수를 통해 평가한다. 이제 모델에 대해서 자세히 알아보자.


#### RNN Encoder-Decoder

앞서 말했듯이 *encode* 는 가변길이의 sequence를 고정길이의 벡터 representation으로 만드는 것이고 *decode* 는 고정길이의 벡터 representation을 다시 가변길이의 sequence로 만드는 것을 뜻한다. 확률적인 관점에서 이 모델은 input 값인 가변길이의 sequence에 대한 decoder에 의해 만들어진 가변길이의 sequence의 조건부 분포를 학습하는 것이다. 즉 아래의 확률 분포를 학습한다.

$$
p(y_1,..,y_{T'}~\vert~x_1,...,x_T)
$$

여기서 길이를 뜻하는 $$T$$와 $$T'$$은 다른 길이이다.

Encoder-Decoder의 그림은 다음과 같다.

![en-de](https://i.imgur.com/Lq8GkLL.jpg)


Encoder는 하나의 RNN이며, input sentence $$\mathbf{x}$$의 각 symbol을 순서대로 읽는다. 각 symbol을 읽을 때, 이 RNN 구조의 각 hidden state는 각 symbol값은 아래의 수식으로 계산된다.

$$
\mathbf{h}_{<t>}=f(\mathbf{h}_{<t-1>},x_t)
$$

전체 sequence에 대해 모든 symbol을 마지막 까지 다 읽은 후에는, 마지막 hidden state에서 전체 input sequence를 하나의 벡터 $$\mathbf{c}$$로 요약한다. 여기까지가 encoder에 대한 설명이다.

Decoder는 또 하나의 다른 RNN이다. 여기서는 RNN은 주어진 hidden state 값인 $$\mathbf{h}_{<t>}$$를 활용해서 다음 symbol인 $$y_t$$를 예측하면서 전체 output sequence를 만들어 내면서 학습된다. 여기서는 위의 encoder에서 사용된 일반적인 RNN수식으로 hidden state값을 계산하는 것이 아니라 이전의 output symbol인 $$y_{t-1}$$값도 사용한다. 즉 아래의 수식으로 계산된다.

$$
\mathbf{h}_{<t>}=f(\mathbf{h}_{<t-1>}, y_{t-1}, \mathbf{c})
$$

따라서 이 수식은 다음 symbol에 대한 조건부 분포로 나타내면 다음과 같다.

$$
p(y_t\vert y_{t-1}, y_{t-2},...,y_1,\mathbf{c})=g(\mathbf{h}_{<t>},y_{t-1},\mathbf{c})
$$

여기서 $f,g$는 활성화 함수이다.(e.g. softmax)

두 개의 부분으로 구성된 이 *RNN Encoder-Decoder* 는 다음의 조건부 log-확률을 최대화하면서 학습된다.

$$
\max_\theta\frac{1}{N}\sum^N_{n=1}\log p_\theta(\mathbf{y}_n\vert\mathbf{x}_n)
$$

$$\theta$$는 모델의 파라미터를 뜻하고 $$(\mathbf{x}_k, \mathbf{y}_k)$$는 학습 데이터의 input sequence, output sequence 쌍이다.

학습의 경우는 gradient-based 알고리즘을 사용해서 모델의 파라미터를 학습시켰다.

일단, RNN Encoder-Decoder가 학습되면 모델은 두개의 용도로 사용된다. 하나는 주어진 input sequence에 대해서 target sequence를 만드는 용도로 사용되었고, 또 다른 하나의 용도는 주어진 두개의 sequence 쌍(input sequence, output sequence)에 대해서 위에 주어진 확률 값을 사용해서 점수를 매기는 용도로 사용되었다.

#### Hidden Unit that Adaptively Remembers and Forgets

위의 모델 architecture 뿐만 아니라 이 구조를 위해 새로운 형태의 hidden unit을 사용했다. 먼저 general한 RNN 수식을 보자.

$$
\mathbf{h}_{<t>}=f(\mathbf{h}_{<t-1>},x_t)
$$

여기서 함수 $f$는 단순히 일반적인 activation 함수를 뜻할 수도 있고 LSTM의 수식으로 activate되는 것을 뜻할 수도 있다. 새로운 형태의 hidden unit이란 새롭게 정의한 이 함수를 뜻하는데 LSTM을 motive로 해서 만들어 졌으나, 훨신 계산이 쉽고 구현이 쉬운형태이다.

새롭게 정의한 hidden unit을 그림으로 표현하면 다음과 같다.

![hidden_unit](https://i.imgur.com/X7RnoXc.jpg)

이제 이 hidden unit에서 어떻게 activate되는지 알아보자. 먼저 reset gate인 $$r_j$$는 다음과 같이 계산된다.

$$
r_j=\sigma\big([\mathbf{W}_r\mathbf{x}]_j+[\mathbf{U}_r\mathbf{h}_{<t-1>}]_j \big),
$$

여기서 $$\sigma$$는 logistic sigmoid 함수이고 $$[.]_j$$는 벡터의 $$j$$번째 원소를 뜻한다. 그리고 $$\mathbf{x}$$ 와 $$\mathbf{h}_{t-1}$$은 각각 input과 이전 hidden state를 뜻한다. 마지막으로 $$\mathbf{W}_r$$과 $$\mathbf{U}_r$$은 학습해야 할 weight matrix이다.

그리고 또 하나의 gate인 update gate $$z_j$$는 다음과 같이 계산된다.

$$
z_j=\sigma\big([\mathbf{W}_z\mathbf{x}]_j+[\mathbf{U}_z\mathbf{h}_{<t-1>}]_j \big),
$$

이제 두 gate의 값을 사용해서 hidden unit 값을 계산한다.

$$
\begin{matrix}
h_j^{<t>}=z_jh_j^{<t-1>}+(1-z_j)\tilde{h}_j^{<t>}\\ \\
\tilde{h}_j^{<t>}=\phi\big([\mathbf{W}\mathbf{x}]_j+[\mathbf{U}(\mathbf{r}\odot\mathbf{h}_{<t-1>})]_ j \big)
\end{matrix}
$$

이 공식에 따르면 reset gate가 0에 가까워지면 hidden state는 이전 hidden state값을 무시하게 된다. 그리고 현재 input값으로만 reset되게 된다. 따라서 효과적으로 hidden state의 다음 앞으로의 state와 관계가 없는 정보는 제거(drop)할 수 있게 되기 때문에 더욱 압축적인 정보를 갖게 된다.

반면에 update gate는 얼마나 많은 정보를 이전 hidden state로 부터 가지고 올것인지를 결정하는데, 이것은 실제로 LSTM의 memory cell과 유사하게 동작한다. 따라서 오래된 정보를 RNN에게 전달할 수 있게 된다. 또한 adaptive variant of leaky-integration unit을 고려한다.

각 hidden unit은 개별의 reset과 update gate를 가지는데, 각 hidden unit은 다른 시간 scale에 대한 의존성을 통해 학습한다.(특성에 따라 long-term의 값을 사용하거나, short-term의 값을 사용한다)


#### Statistical Machine Translation

기존의 흔히 사용되던 statistical machine translation 시스템의 목표는 주어진 문장 $$\mathbf{e}$$에 대해translation function인 $$f$$를 찾는 것이다. 즉 아래의 식을 최대화 하기 위한 것이다.

$$
p(\mathbf{f}\vert\mathbf{e})\propto p(\mathbf{e}\vert\mathbf{f})p(\mathbf{f})
$$

하지만 실제로는 대부분의 SMT 시스템은 아래의 log식을 모델링한다.

$$
\log p(\mathbf{f}\vert\mathbf{e})=\sum^N_{n=1}w_n f_n(\mathbf{f},\mathbf{e})+\log Z(\mathbf{e})
$$

여기서 $$f_n$$과 $$w_n$$은 각각 n번째 feature와 weight이다. 그리고 $$Z(\mathbf{e})$$는 일반화 상수이다. 경우에 따라 여기서의 weight는 BLEU(bilingual evalutation understudy)값을 최대화 하기 위해 학습된다.

phrase 기반의 SMT는 어절 단위의 통계적 기계 번역으로 각 어절에 대해서 그에 대응하는 문장에 대한 확률로 나눠서 계산한다.

#### Experiments

실험은 WMT'14의 English/French 번역 task를 통해 진행되었다. 이 bilingual한 말뭉치는 Europarl(61M words), news commentary(5.5M words), UN(421M words)과 90M, 780M개의 crawled한 copora를 포함한다.

그리고 French language model을 학습하기 위대 대략 712M 개의 단어를 학습했다.

일반적으로 statistical한 모델을 학습시킬 때 여러 데이터들을 concat하는 것이 항상 최적의 성능을 보장하지 않을 뿐더러 큰 모델일수록 다루기 어렵기 떄문에 여기서는 data selection 방법(Moore and Lewis,2010)을 적용했다. 이러한 방법을 사용해서 Language modeling을 위해  418M개의 단어를 뽑았으며, 학습을 위해 348M개의 단어를 뽑았다.

데이터 셀렉션과 weight tuning을 위해 nestest2012 와 2013 사용했고 테스트를 위해서 nestest 22014를 사용했다. 각각의 set은 7만개 이상의 단어로 구성되어있다.

RNN Encoder-Decoder를 포함하는 이 네트워크를 학습하기 위해 source, target vocabulary의 크기를 각각 가장 많이 나오는 English, French 둘다 15,000개의 단어만 포함하도록 제한했다. 이 15,000개의 단어는 전체 단어의 93%에 달한다. 그리고 모든 out-of-vocabulary 단어는 ([UNK]) 토큰으로 바꾸었다.

실험 결과에 대해 baseline phrase-based SMT 시스템은 기본값의 Moses를 사용해서 만들었다. 이 시스템의 BLEU 스코어느 30.64, 33.3이다.

**RNN Encoder-Decoder in experiment**

실험에 사용된 RNN Encoder-Decoder의 구조는 다음과 같다.

* 1000 hidden unit(GRU 사용)
* input symbol에서 hidden unit으로 갈 떄 사용되는 input matrix는 2 lower-rank approxiamtion한 matrix 사용, output matrix로 같은 방식 사용
* rank 100 Matrix 사용
* hidden unit 계산 시 $$\tilde{h}$$ 계산할 때 활성화 함수로 tanh사용
* Decoder에서 hidden state 값으로 output 계산과정에서는 deep neural network 구조 사용(single intermediate layer, 500 maxout units,pooling 2 input)
* 모든 파라미터는 (0, 0.01)의 가우시안 분포로 초기화
* 파라미터 업데이트에는 SGD, Adadelta 사용(hyperparameter는 $$\epsilon=10^{-6}, \rho=0.95$$사용)
* 매 업데이트 마다 64개의 임의 추출된 phrase pair사용


실험에서는 target language를 학습하기 위한 전통적인 neural network 인 CSLM도 추가해서 실험했다. 그리고 추가적으로 word penalty 방식도 추가했다. 따라서 실험을 위한 모든 조합은 다음과 같다.

* Baseline
* Baseline + RNN
* Baseline + CSLM + RNN
* Baseline + CSLM + RNN + Word Penalty

실험에 대한 결과는 다음과 같다.

![table](https://i.imgur.com/N4rOQuB.jpg)

결과를 보면 우선 encoder-decoder방식을 사용하면서 성능이 올라갔으며, CSLM과 같이 사용할 때 더 성능이 올라갔다. 하지만 Word penalty를 적용시켰을 때 test에서 성능이 약간 떨어졌다.

그리고 BLEU 스코어 뿐만 아니라 Qulitative한 분석을 통해서 이 모델이 의미적, 그리고 문법적으로도 잘 분석한다는 것을 확인 할 수 있다.

![quali](https://i.imgur.com/i60LpP7.jpg)



---

긴글 읽어주셔서 감사합니다. 오역 및 잘못된 내용이 있을 수 있습니다. 잘못된 부분 혹은 이해가 잘 안되는 부분은 댓글 혹은 메일로 말씀해주시면 감사하겠습니다!
