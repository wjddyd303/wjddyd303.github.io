---
layout: post
title:  "NLP를 위한 CNN (5): Character-level Convolutional Network for Text Classification"
date:   2018-08-09 13:47:35 +0900
categories: NLP
tag: NLP
---

NLP에서 활용되는 Convolutional Network에 대해서 논문 하나씩 알아보도록 한다. 전체 List는 다음과 같다.

1. [Understanding CNN for NLP](https://reniew.github.io/25/)
2. [Convolutional Neural Network for Sentence Classification, 2014](https://reniew.github.io/26/)
3. [A Convolutional Neural Network for Modelling Sentences, 2014](https://reniew.github.io/27/)
4. [A Sensitivity Analysis of Convolutional Neural Networks for Sentence Classification](https://reniew.github.io/28/)
5. [Character-level Convolutional Networks for Text Classification](https://reniew.github.io/29/) [현재글]

---

### Character-level Convolutional Network for Text Classification

다섯 번째로 소개할 NLP를 위해 CNN을 활용한 모델은 Xiang Zhang, Junbo Zhao, Yann LeCun의 [Character-level Convolutional Network for Text Classification](https://arxiv.org/pdf/1509.01626.pdf)이다.

이때까지의 Convolutional neural network를 활용한 모델들은 input값의 최소단위를 단어(embedded word vector)였다. 하지만 이번 논문에서는 character(글자)단위의 convolutional neural network를 사용해서 문서 분류 문제를 해결하려 한다. 문서 분류문제에서 주류를 이뤘던 모델들은 보통 word2vec으로 임베딩된 단어 벡터들 그리고 TFIDF 정보 혹은 n-gram 정보들을 취합한 Bag of Word 모델들이 주를 이뤘다. 해당 논문에서는 단어보다 좀더 raw한 정보인 글자에 주목한다. 좀 더 근본적인 언어 구조의 특징을 뽑아내려는 시도이다. Charater단위를 ConvNet에 적용시킨 최초의 논문인 만큼 어떤 내용을 담고 있는지 살펴보도록 하자.

#### Introduction

만약 매우 큰 데이터셋으로 학습한다면 ConvNet은 단어에 대한 정보를(통사론, 의미론적이 정보를 포함한) 필요로 하지 않는다. 그리고 이렇게 character 단위로 만들어진 모델은 조금의 수정으로도 여러 언어에 적용 될 수 있고, 오타 혹은 이모티콘 또한 일반적인 단어와 마찬가지로 잘 학습될 수 있다는 장점이 있다.

#### Character-level Conovolutional Networks

해당 단원에서는 character-level ConvNet의 design에 대해서 소개한다. 해당 모델은 모듈식으로 구성되어 있다.

**Key-Module**

가장 핵심적인 부분은 temporal convolutional 모듈이다. 이 모듈은 1-D Convolutional 을 계산한다. discrete한 input 함수를 $$g(x)\in[1,l]\rightarrow\mathbb{R}$$라 하고 discrete한 kenel 함수를 $$f(x)\in[1,k]\rightarrow\mathbb{R}$$라 하자. 그러면 Convolution은 $$h(y)\in[1,\lfloor(l-k)/d\rfloor+1]$$라는 함수로 표현될 수 있고, Conovlution은 모든 input에 대해 kenel이 모두 계산되는 것이므로 함수는 아래와 같이 정의 된다.

$$
h(y)=\sum^k_{x=1}f(x)\cdot g(y\cdot d-x+c)
$$

위에서 사용된 상수 $$c$$는 $$c=k-d+1$$로 offset 상수 이다. Vision에서 흔히 사용되는 Convolutional network처럼 이 모듈은 kernel 함수들의 집합을 파라미터로 가진다. input $$g_i(x)$$에 대해서 output $$h_j(y)$$로 갈 때 계산되는 파라미터를 다음과 같이 정의한다.

$$f_{ij}(x) ,i=1,2,...,m~\&j=1,2,...,n$$

위의 파라미터를 *weight* 라 부른다. 그리고 위의 input, output값인 $$g_i,h_j$$는 *feature* 라고도 불리며, 전체 길이에 해당하는 $m, n$을 feature 라 한다. 즉 다시말해 output $$h_j(y)$$는 모든 $i$에 대해서 $$g_i(x)$$와 $$f_{ij}(x)$$를 convolution 연산한 것을 더하면서 얻어진다.

그리고 이 깊은 모델을 학습하는데 또 중요한 모듈은 max-pooling 모듈이다. input 함수를 $$g(x)\in[1,l]\rightarrow\mathbb{R}$$라 하고 max-pooling 함수를 $$h(y)\in[1,\lfloor(l-k)/d\rfloor+1]\rightarrow\mathbb{R}$$이라 하면 이 함수는 다음과 같이 정의 된다.

$$
h(y)=\max^k_{x=1}g(y\cdot d-x+c)
$$

여기서도 $c$는 offset 상수인 $$c=k-d+1$$이 된다. 이러한 max-pooling 모듈 덕분에 전체 모델은 총 6 layer만큼 깊어 질 수 있다.

>*(수식으로 설명되서 어렵게 느껴질 수 있는데 우리가 흔히 아는 convolution과 padding을 생각하면 된다,)*


그리고 non-linearity를 위한 함수로는 ReLU 함수를 사용했다. 그리고 학습과정에서는 128 크기의 minibatch로 Stochastic gradient descent(SGD)를 사용했으며 모멘텀을 사용해 update했다. 모멘텀 사용 시 모멘텀 상수는 0.9로 하였고 학습률은 0.01 시작해서 3에폭마다 절반으로 줄이는 방식을 사용했다. 그리고 각 에폭은 각 class에 대해 동일하게 추출된 특정 크기의 데이터를 뜻하고 이 값에 대해서는 각 dataset에 따라서 다르므로 뒤에서 설명한다.


#### Character quantization

모델에서 인코딩된 글자(character)들의 sequence를 input값으로 받았다. 여기서 인코딩은 m개의 알파벳에 대해 one-hot 인코딩 방식을 사용했다. 따라서 각 input은 m-dimension의 벡터가 된다. 만약 알파벳에 들어가지 않는 문자에 대해서는 0벡터로 만든다. 그리고 특정 길이까지만 input으로 입력받는데, 길이를 넘어가는 값에 대해서는 무시한다. 이 모델에서는 알파벳을 총 70개의 문자로 정의한다. 알파벳에 속하는 문자는 26개의 영어 문자, 10개의 숫자, 그리고 33개의 특수문자이다. 전체 알파벳은 다음과 같다.

![alpha](https://i.imgur.com/eA7X9Hs.jpg)

여기서는 영어 문자에 대해서는 소문자만 받도록 했는데 나중에 소문자와 대문자를 구별하는 것과도 비교할 것이다.

#### Model Design

2개의 ConvNet을 design했다. 하나는 많은 feature를 가지는 ConvNet이고 하나는 적은 feature를 가지는 ConvNet이다. feature의 개수를 제외하고는 다른 부분은 모두 동일하다. 전체 모델은 총 9개의 layer이고 그 중 6개는 convolutional layer이고 3개는 fully-connected layer이다.

![conv](https://i.imgur.com/IXY0weI.jpg)

세부적인 사항에 대해서 살펴보면 input의 feature의 수는 70이다. 앞서 설명한 encoding 방식을 사용하면 한 문자당 70 dimension의 vector가 되기 떄문이다. 그리고 앞서 말한 것 처럼 특정 길이까지의 문자만 입력으로 받는데 여기서는 1,014개의 문자까지만 입력으로 받는다. 논문에 따르면 이 정도 길이면 글의 주된 내용은 모두 잡아낼 수 있다고 한다. 그리고 앞서 말한 것처럼 feature의 수가 다른 2개의 ConvNet을 design 했다고 했는데, Large feature는 Convolution을 통해 총 1024의 feature를 가지는 Convolution을 수행하고 small feature는 256의 feature를 갖도록 convolution을 수행했다. 즉 다른 필터의 사이즈를 사용했다고 이해하면 된다. 아래의 표는 각 Convolutional layer의 값들을 설명했다. (참고로 stride는 1이고 pooling과정에서 overlap되는 부분이 없도록 한다)

![feat](https://i.imgur.com/INO7psM.jpg)
![feat2](https://i.imgur.com/uKq3tc4.jpg)




그리고 fully-connected layer 사이에 dropout을 2번 사용했다. dropout 확률은 0.5로 설정했다. 그리고 가중치 초기화는 가우시안 분포를 따르도록 하고 분포의 평균과 분산은 큰 모델에 대해서는 (0, 0.02)로 작은 모델은 (0, 0.05)로 설정했다.

#### Data Augmentation using Thesaurus

데이터 증가는 일반적으로 데이터가 많이 필요한 deep learning 에서는 매우 중요한 부분이지만, image나 speech 분야와는 달리 text에서의 데이터 증가는 문자의 순서가 매우 중요할 수 있는데, 이런 언어의 규칙을 데이터를 증가시키면서 손상시킬 수 있기 때문에 조심스럽다. 따라서 여기서 사용한 데이터 증가 방식은 단어나 특정 문자들을 유사어로 대체시키는 방법을 사용했다.(English thesaurus를 사용했다.)

#### Comparison Models

비교를 위한 모델로는 전통적인 NLP 방식과 Deep Learning 방식 두가지 모두 사용해서 비교했다. 각 모델은 다음과 같다.

**Traditional Methods**

* Bag of words and its TFIDF
* Bag of ngrams and its TFIDF
* Bag of means on word embedding

**Deep Learning Methods**

* Word based ConvNets
* Long short term memory


#### Dataset and Result

비교를 위해 사용한 데이터셋은 다음과 같다.

* AG's news corpus
* Sogou news corpus
* DBPedia ontology dataset
* Yelp reviews
* Yahoo! AAnswers dataset
* Amazon reviews

데이터셋에 대한 세부적인 설명은 생략한다. 6개의 데이터셋 중에서 앞의 3개는 비교적 작은 크기의 데이터셋이고 뒤의 3개는 큰 데이터셋이다.

그리고 결과는 다음과 같다. 각 모델에 명명에 대해서 설명하면 Lg 와 Sm은 Large와 small을 의미하고, Lk는 Look-up table을 사용한 것이다. 그리고 Th는 앞서 말한 Thesaurus를 의미한다.

![result](https://i.imgur.com/hc6ZN78.jpg)

이제 결과에 대해서 몇 가지만 요약하면 다음과 같다.


* 글자 단위의 Convolutional Network도 문서 분류에서 높은 성능을 보인다.
* 작은 데이터셋에서는 전통적인 NLP방식이 DL방식보다 더 높은 성능을 보인다.
* ConvNet은 사용자가 만든 데이터에서 좋다.(오타를 잘 잡는다)
* Alphabet의 선택에 따라 성능이 많이 달라진다.
* Bag-of-means 모델은 안좋다.
* 모든 데이터셋에 있어 최적의 모델은 없다.( 많은 실험을 통해 데이터셋에 가장 적합한 모델을 찾아야 한다)


---

긴글 읽어주셔서 감사합니다. 오역 및 잘못된 내용이 있을 수 있습니다. 잘못된 부분 혹은 이해가 잘 안되는 부분은 댓글 혹은 메일로 말씀해주시면 감사하겠습니다!
