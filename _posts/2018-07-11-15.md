---
layout: post
title:  "Chater 5: Neural Laguage Models"
date:   2018-07-11 13:47:35 +0900
categories: Lecture
tag: NLP
---

### Chapter 5

### Neural Language Models

이 내용은 Newyork University의 조경현교수님의 DS-GA 3001강의 lecture note중 Neural Laguage Models단원을 정리한 내용입니다.


<br>
### 5.1 Language Modeling

Machine이 우리의 언어를 이해하는 방법은 어떻게 될까?
가장 basic한 방법으로는 문장들이 얼마나 유사(Likeliness)한지에 대해 이해하는 것이다. 아래의 두 문장을 보자.

> "Colorless green ideas sleep furiously"  
> "Jane and me went to see a movie yesterday"

첫 번째 문장은 문법적으로 완벽한 문장이다. 하지만 문장을 사람이 이해하려해도 어떤 의미인지 쉽게 받아 들여지지 않는다. 단어들끼리의 유사도가 매우 안맞는 문장이다. 그에 반해 두 번째 문장은 문법적으로는 잘못된 문장이다. me 대신 i가 들어가야 하지만, 저 문장을 이해하는데는 어려움이 없이 쉽게 이해 된다.

따라서 우리는 기계에게 문장을 이해시키는 방법으로 유사도를 통한 의미적인 부분을 학습시킬 것이다. 물론, 문법적인 부분도 해야 하겠지만 이는 어려운 문제이다.


#### 5.2 What if those linguistic structures do exist

Language Modeling에 대해 통계학적 접근법은 문법적으로 완벽한 문장을 유사하지 않다고 결론 지을 수 있다. 이러한 문제는 문장들간의 유사도에 대해서 다루는 문제를 확률 모델로 만들 수 있을 것이다. 주어진 문장 $S$가 있다고 하자, 이때 확률 $P(S)$는 어떻게 구할 것인가?
(여기서 $P(S)$는 간략하게 유사도와 문법적 정확도를 같게 보는 것으로 해석할 것이다.)
 우선 $S$에 의해 만들어지는 잠재적 언어학 구조를 $G$라 가정하자. 아래와 같은 식을 얻을 수 있다.

$$p(S,G) = p(S|G)p(G)$$

$$p(S)=\sum_G p(S, G)$$

하지만 여기서 $G$는 정해진 것이 아니므로, 무한할 수 있다. 따라서 실제 계산을 위해서는 Lower bound를 사용해 아래와 같은 식을 사용해 $p(S)$를 근사한다.

$$p(S)=\sum_G p(S, G) \ge p(S,\hat G)$$

$$where\;\hat G = argmax_G p(S,G) = argmax_G p(G|S)$$

하지만 이러한 모델링에도 문제점이 있다. 첫 째로, $G$를 사용하는 것에 대해 명백하지 않다. 둘 째로는 이러한 형식들이 의미적으로 불확실하다.

이제부터는 model-free한 접근법에 대해서 알아볼것이다.

#### 5.1.2 Quick Note on Linguistic Units

 *"언어학적 최소 단위가 무었일까?"*
위 질문에 대한 대답은 음소(phoneme)가 될 것이다. 하지만 "음소가 언어를 이해할 수 있는 단계인가?" 에 대해서는 아니라고 할 것이다.
이러한 low-level의 unit들 (음소, 글자)는 의미를 수반하지 않기 떄문에 이해할 수 있는 단계가 아니다. 따라서 우리는 한 단어를 최소 단위로 의미를 이해할 수 있다. 그러나 여기에도 문제가 있다. '어떤것을 단어로 볼 것인가?' 보통은 띄어쓰기(sequence of non-blank characters)를 기준으로 단어를 구분한다. 그러나 구두점과 같은 것들이 나오면 단어에 대한 정의가 애매해진다.
> "hello," , "hello.", "hello!", "hello?", ""hello"", "'hello'"
> 위의 hello들은 모두 같은 단어이지만 구두점 때문에 다른 의미로 해석될 수 있다.

뿐만 아니라 중국어 같은 언어에서는 띄어쓰기가 사용되지 않아서 위와 같은 정의를 적용시킬 수 없다.
따라서 우리는 이 최소단위에 대한 정의에 대한 고민을 해야 할 것이다. 이 분야는 물론 지금도 계속 연구되고 있고 답이 없는 분야이다. 즉, character와 word 사이의 적절한 liguistic unit이 있는지를 찾아야 한다.

### 5.2 Statical Laguage Model

Liguistic unit에 상관없이, 어떤 문장$S$는 $T$개의 각각의 symbol들로 구분될 수 있다.

$$S=(w_1, w_2, ... ,w_T)$$

각 symbol들은 vocabulary라는 가능한 모든 symbol이 모여있는 집합의 원소이다.

$$V= \{v_1, v_2, ... , v_{|V|}\}$$

Laguage Modeling 문제는 Sentence에 대해 확률 $P(S)$를 할당하는 model을 찾는 문제이다.
그러나 우리는 확률 분포에 대한 정보가 없을 것이고, Data로 부터 학습을 해야 할 것이다.
$D$를 $N$개의 Sentence를 가지고 있는 Data라 하자.

$$D=\{S^1, S^2, ... S^N\}$$

$$S^n = (w_1^n, w_2^n, ... w_{T^n}^n)$$

$T^n$이라는 표현은 각 sentence들의 길이가 다르다는 것을 의미한다.

이제 주어진 Data $D$에 대해 Sentence $S$의 확률을 다음과 같이 정의하자.

$$p(S) = \frac{\sum_n^NI_{S=S^n}}{N} $$

여기서 $I$는 indicator function으로 동일한 Sentence면 1, 아니면 0을 주는 함수이다.

$$I_{S=S^n}=\bigg\lbrace\begin{matrix} 1, & \text{if}\ \ S=S^n \\ 0, & \text{otherwise} \end{matrix}$$

위의 확률은 sentence $S$가 Data 내에서 몇번 나왔는지와 같다.

#### 5.2.1 Data Sparsity/Scarcity

Laguage model에서 가장 중요한 issue는 corpus의 크기가 얼마나 큰가이다. 즉 dataset의 크기가 중요한데, 실제 세계의 사용가능한 모든 문장을 data로 가지고 있을 수는 없다.
$|V|$개의 symbol을 가지는 vocabulary가 있다고 하자. 각 sentence의 최소한 $T$개의 symbol가진다. 이러한 vocabulary는 충분히 큰 크기가 될 것이다(100k ~ 1M). 그럼에도 불구하고, 많은 실제로 사용할만한 문장임에도 불구하고 corpus에 포함되지 않을 수 있다.
그 예를 한번 찾아보자. Google Books Ngram Viewer라는 서비스는 Google Books의 문장들이 모두 모여있는 거대한 corpus를 나타낸다. 여기서 "I like llama"라는 문장을 검색해보자, 이 문장은 충분히 사용할만한 문장임에도 불구하고 결과가 나오지 않는다.
![3123123](https://i.imgur.com/keV0Uh7.jpg)
이전 part에서 정의한 sentence에 대한 확률을 생각해보자, "i like llama"라는 문장은 corpus에 존재하지만, 많이 등장하지는 않을 것이다. 따라서 데이터셋이 매우 크므로 확률은 거의 0에 수렴할 것이다. 이러한 문제를 *data sparsity* 라 부른다.
즉 trainning set이 전체 input space를 cover하지 못하는 경우를 뜻한다.


### 5.3 n-Gram Language Model

Data Sparsity 문제는 sentence의 최대 길이가 커질수록 악화된다. 우리는 이 사실에 착안해서 straightforward한 접근법을 사용한다. (*limit the maximum length of phrases/sentences we estimate a probability on*)
이러한 idea는 n-Gram language model의 base가 되었다.
n-Gram 모델에서는 위에서 정의한 sentence에 대한 확률을 재정의한다.

$$p(S)=p(w_1,w_2,...w_T)=p(w_1)p(w_2|w_1)\dotsm p(w_k|w_{<k})\dotsm p(w_T|w_{<T})$$

여기에서 $w_{<k}$는 k 번째 symbol 이전의 모든 symbol들을 뜻한다. 아래의 식과 같다.

$$p(w_k|w_{<k})\approx p(w_k|w_{k-n},w_{k-n+1},...,w_{k-1})$$

따라서 Sentence의 확률이 다음과 같이 정의된다.

$$p(S)\approx \prod_{t=1}^Tp(w_t|w_{t-n},...w_{t-1})$$

위 식이 의미하는 것은 sentence내의 symbol은 이전의 $n-1$개의 symbol들에 의해 predictable하다는 것이다.
예를 들어 "In Korea, more than half of all the residents speak Korean" 문장을 보면 마지막의 'Korean'이라는 단어는 사람과 언어 두가지 의미를 가지는데, 그 이전의 speak과 Korea라는 단어에 대해 상대적인 확률로 생각을 해보면 언어를 뜻하는 것이 더 타당하다는 것을 알 수 있다. 즉 n-Gram 모델을 사용하면 이전의 symbol에 대한 정보를 가지고 현재 symbol의 의미를 파악하기 때문에 유용하다는 것을 알 수 있다.
위의 예를 통해 보면, 확률 추정의 정확도와 $n$의 크기에 따른 통계적인 효율은 trade-off 관계에 있다는 것을 알 수 있다. 즉, $n$의 값이 커질수록, conditional distribution은 더 나은 결과를 만들 것이지만 data sparsity는 커져 정확도가 떨어질 수 있다.M

**n-gram Probablity Estimate**

n-gram conditional probabilty는 trainning corpus를 통해 계산할 수 있다. 아래의 식을 보자.

$$p(w_k|w_{k-n},w_{k-n+1},...,w_{k-1})=\frac{p(w_{k-n},...w_{k-1},w_k)}{p(w_{k-n},...,w_{k-1})}$$

여기서 분모는 다음과 같이 계산할 수 있다.

$$p(w_{k-n},...,w_{k-1})=\sum_{w^\prime\in V}p(w_{k-n},...,w_{k-1},w^\prime)$$

$$p(w_{k-n},...,w_{k-1},w^\prime)\approx \frac{c(w_{k-n},...w_{k-1},w^\prime)}{N_n}$$

여기서 $c(\cdot)$가 의미하는 것은 trainning corpus에서 주어진 n-gram의 등장 횟수이다. 그리고 $N_n$은 trainning corpus에서의 모든 n-gram의 수 이다.
마지막으로 다음과 같이 식을 정리한다.

$$p(w_k|w_{k-n},...,w_{k-1})=\frac{\cancel{\frac{1}{N_n}}c(w_{k-n},...,w_{k-1},w^\prime)} {\cancel{\frac{1}{N_n}}\sum_{w^\prime\in V}c(w_{k-n},...w_{k-1},w^\prime)}$$


#### 5.3.1 Smoothing and Back-Off

n-gram model의 가장 큰 issue는 어떤 n-gram을 포함하는 sentence에 대해 얼마다 다른 n-gram들과 유사한지와 상관없이 0의 확률을 갖는 것이다.
예시를 통해 이러한 경우에 대해 살펴보자.
* "I like llama which is a domesticated South American camelid"

위의 문장에 대한 확률은 다음과 같이 계산될 것이다.

$$\begin{align*}
&p(\text{"I","like","llama","which","is","a","domesticated","South","American","camelid"}\\
&=p("\text{I}")p("\text{like}"|"\text{I}")\underbrace{p("\text{llama}"|"\text{I}","\text{like}")}_{=0}\dotsm p("\text{camelid}"|"\text{South}","\text{American}")\\
&=0
\end{align*}
$$

위 식과 같이 특정 n-gram 확률 때문에 전체가 0이되는 문제를 해결하기 위해 corpus의 크기를 키우면 되지 않을까 라고 생각한다. 하지만 이러한 "data sparsity" 문제는 corpus의 크기와 상관없이 statistical modeling에서 항상 발생하는 문제이다. 그렇다면 어떻게 해결해야 할 것인가?
방법은 기존에 존재하지 않는 n-gram에 대해 작은 특정 값을 갖도록 하는 것이다. 그러면 기존에 나오지않은 n-gram이더라도 특정 값을 가지므로 0이 되는 문제는 발생하지 않을 것이다. 식을 아래와 같이 구현함으로써 이 방법을 사용할 수 있다.

$$
\begin{align*}
p(w_k|w_{k-n},...,w_{k-1})&=\frac{\alpha+c(w_{k-n},...,w_{k})}{\sum_{w^{\prime}\in V}(\alpha+c(w_{k-n},...,w^\prime))}\\
&=\frac{\alpha+c(w_{k-n},...,w_{k})}{\alpha|V|+\sum_{w^{\prime}\in V}c(w_{k-n},...,w^\prime)}
\end{align*}
$$

$\alpha$는 scalar 값으로 보통 다음의 범위로 정한다. $0<\alpha\le1$

 *unseen n-gram* 을 위와 같이 해결하는 방법을 smoothing 기법(부족한 데이터를 해결하는 방법)이라 표현하고 위와 같이 특정 scalar값을 더해주는 방법을 Additive Smoothing 혹은 Lidstone Smoothing이라고 표현한다. 그 중 $\alpha$값으로 1을 지정하는 방법은 Laplace Smoothing이라고 한다.(Laplace는 한번도 안나온 데이터에 대해 최소 한번은 나왔다고 지정하는 방법)
 하지만 직관적으로 봐도 위 방법은 효율일 잘 안나올 것 같다. unseen n-gram들에 대해 모두 다 똑같은 빈도수를 준다는 점에서 문제가 발생한다.

 이에 대한 해결책은 n-gram 확률을 interpolation 방법을 통해 smoothing하는 것이다. 수식은 다음과 같이 정의한다.

$$
\begin{align*}
p^S(w_k|w_{k-n},...,w_{k-1})=&\lambda(w_{k-n},...,w_{k-1})p(w_k|w_{k-n},...,w_{k-1})\\
&+(1-\lambda(w_{k-n},...,w_{k-1}))p^S(w_k|w_{k-n+1},...,w_{k-1})
\end{align*}
$$   

위 식에 따르면 n-gram 확률은 이전 n-gram확률에 의해 재귀적으로 계산되는 것을 볼 수 있다. 이제 중요한 것은 $\lambda$값을 어떻게 지정해야할까 이다. 간단한 방법은 데이터에 맞추는 방법이다. 즉 데이터에서 출현빈도에 초점을 맞추는 방법이다. $\lambda$값까지 정한뒤 일반화하면 다음과 같다.

$$
p^S(w_k|w_{k-n},...,w_{k-1})=
\begin{cases}
\alpha(w_k|w_{k-n},...,w_{k-1}),~~\text{if}~~c(w_{k-n},...,w_{k-1},w_k)>0\\
\gamma(w_{k-n+1},...,w_k)p^S(w_k|w_{k-n+1},...,w_{k-1}),~otherwise
\end{cases}
$$

여기서 $\alpha$값과 $\gamma$값의 선택에 따라 여러 techniques으로 나뉜다.

대표적인 smoothing 방법은 KN smoothing과 GT smoothing 방법이다. 이에 대한 설명은 생략한다.

#### 5.3.2 Lack of Generalization

n-gram 모델이 대부분의 상황에서 잘 동작하지만, 몇몇 특수한 상황에서 아쉬운 상황을 생긴다. 그 이유 중 하나는 Lack of Generalization인데, 여기서 말하는 Generalization은 아래의 예를 보며 이해를 하자.

* Chases a rabbit
* Chases a cat
* Chases a dog

세 문장을 보면 우리는 Chases라는 단어 뒤에 오는 단어에 대한 패턴을 인식할 수 있다. 즉 사람은 일정한 단어들을 보고 그것들의 상위개념들을 이해할 수 있다. 하지만 n-gram 모델에서는 이런 상위개념들로 일반화시키는 것을 할 수 없다.

만약 n-gram 모델에게 이런 일반화 과정들을 이해시키려면 각 단어의 정의에 대한 사전을 이해시키거나 각 단어의 정확한 뜻을 알려주는 방법인데 이 방법을 해결한다는 것은 정의를 text로 알려주는 한 결국 Language를 이해시키는 것과 똑같기 때문에 불가능 하다.

다음 장에서는 이런 Lack of Generalization문제를 어느정도 해결하는 방법에 대해서 살펴보도록 하자.

### 5.4 Neural Language Model

n-gram 모델은 현재 단어 이전의 n개의 단어에 대해 현재 단어에 대한 조건부 확률을 계산하는 것이였다. 즉 n개의 단어에 대해 확률을 구하는 함수를 찾는 것이다.

$$
p(w_k|w_{k-n},...,w_{k-1})=f^{w_k}_\theta (w_{k-n},...,w_{k-1})
$$

위 함수를 정의하기 위해 가장 먼저 해야 할 것은 input값을 정의하는 것이다. 사전의 단어에 대한 정보를 최소화 해야하기 때문에 우리는 one-hot encoding 방식을 사용할 것이다. input은 다음과 같이 정의된다.

$$
w_i=[0, 0,...,1,...0]^\intercal\in{0,1}^{|V|}
$$


 이제 일반적인 Neural Net과 비슷한 방식으로 계산한다. 우선 간 단어 벡터에 가중치 행렬( $E\in\mathbb{R}^{\|V\| \times d}$ )을 곱해서 연속된 vector들을 구하자.


$$
p^j=E^\intercal w^j
$$

input값이 one-hot vector였던 것을 생각하자. 가중치 행렬과 곱해질 때 $w_k$는 1이 1개만 있으므로 가중치 행렬의 k번째 행과만 계산될 것이다. 따라서 가중치 행렬은 다음과 같이 각 벡터들의 모음으로 작성할 수 있다.

$$
E=
\begin{bmatrix}
e_1\\
e_2\\
\vdots\\
e_{|V|}
\end{bmatrix}
$$

따라서 input과 가중치 행렬의 곱은 다음과 같이 정의된다.

$$
E^\intercal w_i = e_i
$$

이 식이 의미하는 바는 가중치 행렬의 i행 vecter인 $e_i$는 단어들중 i번째 단어를 포현하는 벡터로 해석될 수 있다는 점이다. 따라서 $e_i$벡터를 input값으로 사용할 것이다. 이제 input값이 n개의 vector를 concat한 뒤 non-linear하게 만들어 주면 모델링 과정이 끝난다.

$$
\mathbf{p}=[p^1;p^2;...;p^n]^\intercal
$$

$$
\mathbf{h}=\tanh(\mathbf{Wp+b})
$$

최종적으로 모델링을 사용한 조건부 확률은 다음과 같이 정의된다.

$$
p(w_n=k|w_1,...w_{n-1}) =\mu_k=\frac{\exp(u_k^\intercal h+c_k)}{\sum^{|K|}_{k'=1}\exp(u_{k'}^\intercal h+c_{k'})}
$$

#### 5.4.1 How does Neural Language Model Generalize to Unseen n-Grams? - Distributional Hypothesis

이때까지 Neural language model에 대해서 소개했다. 이번에는 주로 unseen n-gram을 generalize하는 방법에 대해서 알아보도록 한다.

이전의 neural language model을 두개의 합수의 합성으로 만들어보자. ($f\circ g$) 먼저 $f$는 context word의 순열 또는 n-1개의 앞선 단어를 벡터로 만들어 주는 함수라고 한다.

$$
f:\{0,1\}^{|V|\times n-1}\rightarrow \mathbb{R}^d
$$

각 단어에 대해 $f$함수를 통과시킨 결과로 나온 $d$ 차원의 벡터를 $\mathbf{h}$라 하고 context vector라 부른다. 두 번째 단계인 $g$는 continuous vector인 $\mathbf{h}$를 단어 확률로 만들어 주는 함수이다. 이 단계에서 affine 변환과 softmax함수를 통한 일반화 과정이 포함된다. 이전에 사용된 확률 함수인 $g$를 다시 한번 살펴보자.

$$
\frac{\exp(u_k^\intercal h+c_k)}{\sum^{|K|}_{k'=1}\exp(u_{k'}^\intercal h+c_{k'})}
$$

간단한 형태로 보기위해 bias인 $c$를 빼고 확인해보자.

$$
\frac{\exp(u_k^\intercal h)}{\sum^{|K|}_{k'=1}\exp(u_{k'}^\intercal h)}
$$

$k$ 번째 확률은 output vector인 $u_k$(output matrix $\mathbf{U}$의 $k$ 번째 행)가 context vector인 $\mathbf{h}$와 잘 맞을 경우 커질 것이다. 즉 다음 단어로 k 번째 단어가 등장할 확률은 context vector인 $\mathbf{h}$와 해당 word vector인 $u_k$와 내적한 값에 비례할 것이다.

두 개의 context vector $\mathbf{h}_j$와 $\mathbf{h}_k$가 있다고 하자. 각 문맥은 단어들 간의 유사도(다음 단어의 조건부 분포가 서로 얼마나 유사한지)에 따라 결정된다.

context vector 를 고려한 특정 target 단어 $w_l$에 대한 확률을 보자.

$$
\begin{matrix}
p^l_j=p(w_l|h_j)=\frac{1}{Z_j}\exp(\mathbf{w}_l^T \mathbf{h}_j)\\
p^l_k=p(w_l|h_k)=\frac{1}{Z_k}\exp(\mathbf{w}_k^T \mathbf{h}_j)
\end{matrix}
$$

두 context(문맥)에 대한 확률의 비율은 다음과 같다.

$$
\frac{p^l_j}{p^l_k}=\frac{Z_K}{Z_j}\exp(\mathbf{w}_l^T(\mathbf{h}_j-\mathbf{h}_k))
$$

위 식을 보면, 두 context에 대한 확률이 같아질려면($\frac{p^l_j}{p^l_k}$) 아래의 식을 만족시켜야 한다.

$$
\mathbf{w}_l^T(\mathbf{h}_j-\mathbf{h}_k)=0
$$

위 식을 만족시키는 경우에 대해서 알아보자. 우선 단어 벡터인 $\mathbf{w}$는 0이 아니라 가정한다. 위 식이 0이 되려면 두 context 벡터의 값이 같아져야 한다. 즉 다시 말해, 두 context 벡터값이 서로 유사해져야 한다는 뜻이다.(이를 테면 유클리디언 거리가 가깝다) 이러한 상황이 의미하는 것이 무엇일까? 이것은 neural language model은 다른 주변의 n-gram은 먼 지점으로 투영하면서 동일한 단어에 따라 나오는 (n-1)-gram을 context 벡터 공간에서 비슷한 지점으로 투영해야 한다. 즉 이는 같은 단어에 대해 비슷한 확률을 주기 위해 필요하다. 만약 이와 같이 동일한 단어에 대해 context vector 공간의 다른 지점으로 투영한다면 우리가 예측해야할 다음 단어에 대한 확률이 다르게 나올 것이고 이는 좋지 않은 language model이다.


위의 설명에 대해 직관적인 이해를 위해 극단적인 예를 한번 들어보자. 아래의 세 문장을 보자

* There are **three teams** left for the qualification
* **four teams** have passed the first round
* **four groups** are playing in the field

굵은 글씨들을 보자. 이 bigram의 각 첫 번째 단어가 context word이다. 그리고 neural language model은 이 context 단어 뒤에 나오는 단어의 확률을 예측해야 한다.

여기서 neural laguage model은 "three"와 "four"를 context space의 비슷한 지점으로 투영해야 할 것이다. 즉 이 두 단어의 context vector는 "teams"에 대한 비슷한 확률을 만들어야 한다. 그리고 각 target 단어인 "teams"와 "groups"은 각 vector가 유사해야 할 것이다.

이제 위의 세 문장을 학습시킨 neural language 모델에서 unseen n-gram인 "three group"에 대한 확률을 할당하는 것을 생각해보자. 위 문장으로 학습시킨 모델은 "three"의 context vector 와 "four"의 context vector는 context 공간에서 비슷한 지점으로 투영시킬 것이다. 따라서 "three"의 context vector에 의해 다음 단어가 "group"일 확률을 높게 계산할 것이다.

이러한 과정을 통해 neural language 모델은 unseen n-gram 에 적당한 확률값을 줄 수 있다. 위의 예를 통해 확인할 수 있는 점은 neural laguage 모델은 자동으로 다른 context vector들 사이의 유사도를 측정할 수 있다는 점이다.(target word가 다르더라도, 동시 등장확률에 의해)

실제 세계에서 우리의 언어는 "Distributional Hypothesis"를 만족하는 것은 자명하다. 즉 유사한 의미를 갖는 단어는 비슷한 위치에 등장한다. 따라서 단어들이 같이 등장하는 단어들을 관찰하면 단어의 잠재 의미를 뽑아낼 수 있을 것이다.

#### 5.4.2 Continuous Bag-of-Words Language Model: Maximum Pseudo-Likelihood Approach

"*왜 우리는 language modeling을 할 때 앞선 단어만 고려하는 것일까?*" "*앞선 단어만 의존한 단어 분포가 적합한 가정일까?*"

위 두개의 질문에 대한 대답은 "*꼭 그럴 필요는 없다*" 가 될것이다. 우리는 특정 단어에 대해 앞선 단어가 아닌 해당 단어 앞 뒤로 $n$개의 단어를 고려하는 모델을 만들 수 있다. 이러한 모델이 *Markov random field language model* 이다.

![markov](https://i.imgur.com/BYTtwE8.jpg)

Markov random field language 모델 (MRF-LM)에서 주어진 문장에서 각 단어는 랜덤 변수 $w_i$로 말할 수 있다. 그리고 둘러싼 $2n$개의 각 단어를 비방향 선(undirected edges)으로 각 단어를 연결할 수 있다. 여기서 연결된 선은 조건부 의존성(conditional dependency) 구조를 의미한다. 위의 그림이 n=1 일때의 MRF-LM을 그림으로 나타낸 것이다. 이 경우에 Markov random field의 확률은 각 그래프의 꼭지점(clique)의 potential 값들의 곱으로 표현된다. 여기서 potential이란 각 꼭지점에서 각 input 값이 random 변수인 positive 함수를 의미한다.

MRF-LM에서 두개의 랜덤 변수의 꼭지점을 제외한 모든 꼭지점에는 1의 potential을 할당한다.(즉, pairwise potential만을 사용한다) $i$ 와 $j$ 단어의 pairwise potential은 다음과 같이 정의된다.

$$
\phi(\mathbf{w}^i,\mathbf{w}^j)=\exp\big((\mathbf{E}^T\mathbf{w}^i)^T(\mathbf{E}^T\mathbf{w}^j)\big)
$$

여기서 $E$는 위에서 나왔던 가중치 행렬을 의미한다. 그리고 $\mathbf{w}^i$는 $i$번 째 단어의 one-hot vector이다. 위 식은 많은 pairwise potential 중 하나이고 다른 많은 경우도 있다.(가령 dot product로 정의 할 수도 있다)

위의 식과 함께 전체 Sentence의 확률은 다음과 같이 정의된다.

$$
p(w_1,w_2,...,w_T)=\frac1Z \prod_{t=1}^{T-n}\prod_{j=t}^{t+n}\phi(\mathbf{w}^t,\mathbf{w}^j)=\frac1Z \exp(\sum^{T-n}_{t=1}e^T_{w^t}e_{w^j})
$$

여기서 $Z$는 일반화 상수이다. 이 값은 potential의 곱을 확률로 만들어준다. 이 값을 전체 문장에 대해 계산하는건 불가능하기 떄문에 주어진 condition의 단어들에 대해 각 단어에 대해 조건부 확률을 계산한다.

조건부 확률을 계산할 떄는 각 단어에 대해 Markov blanket이라 불리는 부분에 포함된 단어의 값에 의해 계산된다. 여기서 Markov blanket이란 각 단어를 둘러싼 $n$개의 단어를 뜻한다. 즉 각 단어의 앞의 $n$개 뒤의 $n$개의 단어를 본다. 따라서 조건부 확률은 다음과 같이 정의 될 것이다.

$$
p(w^i|w^{i-n},...,w^{i-1},w^{i+1},...,w^{i+n})=\frac{1}{Z'}\exp\bigg(\mathbf{e}^T_{w^i}\bigg(\sum^n_{k=1}\mathbf{e}^T_{w^{i-k}}+\sum^n_{k=1}\mathbf{e}^T_{w^{i+k}}\bigg)\bigg)
$$

여기서 일반화 상수인 $Z'$은 다음과 같이 계산된다.

$$
Z'=\sum_{v\in V}\exp\bigg(\mathbf{e}^T_v\bigg(\sum^n_{k=1}\mathbf{e}^T_{w^{i-k}}+\sum^n_{k=1}\mathbf{e}^T_{w^{i+k}}\bigg)\bigg)
$$

앞서봤던 neural language 모델과 굉장히 비슷하다는 것을 볼 수 있다. 이러한 조건부 확률은 얕은(shallow) neral network와 같다. input이 context 단어들인 한개의 linear한 은닉층을 가지고, output이 중심 단어에 대한 조건부 확률이 되는 것이다. 이 network를 그림으로 표현하면 다음과 같다.

![cbo](https://i.imgur.com/KBcqdfp.jpg)

우리는 이미 전체 문장에 대한 확률 $p(w^1,...,w^T)$을 직접 계산하는 것은 $Z$ 게산량이 많아 어렵다는 것을 알고 있다. 다행이 상대적으로 계산이 쉬운 조건부 확률을 계산하는 것을 확인했다. 전자의 경우는 log-likelihood를 최대화하는 것과 같고, 후자의 경우에는 pseudo-likelihood를 최대화 하는 것과 같다.

따라서 MRF-LM의 Pseudo-likelihood는 다음과 같다.

$$
\log \text{PL}=\sum^T_{i=1}\log p(w^i|w^{i-n},...,w^{i-1},w^{i+1},...,w^{i+n})
$$

위의 Pseudo-likelihood를 최대화 하는 것은 위의 그림의 network를 학습하는 것과 동일하다. 즉 주변 단어에 대한 중심 단어의 확률을 높이는 것이다.

위의 식을 잘 계산한다해도 아직 전체 문장에 대한 확률을 계산하는 좋은 방법은 없다. 어떤 특정한 상황에서는 pseudo-likelihood 값이 maximum likelihood 값에 수렴하기도 하지만 그렇다고 조건부 확률들의 곱이 전체 문장의 확률을 대체할 수 있는 것은 아니다. 그렇지만 다행이도 MRF-LM 모델을 사용한다면 pseudo-probability도 다른 문장의 점수를 줄 수 있다. 앞서서 우리가 처음 neural language 모델에 대해 처음 봤을 떄는 각 단어의 조건부 확률을 첫 단어부터 마지막까지 곱하면 주어진 문장의 확률을 구할 수 있다는 내용과는 대조적이다. 이러한 결과가 MRF-LM을 laguage 모델로 잘 사용하지 않는 이유 중 하나일 것이다. 하지만 이 단원에서 처음으로 이 복잡한 모델을 설명한 이유가 있다.

Continuous bag-of-words(CBOW)에서 처음 소개된 이러한 접근법은 흥미로운 특징을 발견했다. CBOW 모델의 한 부분인 학습된 단어 임베딩 행렬 $\mathbf{E}$가 단어의 잠재 구조를 매우 잘 반영한다는 것이다. 결국 이 모델은 최근 자연어 처리 분야에서 가장 각광받는 기술이 되었다. 다음 장에서 이 부분에 대해 더욱 다뤄볼 것이다.

**Skip-Gram and Implicit Matrix Factorization**

Markov에 의해 CBOW모델 뿐 아니라 skip-gram이라 불리는 모델 또한 소개되었다. skip-gram모델은 CBOW의 반대되는 모델이라 생각하면 된다. 주변 $2n$개의 단어에 의해 중심단어를 예측한는 CBOW와는 반대로 skip-gram의 경우는 중심 단어를 통해 주변 $2n$개의 단어를 예측하는 모델이다. 그리고 실제로 두 모델에 의해 생성된 단어 vector 중 skip-gram에 의해 만들어진 vector가 좀 더 효과가 좋다는 것이 결론이다. 물론 좋은 단어 vector를 판단하는 것은 논란의 여지가 있는 부분이지만 많은 "intrinsic"한 평가에서 skip-gram 모델이 더 좋은 것으로 보여진다.
위 기술들을 만든 Markov는 negative sampling으로 skip-gram모델을 학습시키는 것은 positive point-wise mutual information matrix(PPMI) 을 2 lower-dimensional matrix로 만드는 것과 동일하다고 한다.


#### 5.4.3 Semi-Supervised Learning with Pretrained Word Embeddings

위에 나온 n-gram language model, neural language modellanguage model, continuous bag-of-words 모델들에서 중요한 점은 모든 모델이 unsupervised 하다는 점이다. unsupervised가 중요한 이유는 각 데이터에 대해 label을 필요로 하지 않는다. 이러한 점이 다른 modeling에 비해 statical한 modeling이 language modeling에 더욱 적합한 이유이다. 그리고 label이 필요없기 때문에 학습 시킬 수 있는 데이터는 무한히 많이 존재한다는 장점이 있다. 그리고 소개된 modeling중에 embedding된 word vector는 자연어 처리 분야에서 최근에 대부분이 사용하게 됬다.

하나의 예를 들어보자. 영어 단어를 긍정과 부정으로 나누는 문제이다. 예를 들어 "happy"는 긍정이고, "sad"는 부정으로 분류하는 문제이다. 긍정 부정 각각 1개씩 총 2개의 데이터만 존재한다고 하자. 어떻게 classifier를 만들 것인가?

우선 두 가지 이슈가 있다. 먼저 input을 어떻게 표현할지를 선택해야한다. 기본적으로는 one-hot vector로 input을 넣고 output을 위해 softmax layer를 사용할 것이다. 그러나 여기에도 문제가 아직 남아있다. 학습을 위한 데이터가 오직 2개 뿐이라는 것이다.

이러한 문제를 해결하기 위해 한가지 가정을 한다. 비슷한 input값은 비슷한 감정을 가진다는 가정을 한다. 이러한 가정은 semi-supervised 학습의 핵심이다. 이것을 높은 차원의 데이터를 효과적으로 저차원으로 옮기는 것이다. 이러한 과정을 통해 저차원의 데이터를 가지고 좋은 모델을 만들 수 있다.

이제 사전 학습된 단어 벡터를 사용한다고 하자. 그러면 다음과 같은 nearest neighbour(NN) 분류기를 만들 수 있다.

$$
NN(w)=
\begin{cases}
\text{positive,~~~~if~}\cos(\mathbf{e}_w,\mathbf{e}_{\text{happy}})>\cos(\mathbf{e}_w,\mathbf{e}_{\text{bad}})\\
\text{negative,~~~otherwise}
\end{cases}
$$

여기서 $$\cos(\cdot ,\cdot )$$은 아래의 식인 cosine similarity를 의미한다.


$$
\cos(\mathbf{e}_i,\mathbf{e}_j)=\frac{\mathbf{e}_i^T\mathbf{e}_j}{\|\mathbf{e}_i\|\|\mathbf{e}_j\|}
$$

이 연산을 통해 우리는 '유사도'라는 성질을 이용한다. 이 방법을 통해 사전 학습된 단어 벡터는 대부분의 문제를 해결할 수 있지만, 경우에 따라 사용하는데 주의해야 한다. 이러한 단어 벡터는 어떤 objective function을 최대화 하면서 만들어진다. 실제로 유사도에는 다양한 측면에서 적용되지만 이렇게 학습된 단어 벡터는 몇 개의 특정한 측면의 유사도만 적용된다. 몇 가지 측면은 학습할 때의 데이터에 따라 달라진다.

예를 들어 "happy", "sad", "angry"와 같은 감정을 표현하는 단어를 continuous bag-of-words의 문맥에서 생각해보자. 이러한 감정을 표현하는 단어들은 "feel"이라는 단어와 같이 등장하는 경우가 많다. 따라서 위와 같은 단어들은 "feel"이후에 등장할 확률이 높도록 학습될 것이다. 즉 각각의 감정을 표현하는 단어가 비슷한 벡터 공간에 존재할 수 있다. 그러나 감정 분석적인 측면에서는 이러한 표현은 좋지 않다.

만약 적은 데이터로 언어와 관련된 문제를 해결하려 한다면, 사전 학습된 벡터를 사용하는 것을 고려해보도록 한다. 그러나 해결해야 하는 문제를 고려해서 학습된 벡터를 사용해야 한다.  

### 5.5 Recurrent Laguage Model

Neural Language modeling은 일반적인 n-gram모델의 일반화 부족(lack of generalization)문제를 해결한다. 하지만 여전히 n번째 단어는 이전 n-1 개의 단어를 보는 Markov 속성을 가정한다. 즉 이전에 들었던 예시인 "In Korea, more than half of all the residents speak Korea"이라는 문장에서 처럼 마지막 단어에 대한 조건부 확률 분포는 문장의 2번째단어, 즉 10개 전인 단어에 대해 추정하는 것이 효율적이다.

4.1.4 단원에서 배웠던 것을 다시 생각해보자. 우리는 가변 길이의 문장을 읽고 가변 길이의 output을 만드는 recurrent neural network에 대해 배웠다. POS-tagging 을 하는 이전에 봤던 예시를 다시 보자. input은 다음과 같다.

$$
x=(\text{Children},~\text{eat},~\text{sweet},~\text{candy})
$$

그리고 target output 은 품사의 sequence이다.

$$
y=(\text{noun},~\text{verb},~\text{adjective},~\text{noun})
$$

각각의 예측되는 tag들이 서로 독립이라는 가정을 제거하기 위해 각 step의 예측 값인 $$Y_t$$의 값이 다음 step의 input 값인 $$X_{t+1}$$과 함께 다음 step에서 계산된다.

![recurrent](https://i.imgur.com/cDsUgXm.jpg)

전체 문장 확률을 계산하는 과정에서의 단일 단어에 대한 확률계산하는 것을 생각해보자.($a$)

$$
p(w^1,w^2,...w^T)=\prod^T_{t=1}\underbrace{p(w^t|w^1,...,w^{t-1})}_{(a)}
$$

이때까지 배웠던 내용을 토대로 우리는 input $$(w^1,...,w^{t-1})$$ 에 대해 위의 조건부 확률($a$)을 neural network를 통해 구할 수 있다.(앞서 배운 것과 input이 가변길이라는 점만 다르고 나머지는 모두 같다)

하지만 여기서는 가변 길이의 input 문장을 summarizing/memorizing 할 수 있는 recurrent neural network를 사용해 본다. recurrent neural network는 input 문장 (w^1,...,w^{t-1})를 메모리 상태인 $$\mathbf{h}^{t-1}$$로 summarizing한다.

$$
\mathbf{h}^{t'}=
\begin{cases}
\begin{matrix}
0,&\text{if~}t'=0\\
f(\mathbf{e}_{w^{t'}},\mathbf{h}^{t'-1}),&\text{otherwise}
\end{matrix}
\end{cases}
$$

여기서 $$t'$$은 $$0$$부터 $$t-1$$까지 계산된다. $$f$$는 이전에 배웠던 recurrent 함수 중 선택해서 사용하면 된다.(GRU, LSTM) 그리고 $\mathbf{e}_{w'}$은 단어 $w'$의 벡터 이다.

마지막 과정은 어떻게 해야 하는 지 기억 할 것이다.

$$
\mu = \text{softmax}(\mathbf{V}\mathbf{h}^{t-1})
$$

이 계산결과로 나온 $\mu$는 모든 단어에 대한 확률 벡터가 된다. 이 과정에서 input 문장을 단 한번만 읽어서 계산한다. 즉 각 setp에서 단어 하나만 읽어서 update한다.

이러한 lagugage model을 recurrent neural netwrok language model(RNN-LM)(b)이라 부른다.

![rnn1](https://i.imgur.com/HBTm3bC.jpg)*(a) A Recurrent neural network, (b) Recurrent neural network language model*



### 5.6 How do *n*-gram language model, neural language model and RNN-LM compare?

마지막으로 이제 하나의 질문이 남았다. 이 때까지 배운 language model 중 어떤 model을 실전에서 선택해서 사용해야 하는 가? 이다. 우선 이 질문에 대답하기 위해 먼저 일반적으로 language model들을 평가하는 방법에 대해 얘기해 보자.

가장 일반적으로 많이 사용하는 측정법은 *perplexity* 이다. model $\mu$에 대해 perplexity PPL은 다음과 같이 계산한다.

$$
\text{PPL} = b^{-\frac{1}{N}\sum^N_{n=1}\log_b p_\mu(w_n|w_{<n}) }
$$

$N$은 validation/test 말뭉치의 모든 단어의 수 이고, $b$는 상수로 보통 2 또는 10의 값을 사용한다.

이 값이 의미하는 것은 무었일까? 이 값에 대해 정보이론을 바탕으로 자세히 설명된 내용이 있지만 이 값을 전부 우리가 이해할 필요는 없다.

exponential 함수는 단조 증가 함수이므로 위 함수에서 $log_b$ 대신 자연 로그를 사용하더라도 괜찮다.(b>1 이라 가정한다)

$$
-\frac{1}{N}\sum^N_{n=1}\log p_\mu(w_n|w_{<n})
$$

이 함수는 cost 함수 혹은 negative log-likelihood와 매우 유사하다. summation 안의 항에 대해서만 보자.

$$
\log p_\mu(w_n|w_{<n})
$$

이 값은 language model $\mu$이 주어진 이전 단어에 대해 정확한 현재 단어를 예측하면 높은 값을 가진다. (log는 단조 증가이기 떄문)

요약하면 perplexity를 측정하는 것은 language model이 test/validate 말뭉치에 대해 정확히 예측한 평균치 값을 의미한다. 따라서 더 나은 language model은 적은 perplexity 값은 갖는 model이다.

이제 우리는 language model을 비교할 준비가 되었다. 다음의 3가지의 model을 생각해보자.

* count-based *n*-gram language model
* neural *n*-gram language model
* recurrent neural netowrk language model

이렇게 여러 모델을 비교할 때 가장 큰 어려움은 제어하기 어려운 요소들이다. 예를들면 다음과 같다.

* Language
* Genre/Topic of training, validation and test corpora
* Size of a training corpus
* Size of a language model

이러한 어려움 때문에 모델들의 비교는 종종 특정한 downstream application에서 진행한다. 이런 downstream application은 가능한 크기, 말뭉치 target language, language model의 size등 많은 제약을 가진다. 예를 들어 [Pragmatic neural language modelling in machine translation](https://arxiv.org/abs/1412.7119) 에서는 n-gram과 neural language model을 다양한 근사 기술들을 통해 기계 번역 분야에서 비교했다. 그리고 [From feedforward to recurrent lstm neural networks for language
modeling](https://www.lsv.uni-saarland.de/fileadmin/teaching/seminars/ASR-2015/DL-Seminar/From_Feedforward_to_Recurrent_LSTM_Neural_Networks_for_Language_Modeling.pdf)에서는 저자는 세개의 language model을 자동 음성 인식분야에서 비교했다.

먼저 [From feedforward to recurrent lstm neural networks for language
modeling](https://www.lsv.uni-saarland.de/fileadmin/teaching/seminars/ASR-2015/DL-Seminar/From_Feedforward_to_Recurrent_LSTM_Neural_Networks_for_Language_Modeling.pdf)의 결과를 살펴보자.

![comp](https://i.imgur.com/lJ1tbue.jpg)

결과를 보면 RNN-LM이 일반적인 neural language model을 사용하는 것과 비교해서 효과적이다. 특지 LSTM을 사용할 때는 다른 모델에 비해 성능 향상이 눈에 띈다. 그리고 같은 모델 안에서도 model의 크기를 키우면 성능이 좋아진느 것도 볼 수 있다. 그리고 perplexity도 더 큰 language 모델이 적은 값을 가졌고 여러 모델 중에서는 RNN-LM이 적은 값을 가졌다.

이러한 결과를 통해 보면, neural model 또는 recurrent model이 language modeling 할 때 좋은 후보가 될 수 있다. 그리고 많은 논문에서의 관찰을 통해 보면 count-based n-gram과 neural, recurrent 모델들을 같이 사용할 때 좋은 결과를 보여줬다. 즉 hybird한 model에서 각 model이 잠재적인 언어 구조를 잘 잡아 냈다는 것이다. 그러나 아직 어떻게 다른 구조를 잡아내는 지에 대해 밝혀지진 않았다.
