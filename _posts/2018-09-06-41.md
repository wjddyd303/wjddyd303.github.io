---
layout: post
title:  "CS20(TensorFlow) Lecture Note (11): RNNs in the TensorFlow"
date:   2018-09-06 13:47:35 +0900
categories: TensorFlow
tag: tensorflow
---

ìŠ¤íƒ í¬ë“œì˜ TensorFlow ê°•ì˜ì¸ cs20 ê°•ì˜ì˜ lecture noteë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ê°•ì˜ëŠ” ì˜¤í”ˆë˜ì§€ ì•Šì•„ì„œ Lecture note, slide ìœ„ì£¼ë¡œ ì •ë¦¬ëœ ê¸€ì„ì„ ì°¸ê³  í•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ê°•ì˜ì˜ ìì„¸í•œ Syllabus ë° ìë£Œë“¤ì„ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”.

[CS20: TensorFlow for Deep Learning Research](http://web.stanford.edu/class/cs20si/)


---


##### Post list

* [Lecture 1, 2: Overview & TensorFlow Operation](https://reniew.github.io/32)
* [Lecture 3: Linear and Logistic Regression](https://reniew.github.io/33)
* [Lecture 4: Eager execution and interface](https://reniew.github.io/34)
* [Lecture 5: word2vec + manage experiments](https://reniew.github.io/36)
* [Lecture 6, 7: Intro to ConvNet & ConvNet in TensorFlow](https://reniew.github.io/38)
* [Lecture 8: CNN(Style transfer), TFRecord ](https://reniew.github.io/39)
* [Lecture 10: Variational Auto Encoders(VAE)](https://reniew.github.io/40)
* [Lecture 11: RNNs in the TensorFlow](https://reniew.github.io/41)
* [Lecture 12: Machine Translation, Seqeunce-to-sequence and Attention](https://reniew.github.io/42)

---

### 11. RNNs in the TensorFlow

ì´ë²ˆ lectureì—ì„œ ë°°ìš¸ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

* From feed-forward to recurrent
* Tricks & treats
* Presidential tweets


#### From feed-forward to Recurrent Neural Networks(RNNs)

ì§€ë‚œ ëª‡ë…„ê°„ feed-forward network ë¶€í„° convolutional neural networkëŠ” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ê³  ë§ì€ ë¬¸ì œì— ì ìš©ì‹œì¼œì„œ ì—„ì²­ë‚œ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤¬ë‹¤.

ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì•„ì§ë„ ì´ëŸ¬í•œ feed-forward networkì™€ convolutional neural network ëª¨ë¸ë¡œëŠ” ì ìš©ì‹œí‚¤ê¸° ì–´ë ¤ìš´ ë¬¸ì œë“¤ì´ ì•„ì§ ë§ì´ ìˆì—ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ì˜ ê°€ì¥ í° ì´ìœ ëŠ” ëª¨ë¸ì— ì ìš©ì‹œí‚¬ ìˆ˜ ìˆëŠ” dataê°€ singularí•œ í˜•íƒœë§Œ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ì–¸ì–´ë‚˜ ìŒì•…ê³¼ ê°™ì€ sequenceë°ì´í„°ë¥¼ ì ìš©ì‹œí‚¤ê¸°ì—ëŠ” ë§ì€ ì–´ë ¤ì›€ì´ ìˆì—ˆë‹¤. ë”°ë¼ì„œ ìœ„ì˜ ë¬¸ì œì— ì´ì–´ì„œ ì´ëŸ° sequencialí•œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ëª¨ë¸ì— ëŒ€í•´ ì—°êµ¬ê°€ ë§ì´ ì´ë¤„ì¡Œë‹¤.

ì´ëŸ¬í•œ ì—°êµ¬ì˜ ê²°ê³¼ë¡œ ë‚˜ì˜¨ê²ƒì´ RNNì´ë‹¤. RNNì€ sequentialí•œ ì •ë³´ë¥¼ ì¡ì•„ë‚´ê¸° ìœ„í•´ ë§Œë“¤ì–´ ì¡Œê³  ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ RNNì¸ Simple Recurrent Network(SRN)ì€ Jeff Elmanì— ì˜í•´ ë§Œë“¤ì–´ì¡Œë‹¤.

RNNì€ feed-forwardì˜ unitê³¼ ë˜‘ê°™ì€ ì—°ì‚°ì„ í•˜ëŠ” unitì´ ì ìš©ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ unitë“¤ì´ ê³„ì†í•´ì„œ ì—°ê²°ë˜ì–´ ìˆë‹¤ëŠ” ì ì´ë‹¤. Feed-forwardì˜ ê²½ìš° inputì— ì˜í•œ ì‹ í˜¸ëŠ” ê³„ì†í•´ì„œ í•œë°©í–¥ìœ¼ë¡œ ì´ì–´ì§€ê³ , loopì€ ë§Œë“¤ì–´ ì§€ì§€ ì•ŠëŠ”ë‹¤. ê·¸ì— ë°˜í•´ RNNì€ loopì´ ìƒê¸°ê³  neuronë“¤ì´ ê°ì ìŠ¤ìŠ¤ë¡œ ì—°ê²°ëœë‹¤. ì¦‰ ì´ì „ì˜ neuronì´ ë˜ ì˜†ì˜ neuronì— ì˜í–¥ì„ ì¤€ë‹¤.

![9999](https://i.imgur.com/TZZgorB.jpg)

ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ RNNì¸ simple recurrent networks(SRN)ì€ Elman networkì™€ Jordan networkë¥¼ ëœ»í•œë‹¤.

* Elman Network, Jordan Network

$$
\begin{align*}
h_t &= \sigma_h(W_h x_t + U_h h_{t-1}+b_h)\\
y_t &=\sigma_y(W_y h_t+b_y)
\end{align*}
$$

* $x_t$ : input vector
* $h_t$ : hidden layer vector
* $y_t$ : output vector
* $W,~U$ : and $b$ : parameter matrices and vector
* $\sigma_h$ and $\sigma_y$ : Activation functions

![992](https://i.imgur.com/SnG0uQ0.jpg)

ëŒ€ë¶€ë¶„ì˜ ì‚¬ëŒë“¤ì€ RNNì„ NLPì˜ í•œ ë¶„ì•¼ë¼ ìƒê°í•œë‹¤. ê·¸ë„ ë‹¹ì—°í•  ê²ƒì´ ì–¸ì–´ê°€ ë§¤ìš° ëŒ€í‘œì ì¸ sequentialí•œ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë‚˜ NLPë¶„ì•¼ ì™¸ì—ë„ audio, image, videoë“± ë§ì€ ë¶„ì•¼ì—ì„œë„ RNNì€ ì‚¬ìš©ëœë‹¤. ê°€ë ¹ MNISTì˜ ê²½ìš°ì—ë„ ì ìš©í•  ìˆ˜ ìˆë‹¤. ì´ ë•ŒëŠ” ê° imageë¥¼ pixelë“¤ì˜ sequenceë¡œ ì ìš©ì‹œí‚¨ë‹¤.

#### Back-propagation through Time(BPTT)


Feed-forwardì™€ Convolutional Networkì—ì„œëŠ” errorë¥¼ back-propagationì„ í†µí•´ loss ê°’ì´ ëª¨ë“  layerì— ì „ë‹¬ ë˜ì—ˆë‹¤. ì´ëŸ° ë°©ë²•ì„ í†µí•´ parameterë“¤ì„ updateì‹œì¼°ë‹¤.

RNNì—ì„œëŠ” errosëŠ” lossê°’ì´ ëª¨ë“  timestepì— ì „ë‹¬ëœë‹¤. ì•ì„  ë‚´ìš©ê³¼ ë‘ê°€ì§€ í° ì°¨ì´ì ì´ ìˆë‹¤.

* feed-forwardì˜ ê° layerëŠ” ê°ì ìì‹ ì˜ parameterë¥¼ ê°€ì§€ëŠ” ë°˜ë©´ RNNì—ì„œëŠ” ëª¨ë“  timestepë“¤ì´ parameterë“¤ì„ ê³µìœ í•œë‹¤. ë”°ë¼ì„œ ëª¨ë“  timestepì˜ gradient ê°’ë“¤ì„ ëª¨ë‘ í•©ì³ì„œ parameterë¥¼ updateí•˜ëŠ”ë° ì ìš©ì‹œì¼°ë‹¤.
* feed-forwardì˜ ê²½ìš° ê³ ì •ëœ ìˆ«ìì˜ layerë¥¼ ê°€ì§€ëŠ” ë°˜ë©´ RNNì€ ì„ì˜ì˜ timestep ìˆ˜ë¥¼ ê°€ì§„ë‹¤.

ì•„ë˜ì˜ ì°¨ì´ì ì„ ë³´ì. ë§Œì•½ì— sequenceê°€ ë§¤ìš° ê¸¸ì–´ì§„ë‹¤ë©´, back-propagationì€ ëª¨ë“  time-stepì—ì„œ ê³„ì‚°ë˜ëŠ”ë° ì´ ê³„ì‚°ëŸ‰ì´ ë§¤ìš° ë§ì•„ì§ˆ ê²ƒì´ë‹¤. ë˜ë‹¤ë¥¸ ë³¸ì§ˆì ì¸ ë¬¸ì œëŠ” gradient ìì²´ê°€ ë§¤ìš° ì»¤ì§€ê±°ë‚˜ ë§¤ìš° ì‘ì•„ì ¸ì„œ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ê°€ ìƒê¸´ë‹¤.(vanishing/exploding gradients)

![9282](https://i.imgur.com/K53Co1t.jpg)

ëª¨ë“  timestepì—ì„œ ëª¨ë“  parameterë¥¼ updateí•´ì„œ ê³„ì‚°ëŸ‰ì´ ë§¤ìš° ë§ì•„ì§€ëŠ” ìƒí™©ì„ í”¼í•˜ê¸° ìœ„í•´ ë³´í†µ updateì‹œí‚¤ëŠ” timestepì˜ ìˆ˜ë¥¼ ì œí•œì‹œí‚¤ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.(truncated BPTT)

TensorFlowì—ì„œ RNNì€ unrolledëœ ë²„ì „ì˜ networkë¥¼ ì‚¬ìš©í•œë‹¤. ì¦‰ ì •í™•íˆ ëª‡ ê°œì˜ timestepì„ ì‚¬ìš©í• ì§€ë¥¼ ì •í•´ì¤˜ì•¼ í•œë‹¤ëŠ” ëœ»ì´ë‹¤. RNNì˜ íŠ¹ì„±ì„ ìƒê°í•´ë³´ë©´ ì´ëŸ¬í•œ êµ¬í˜„ ë°©ë²•ì€ í° ì œì•½ì´ ëœë‹¤. inputì˜ ê²½ìš° ê¸¸ì´ê°€ ì¼ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ ì •í•´ì§€ì§€ ì•Šì„ ìˆ˜ë„ ìˆê¸° ë–„ë¬¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ì—¬ëŸ¬ textë¥¼ ë‹¤ë£¨ëŠ”ë° í•˜ë‚˜ì˜ textëŠ” 20ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ë˜ì§€ë§Œ ë˜ ì–´ë–¤ textëŠ” 200ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•˜ë‚˜ì˜ ë°©ë²•ì€ dataë¥¼ ë‚˜ëˆ ì„œ ê°ê° ë‹¤ë¥¸ bucketìœ¼ë¡œ ë„£ëŠ” ê²ƒì´ë‹¤. ì´ bucketì—ëŠ” ë¹„ìŠ·í•œ í¬ê¸°ì˜ sequenceê°€ ë“¤ì–´ê°„ë‹¤. ë§Œì•½ becketë³´ë‹¤ ê¸¸ì´ê°€ ì§§ë‹¤ë©´ paddingì„ ì´ìš©í•˜ë©´ ëœë‹¤.

#### Gated Recurrent unit(LSTM and GRU)

ì‹¤ì œë¡œ RNNì„ ì‚¬ìš©í•´ë³´ë‹ˆ ê¸°ëŒ€ì™€ëŠ” ë‹¬ë¦¬ Long-termì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ëª»ì¡ì•„ë‚´ëŠ” ê²ƒì´ ë°í˜€ì¡Œë‹¤. ì´ëŸ° ê²°í•¨ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒì´ LSTMì´ë‹¤. ì´ëŸ¬í•œ LSTMì˜ ê°œë°œì€ ì‚¬ì‹¤ ì˜¤ë˜ì „ì— vanishing gradient ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ ì¡Œë˜ ê²ƒì´ë‹¤.

LSTMì˜ unitì€ gating mechanismì´ë¼ ë¶ˆë¦¬ëŠ” ê²ƒì„ ìœ„í•´ ì‚¬ìš©ëœë‹¤. ì´ 4ê°œì˜ gateê°€ ì‚¬ìš©ë˜ê³  ì¼ë°˜ì ìœ¼ë¡œ $i,o,f,\tilde{c}$ë¡œ ì‘ì„±í•˜ê³  ê°ê° input, output, forget, candidate/new memory gateë¼ ë¶€ë¥¸ë‹¤.

$$
\begin{align*}
&i^{(t)}=\sigma(W^{(i)}x^{(t)}+U^{(i)}h^{(t-1)})\\
&f^{(t)}=\sigma(W^{(f)}x^{(t)}+U^{(f)}h^{(t-1)})\\
&o^{(t)}=\sigma(W^{(o)}x^{(t)}+U^{(o)}h^{(t-1)})\\
&\tilde{c}^{(t)}=\tanh(W^{(c)}x^{(t)}+U^{(c)}h^{(t-1)})\\
&c^{(t)}=f^{(t)}\circ\tilde(c)^{(t-1)}+i^{(t)}\circ\tilde{c}^{(t)}\\
&h^{(t)}=o^{(o)}\circ\tanh(c^{(t)})
\end{algin*}
$$

ì§ê´€ì ì¸ ê° gateì— ëŒ€í•œ ì´í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

* input gate: í˜„ì¬ inputì´ ì–¼ë§ˆë‚˜ ì‚¬ìš©í• ì§€ ê²°ì •í•œë‹¤.
* forget gate: ì´ì „ stateì˜ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì‚¬ìš©í• ì§€ ê²°ì •í•œë‹¤.
* output gate: hidden state ê°’ì´ ë‹¤ìŒ timestepì— ì–¼ë§ˆë‚˜ ì „ë‹¬í• ì§€ ê²°ì •í•œë‹¤.
* candidate gate: ì¼ë°˜ì ì¸ RNNì™€ ìœ ì‚¬í•œ ë¶€ë¶„ì´ë‹¤. ì´ì „ hidden state ê°’ê³¼ í˜„ì¬ inputê°’ì„ ê¸°ë°˜ìœ¼ë¡œ candidateë¥¼ ê³„ì‚°í•œë‹¤.
* final memory cell: candidate hidden stateë“¤ì„ í•©ì³ì„œ ë‚´ë¶€ì˜ memory ê°’ì„ ë§Œë“ ë‹¤.

Long Termì— ëŒ€í•œ ì •ë³´ë¥¼ ì¡ì•„ë‚´ê¸° ìœ„í•œ ëª¨ë¸ì— LSTM ë¿ë§Œ ì•„ë‹ˆë¼ GRUë„ ë§ì´ ì‚¬ìš©ëœë‹¤. ì¡°ê¸ˆ ë‹¤ë¥¸ êµ¬ì¡°ì´ì§€ë§Œ ê±°ì˜ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•œë‹¤.

![119](https://i.imgur.com/YpHfETv.jpg)

#### Application

RNNëª¨ë¸ì„ í™œìš©í•œ applicationì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

* Language modeling
* Machine Translation
* Text Summarization
* Image Captioning

#### RNN in TensorFlow

RNNì€ ê¸°ë³¸ì ìœ¼ë¡œ í•˜ë‚˜í•˜ë‚˜ì˜ cellë“¤ì´ ê²°í•©ëœ êµ¬ì¡°ì´ë‹¤. TensorFlowì—ì„œëŠ” ì—¬ëŸ¬ê°€ì§€ RNN ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ cellë“¤ì„ ì§€ì›í•œë‹¤.

* BasicRNNCell: ê°€ì¥ ê¸°ë³¸ì ì¸ RNN cell
* RNNCell: RNN Cellì„ ìœ„í•œ Abstract Object
* BasicLSTMCell: ê¸°ë³¸ì ì¸ LSTM recurrent network cell
* LSTMCell: LSTM recurrent network cell
* GRUCell: GRU cell

ìœ„ì˜ cellë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„í•œë‹¤.

```python
cell = tf.nn.rnn_cell.GRUCell(hidden_size)
```

ê·¸ë¦¬ê³  RNNì˜ ëª¨ë¸ì„ ìƒê°í•´ë³´ì. cellë“¤ì´ stackedëœ êµ¬ì¡°ì´ë‹¤. ë”°ë¼ì„œ ì—¬ëŸ¬ê°œì˜ cellë“¤ì„ ìŒ“ì•„ì•¼í•˜ëŠ”ë° ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„í•˜ë©´ ëœë‹¤.

```python
layers = [tf.nn.rnn_cell.GRUCell(size) for size in hidden_sizes]
cells = tf.nn.rnn_cell.MultiRNNCell(layers)
```

ê·¸ë¦¬ê³  ë™ì ìœ¼ë¡œ graphë¥¼ ë§Œë“¤ê¸° ìœ„í•´ `tf.nn.dynamic_rnn`, `tf.nn.bidirectional_dynamic_rnn`ë¥¼ ì‚¬ìš©í•œë‹¤. ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

* `tf.nn.dynamic_rnn`: tf.While loopì„ ì‚¬ìš©í•´ì„œ ë™ì ìœ¼ë¡œ graphë¥¼ ë§Œë“ ë‹¤. graphì˜ ìƒì„±ì´ ë¹ ë¥´ê³  batchë¥¼ ê°€ë³€ í¬ê¸°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.(batchì˜ ê°€ë³€ê¸¸ì´ê°€ sequenceì˜ ê°€ë³€ê¸¸ì´ë¥¼ ëœ»í•˜ì§„ ì•ŠìŒ)
* `tf.nn.bidirectional_dynamic_rnn`: ìœ„ì™€ ê°™ì€ ë°©ì‹ì´ì§€ë§Œ ì–‘ë°©í–¥ì˜ RNNì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

`dynamic_rnn`ì„ ì‚¬ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ RNNë“¤ì„ stack í•  ìˆ˜ ìˆë‹¤.

```python
layers = [tf.nn.rnn_cell.GRUCell(size) for size in hidden_sizes]
cells = tf.nn.rnn_cell.MultiRNNCell(layers)
output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)
```

í•˜ì§€ë§Œ ì•ì„œ ì†Œê°œí•œ RNNì˜ ì œì•½ì„ ìƒê°í•´ë³´ì. sequenceëŠ” ì–´ëŠì •ë„ ë¹„ìŠ·í•œ í¬ê¸°ë¥¼ ê°€ì ¸ì•¼ í–ˆì—ˆë‹¤. ë”°ë¼ì„œ ì¼ì • í¬ê¸°(max_length)ë¥¼ ì •í•˜ê³  ê·¸ í¬ê¸°ë³´ë‹¤ í° ê²½ìš° ìë¥´ê³  í¬ê¸°ë³´ë‹¤ ì‘ì€ ê²½ìš°ì—ëŠ” zero-paddingì„ ì‚¬ìš©í•œë‹¤.

í•˜ì§€ë§Œ paddingì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì—ë„ ìƒˆë¡œìš´ ë¬¸ì œê°€ ìƒê¸´ë‹¤. input ë¿ë§Œ ì•„ë‹ˆë¼ labelì—ë„ paddingì„ í•´ì•¼í•˜ëŠ”ë° ì´ë ‡ê²Œ labelì— paddingì„ í•˜ê²Œë˜ë©´ lossì— ì˜í–¥ì„ ì¤˜ì„œ í•™ìŠµì— ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ ì ‘ê·¼ë²•ì´ ìˆë‹¤.

* Approach 1
  * maskë¥¼ ì‚¬ìš©í•œë‹¤. ì‹¤ì œ labelì—ëŠ” Trueê°’ì„ ì£¼ê³  paddingëœ labelì—ëŠ” Falseë¥¼ ì¤€ë‹¤.
  * modelì„ real/padded token ëª¨ë‘ë¥¼ ê°€ì§€ê³  ëŒë¦°ë‹¤.
  * real ê°’ë“¤ë§Œì„ ê°€ì§€ê³  lossë¥¼ ê³„ì‚°í•œë‹¤.

ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ êµ¬í˜„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels)
loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))
```

* Approach 2
  * modelì—ê²Œ ì‹¤ì œ sequence ê¸¸ì´ë¥¼ ì•Œë ¤ì¤˜ì„œ ì˜ˆì¸¡ë„ ì‹¤ì œ ê¸¸ì´ë§Œí¼ë§Œ í•˜ë„ë¡ í•´ì„œ labelê³¼ ë¹„êµí•œë‹¤.

ì´ ë°©ë²•ì„ ì‚¬ìš©í•œ êµ¬í˜„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. (line 3)

```python
cell = tf.nn.rnn_cell.GRUCell(hidden_size)
rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)
tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)
output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)
```

#### Tips and Tricks for implementation

**Vanishing Gradient**

RNNì˜ ì¤‘ìš”í•œ ë¬¸ì œì  ì¤‘ í•˜ë‚˜ì¸ Vanishing gradientë¥¼ ë§‰ê¸°ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

* ë‹¤ë¥¸ ì¢…ë¥˜ì˜ Activation í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°(ReLUê³„ì—´)
  * tf.nn.relu
  * tf.nn.relu6
  * tf.nn.crelu
  * tf.nn.elu

* ë‹¤ë¥¸ ì¢…ë¥˜ì˜ Activation í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°(ê¸°íƒ€)
  * tf.nn.softplus
  * tf.nn.softsign
  * tf.nn.bias_add
  * tf.sigmoid
  * tf.tanh

**Exploding Gradient**

ê·¸ë¦¬ê³  ë˜ í•˜ë‚˜ì˜ ë¬¸ì œì ì¸ Exploding gradientë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ gradient ê°’ì„ ì¼ì • í¬ê¸° ì´ìƒ ëª»ì˜¬ë¼ê°€ë„ë¡ ì œí•œì‹œí‚¨ë‹¤.

```python
# ëª¨ë“  í•™ìŠµê°€ëŠ¥í•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ costì˜ gradientë¥¼ êµ¬í•œë‹¤.
gradients = tf.gradients(cost, tf.trainable_variables())

# gradientë¥¼ ì¼ì • í¬ê¸° ì´ìƒ ëª»ì˜¬ë¼ê°€ë„ë¡ í•  clipì„ ì •ì˜í•œë‹¤.
clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)


optimizer = tf.train.AdamOptimizer(learning_rate)
train_op = optimizer.apply_gradients(zip(gradients, trainables))
```

**Anneal learning rate**

í•™ìŠµë¥ (learning rate)ë¥¼ í•™ìŠµ ê³¼ì •ì—ì„œ ì ì°¨ ê°ì†Œí‚¤ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
learning_rate = tf.train.exponential_decay(init_lr,
										   global_step,
										   decay_steps,
										   decay_rate,
										   staircase=True)
optimizer = tf.train.AdamOptimizer(learning_rate)
```

**Overfitting**

Dropoutì„ ì‚¬ìš©í•´ì„œ overfittingì„ ë°©ì§€í•˜ëŠ”ë° dropoutì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ `tf.nn.dropout`ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ê³¼, `DropoutWrapper`ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.

* `tf.nn.dropout`

```python
hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)
```

* DropoutWrapper

```python
cell = tf.nn.rnn_cell.GRUCell(hidden_size)
cell = tf.nn.rnn_cell.DropoutWrapper(cell,     
                                    output_keep_prob=keep_prob)
```

#### Language Modeling in TensorFLow

ì´ë²ˆì—ëŠ” TensorFlowë¥¼ í†µí•´ Language modelingì„ êµ¬í˜„í•´ë³´ë„ë¡ í•œë‹¤. ìš°ì„  ì–´ë–¤ language modelingì„ í• ì§€ ë¶€í„° ì •í•´ì•¼ í•˜ëŠ”ë°, ë³´í†µ í”íˆ ì‚¬ìš©ë˜ëŠ” Neural Language Modelingì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

* Word-level: n-gram
  * ë§¤ìš° ì „í†µì ì¸ ëª¨ë¸ì´ë‹¤.
  * íŠ¹ì • ë‹¨ì–´ ì´ì „ì˜ nê°œì˜ ë‹¨ì–´ë¥¼ í†µí•´ íŠ¹ì • ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸.
  * ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ì €ì¥í•˜ëŠ” vocabularyê°€ í•„ìš”í•œë° ì´ í¬ê¸°ê°€ ë§¤ìš° í¬ë‹¤.
  * Out-of-vocabularyì— ëŒ€í•œ ëŒ€ì²˜ê°€ í•„ìš”í•˜ë‹¤.
  * ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ìš”êµ¬í•œë‹¤.
* Character-level
  * inputê³¼ outputëª¨ë‘ ë¬¸ì í•˜ë‚˜í•˜ë‚˜ë¡œ êµ¬ì„±ëœë‹¤.
  * vocabulary í¬ê¸°ê°€ ë§¤ìš° ì‘ë‹¤(ì˜ì–´ì˜ ê²½ìš° ì†Œë¬¸ì 26ê°œ)
  * ë‹¨ì–´ embedding ê³¼ì •ì´ í•„ìš”ì—†ë‹¤.
  * í•™ìŠµì´ ë¹ ë¥´ë‹¤.
  * ìœ ì—°í•˜ì§€ ì•Šì€ ë‹¨ì ì´ ìˆë‹¤.
* Subword-level:
  * inputê³¼ outputì´ subwordì´ë‹¤.
  * Wê°œì˜ ê°€ì¥ ìì£¼ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì™€, Sê°œì˜ ìì£¼ë‚˜ì˜¤ëŠ” ìŒì ˆì„ ì •í•œ í›„ ê¸°ì¡´ textë¥¼ ë³€í˜• ì‹œí‚¨ë‹¤. (e.g new company dreamworks interactive -> new company dre+ am+ wo+ rks: in+ te+ ra+ cti+ ve:)
  * word-levelê³¼ char-levelë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

ì´ë²ˆ êµ¬í˜„ì—ì„œëŠ” Character-levelì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ë„ë¡ í•œë‹¤. ë°ì´í„°ëŠ” 'Donald Trump's tweets'ë°ì´í„°ë¡œ 2018ë…„ 2ì›” 15ì¼ê¹Œì§€ì˜ donald trumpì˜ íŠ¸ìœ—ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì´ 19,469ê°œì˜ íŠ¸ìœ—ì´ ìˆìœ¼ë©° ê°ê° ìµœëŒ€ 140ìì´ë‹¤. ê·¸ë¦¬ê³  íŠ¸ìœ—ì˜ ëª¨ë“  ë§í¬ë“¤ ì¦‰, URLì€ \__HTTP__ë¡œ ì‘ì„±ë˜ì–´ ìˆë‹¤. ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¨ í›„ ë‚˜ì˜¨ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![wwwwwww](https://i.imgur.com/M8tr031.jpg)

ì´ì œ êµ¬í˜„ì„ í•´ë³´ì. ë¨¼ì € ìš°ë¦¬ê°€ ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ importë¨¼ì € í•œë‹¤.

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import random
import sys
sys.path.append('..')
import time

import tensorflow as tf

import utils
```

ìš°ì„ ì€ charë¥¼ ê°ê°ì˜ inputìœ¼ë¡œ ë„£ëŠ”ë‹¤ê³  í–ˆì—ˆë‹¤. ê·¸ë ‡ë‹¤ê³  inputìœ¼ë¡œ ë°”ë¡œ 'c'ë¥¼ ë„£ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì „ì²´ characterë“¤ì„ vocabularyì— ë„£ê³  ê° characterë¥¼ indexí™” ì‹œì¼œì„œ inputìœ¼ë¡œ ë„£ëŠ”ë‹¤. ì¦‰ ê° inputì˜ characterì— í•´ë‹¹í•˜ëŠ”ê²ƒì„ vocabularyì—ì„œ ì°¾ê³  indexë¥¼ ë°˜í™˜í•˜ëŠ” encodeí•¨ìˆ˜ì™€ ì¶œë ¥ì‹œ ë‹¤ì‹œ ìˆ«ìë¥¼ characterë¡œ ë°”ê¿”ì£¼ëŠ” decodeí•¨ìˆ˜ë¶€í„° êµ¬í˜„í•œë‹¤.

```python
# encode í›„ indexë¥¼ 0ì´ ì•„ë‹Œ 1ë¶€í„° ê°–ë„ë¡ ë§Œë“ ë‹¤.
def vocab_encode(text, vocab):
    return [vocab.index(x) + 1 for x in text if x in vocab]

def vocab_decode(array, vocab):
    return ''.join([vocab[x - 1] for x in array])
```

ì´ì œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ë°ì´í„°ëŠ” txt íŒŒì¼ë¡œ ë˜ì–´ìˆë‹¤. ê° ë°ì´í„°ë¥¼ í•œ ì¤„ì”© ì½ì–´ì™€ì„œ ìœ„ì— ì •ì˜í•œ encodeí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ vectorí™” ì‹œì¼œì¤€ë‹¤.

```python
def read_data(filename, vocab, window, overlap):
    lines = [line.strip() for line in open(filename, 'r').readlines()]
    while True:
        random.shuffle(lines)

        for text in lines:
            text = vocab_encode(text, vocab)
            for start in range(0, len(text) - window, overlap):
                chunk = text[start: start + window]
                chunk += [0] * (window - len(chunk))
                yield chunk

```

ê·¸ë¦¬ê³  ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ë°°ì¹˜í™” ì‹œì¼œì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“ ë‹¤.

```python
def read_batch(stream, batch_size):
    batch = []
    for element in stream:
        batch.append(element)
        if len(batch) == batch_size:
            yield batch
            batch = []
    yield batch
```

ì´ì œ ì „ì²´ ëª¨ë¸ì„ Class í˜•íƒœë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤.

```python
class CharRNN(object):
    def __init__(self, model):
        self.model = model
        self.path = 'data/' + model + '.txt'
        self.vocab = ("$%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                    " '\"_abcdefghijklmnopqrstuvwxyz{|}@#â¡ğŸ“ˆ")
        self.seq = tf.placeholder(tf.int32, [None, None])
        self.temp = tf.constant(1.5)
        self.hidden_sizes = [128, 256]
        self.batch_size = 64
        self.lr = 0.0003
        self.skip_step = 1
        self.num_steps = 50 # for RNN unrolled
        self.len_generated = 200
        self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')

    def create_rnn(self, seq):
        layers = [tf.nn.rnn_cell.GRUCell(size) for size in self.hidden_sizes]
        cells = tf.nn.rnn_cell.MultiRNNCell(layers)
        batch = tf.shape(seq)[0]
        zero_states = cells.zero_state(batch, dtype=tf.float32)
        self.in_state = tuple([tf.placeholder_with_default(state, [None, state.shape[1]])
                                for state in zero_states])
        # this line to calculate the real length of seq
        # all seq are padded to be of the same length, which is num_steps
        length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)
        self.output, self.out_state = tf.nn.dynamic_rnn(cells, seq, length, self.in_state)

    def create_model(self):
        seq = tf.one_hot(self.seq, len(self.vocab))
        self.create_rnn(seq)
        self.logits = tf.layers.dense(self.output, len(self.vocab), None)
        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits[:, :-1],
                                                        labels=seq[:, 1:])
        self.loss = tf.reduce_sum(loss)
        # sample the next character from Maxwell-Boltzmann Distribution
        # with temperature temp. It works equally well without tf.exp
        self.sample = tf.multinomial(tf.exp(self.logits[:, -1] / self.temp), 1)[:, 0]
        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)

    def train(self):
        saver = tf.train.Saver()
        start = time.time()
        min_loss = None
        with tf.Session() as sess:
            writer = tf.summary.FileWriter('graphs/gist', sess.graph)
            sess.run(tf.global_variables_initializer())

            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/' + self.model + '/checkpoint'))
            if ckpt and ckpt.model_checkpoint_path:
                saver.restore(sess, ckpt.model_checkpoint_path)

            iteration = self.gstep.eval()
            stream = read_data(self.path, self.vocab, self.num_steps, overlap=self.num_steps//2)
            data = read_batch(stream, self.batch_size)
            while True:
                batch = next(data)

            # for batch in read_batch(read_data(DATA_PATH, vocab)):
                batch_loss, _ = sess.run([self.loss, self.opt], {self.seq: batch})
                if (iteration + 1) % self.skip_step == 0:
                    print('Iter {}. \n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))
                    self.online_infer(sess)
                    start = time.time()
                    checkpoint_name = 'checkpoints/' + self.model + '/char-rnn'
                    if min_loss is None:
                        saver.save(sess, checkpoint_name, iteration)
                    elif batch_loss < min_loss:
                        saver.save(sess, checkpoint_name, iteration)
                        min_loss = batch_loss
                iteration += 1

    def online_infer(self, sess):
        """ Generate sequence one character at a time, based on the previous character
        """
        for seed in ['Hillary', 'I', 'R', 'T', '@', 'N', 'M', '.', 'G', 'A', 'W']:
            sentence = seed
            state = None
            for _ in range(self.len_generated):
                batch = [vocab_encode(sentence[-1], self.vocab)]
                feed = {self.seq: batch}
                if state is not None: # for the first decoder step, the state is None
                    for i in range(len(state)):
                        feed.update({self.in_state[i]: state[i]})
                index, state = sess.run([self.sample, self.out_state], feed)
                sentence += vocab_decode(index, self.vocab)
            print('\t' + sentence)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ êµ¬í˜„í•œ ëª¨ë¸ë“¤ì„ ì‹¤í–‰ì‹œí‚¤ëŠ” mainí•¨ìˆ˜ë¥¼ ë„£ìœ¼ë©´ ëë‚œë‹¤.

```python
def main():
    model = 'trump_tweets'
    utils.safe_mkdir('checkpoints')
    utils.safe_mkdir('checkpoints/' + model)

    lm = CharRNN(model)
    lm.create_model()
    lm.train()

if __name__ == '__main__':
    main()
```


RNNì„ í†µí•´ characterë‹¨ìœ„ ë¿ë§Œ ì•„ë‹ˆë¼ word levelë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë²ˆ ê¸°íšŒì— ìì„¸íˆ ì•Œì•„ë³´ë„ë¡ í•˜ì.
