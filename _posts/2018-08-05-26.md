---
layout: post
title:  "NLP를 위한 CNN (2): Convolutional Neural Network for Sentence Classification"
date:   2018-08-05 13:47:35 +0900
categories: NLP
tag: NLP
---

NLP에서 활용되는 Convolutional Network에 대해서 논문 하나씩 알아보도록 한다. 전체 List는 다음과 같다.


1. [Understanding CNN for NLP](https://reniew.github.io/25/)
2. [Convolutional Neural Network for Sentence Classification, 2014](https://reniew.github.io/26/) [현재글]
3. [A Convolutional Neural Network for Modelling Sentences, 2014](https://reniew.github.io/27/)
4. [A Sensitivity Analysis of Convolutional Neural Networks for Sentence Classification](https://reniew.github.io/28/)
5. [Character-level Convolutional Networks for Text Classification](https://reniew.github.io/29/)

---

### Convolutional Neural Network for Sentence Classification


가장 먼저 소개할 논문은 Newyork 대학의 Yoon kim님의 논문인 [Convolutional Neural Network for Sentence Classification](http://www.aclweb.org/anthology/D14-1181)입니다. 매우 간단한 구조의 CNN을 활용해서 문장 분류에서 상당한 효율을 보이며 많은 주목을 받았던 논문입니다.

자연어 처리 분야에서 딥러닝 기법을 활용한 중요한 기술은 단어 표현을 학습하는 것이다. 즉 one-hot 벡터로만 사용하던 단어표현이 [word2vec](https://reniew.github.io/21/), [Glove](https://reniew.github.io/23/)등 많은 기술들을 통해 dense한 low-dimension vector로 표현함으로써 비슷한 의미의 단어는 가까운 거리(유클리디언 거리 혹은 코사인 유사도)에 있도록 만들었다.

그리고 CNN은 원래는 Computer vision분야의 문제를 해결하기 위해 나왔지만 많은 NLP 문제에서도 효과적이라고 알려졌다. 이 논문에서는 한개의 layer를 사용하는 CNN에 대해서 소개한다. 그리고 CNN에 사용할 단어 벡터는 1,000억개의 단어를 사전학습한 벡터를 사용한다. 여기서는 이 벡터를 수정하거나 하지 않고 Network의 parameter만 학습한다. 우선 사용할 architecture에 대해서 보자.

#### Model

![yoon.k](https://i.imgur.com/TNjCKHf.jpg)

위의 architecture는 Collobert(2011)의 architecture를 약간 수정한 것이다. 세부적으로 하나씩 알아보자.

우선 input은 $k$-dimension의 단어 벡터이다. 문장의 $i$번 째 단어는 다음과 같다.

$$
x_i\in\mathbb{R}^k
$$

그리고 문장은 $n$개의 단어를 concat하여 사용하는데, 필요하다면 padding을 추가한다. 따라서 다음과 같이 표현된다.

$$
\mathbf{x}_{i:i+j}=\mathbf{x}_i\oplus \mathbf{x}_{i+1}\oplus ... \oplus \mathbf{x}_{i+j}.
$$

위의 $\oplus$는 concatenation 연산자이다. 위의 sentence에 convolution을 한다. 필터는 $hk$크기의 벡터의 모양이다. $h$개의 단어에 적용되서 새로운 feature를 뽑아낸다.

$$
\mathbf{w}\in\mathbb{R}^{h\times k}
$$

convolution 연산을 하면 feature $c_i$가 만들어진다.

$$
c_i=f(\mathbf{w}\cdot \mathbf{x}_{i:i+h-1}+b)
$$

여기서 $b\in \mathbb{R}$는 bias이고 $f$는 tanh와 같은 non-linear 함수이다. 이 필터는 sliding하면서 sentence $$[\mathbf{x}_{1:h},\mathbf{x}_{2:h+1},...,\mathbf{x}_{n-h+1:n}]$$에 대해서 각각 적용되서 feature map $$\mathbf{c}$$를 만든다.

$$
\mathbf{c}=[c_1,c_2,...,c_{n-h+1}] \in \mathbb{R}^{n-h+1}
$$

만들어진 feature map에 max-over-time pooling(Collobert et al., 2011)을 하고 최대 값을 뽑아낸다.

$$
\hat{c}=\max\{\mathbf{c}\}
$$

위 값은 특정 필터에 상응하는 feature가 된다. feature map에서 가장 중요한 값(높은 값)을 뽑아내는 방법이다. 이러한 방법을 사용하면 하나의 필터에 대해서 하나의 feature를 뽑아 낸다. 이러한 피쳐들은 penultimate layer를 구성하고 fully connected sofmax layer로 통과시켜서 라벨에 대한 확률 분포를 만들어 낸다.

그리고 마지막으로 그림을 보면 input에서 2개의 channel이 존재한다. 하나는 위에서 설명한 static한 word vector들을 모아둔 것이고, 나머지는 backpropagation을 통해 fine tuning 한 것이다. 즉 두 개의 channel에 대해서 filter를 적용시킨 후 더해서 사용한다. 1개의 channel만 사용한다면 더하는 과정 없이 바로 사용하면 된다.

#### Regularization

여기서는 정규화를 위해 dropout과 l2-norm 정규화를 사용했다. dropout의 경우에는 penultimate layer에서 마지막 layer로 가는 파라미터에서 사용했는데 이를 사용함으로써 hidden unit들이 같이 더해지는 것을 방지한다. forward와 back propagation에서 모두 사용했다. 즉 일반화해서 나타내면, penultimate layer를 $\mathbf{z}=[\hat{c}_1,...,\hat{c}_m]$이라 하자. 그러면,

$$
y=\mathbf{w}\cdot\mathbf{z}+b
$$

를 사용하는 대신에, 아래의 식을 사용했다.(in forward)


$$
y=\mathbf{w}\cdot(\mathbf{z}\odot\mathbf{r})+b
$$

여기서 $\odot$은 원소별 곱샘을 뜻하고 $\mathbf{r}$은 'masking' 벡터로 각 값이 확률 $p$의 Bernoulli분포를 따르기 때문에 0 혹은 1의 값을 가진다. test과정에서는 dropout을 하지 않고 $\mathbf{w}$ 대신에 dropout 확률 $p$를 곱해서 사용한다. $\hat{\mathbf{w}}=p\mathbf{w}$

그리고 $l_2$ 정규화의 경우 가중치 벡터의 $l_2$ norm이 $s$보다 클 경우 $s$값을 가지도록 조정한다.

$$
\mathbf{w}=
\begin{cases}
w, ~\text{if }{\|\mathbf{w}||}_2<s\\
s, ~\text{other wise}
\end{cases}
$$

#### DataSet

이 논문에서 사용 된 데이터셋은 다음과 같다.

* MR : 리뷰당 하나의 문장으로 된 영화 리뷰
* SST-1 : Stanford Sentiment Treebank (label이 세분화)
* SST-2 : Stanford Sentiment Treebank (label이 binary)
* Subj : 주관성 데이터셋
* TREC : TREC 질문 데이터셋
* CR : 소비자 리뷰 데이터셋
* MPQA

#### 학습과 그 외 기타 사항들

이 모델에서 사용한 hyperparameter 및 함수는 다음과 같다.

* ReLU 함수
* 100 feature map으로 filter의 크기는 3,4,5로 지정
* Dropout 확률 : 0.5
* $l_2$ 값 : 3
* 미니 배치 크기 : 50

그리고 early stopping이 추가적으로 사용된 것 외에는 다른 특별한 사항은 없다. 그리고 학습 과정에서는 무작위의 mini batch에 대해서 SGD를 사용했고(엄밀히 말하면 MGD를 사용한 것이다), 파라미터 업데이트는 Adadelta를 사용했다.

단어 벡터의 경우는 앞에서 나왔듯이 google news를 활용해 word2vec으로 사전 학습된 임베딩 벡터를 사용했으며 만약 vocabulary에 들어가 있지 않은 단어의 경우에는 임의의 값으로 초기화해서 사용했다. 그리고 실험단계에서 여러가지 상황을 바꿔가며 성능을 비교했는데 아래의 종류로 구분지어서 실험했다.

* **CNN-rand** : baseline값으로 사용하기 위해 사용. 모든 단어 벡터를 임의의 값으로 초기화해서 사용했다.
* **CNN-static** : 앞서 말한 사전 학습된 word2vec 단어 벡터를 사용한 모델이다.
* **CNN-non-static** : 위의 모델과 같이 학습된 벡터를 사용했지만 각 task에서 벡터값은 update된다.
* **CNN-multichannel** : architecture 소개 부분에서 나왔듯이 input값을 1-channel로 한 것이 아니라, 2-channel인 모델. 둘 다 word2vec으로 학습한 단어 벡터인데 하나는 static하게 값이 그대로이고 나머지 하나는 학습 중간 계속 update된다. 즉 위의 static과 non-static을 섞어서 사용한 것과 같다.

이렇게 4개의 경우로 나눠서 실험 했으며 결과는 다음과 같다.

![yoon.k2](https://i.imgur.com/46YUAdQ.jpg)

표의 상위 4개가 비교하는 모델이고 아래의 값들은 다른 모델의 값을 비교한 것이다. 결과를 보면 절대적으로 좋은 모델은 없고 데이터셋에 따라 좋은 결과를 보이는 모델이 다르다. 하지만 명확한 것은 CNN을 활용해 만든 매우 간단한 모델임에도 불구하고 다른 모델들과 비교해서도 결코 적지 않은 성능을 보이거나 아니면 오히려 뛰어난 성능을 보여준다.

이번 논문의 내용만 보더라도 CNN은 Image에서만이 아니라 자연어 처리 분야에서도 쉽고 간단하게 높은 성능을 기대할 수 있다. 그리고 이 논문을 통해 알게 된 또 하나의 사실은 word2vec등 단어를 임베딩하는 것이 NLP분야에서는 필수적이고 그 자체만으로도 성능이 엄청 오른다는 것을 알 수 있다.


---

긴글 읽어주셔서 감사합니다. 오역 및 잘못된 내용이 있을 수 있습니다. 잘못된 부분 혹은 이해가 잘 안되는 부분은 댓글 혹은 메일로 말씀해주시면 감사하겠습니다!
