---
layout: post
title:  "PCA"
date:   2019-07-24 10:00:00 +0700
categories: [others]
---

### PCA
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
PCA(Principal Component Analysis)은 **차원축소(dimensionality reduction)**와 **변수추출(feature extraction)**기법으로 널리쓰이고 있다.  

아래 **그림1**은 Linear Regression **그림2**은 PCA를 나타낸다.  
Cost Function을 통하여 Model을 만든다고 가정하면  
**그림1**은 **예측값과 실제값의 차이**를 통하여 Cost가 최소가 되는 Model을 만들게 된다.  
<a href="https://wjddyd66.github.io/ai/2019/07/08/A.I-Linear-Regression.html">Linear Regression 자세한 내용</a>  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/26.PNG" height="250" width="600" /></div>  
**그림1**
<br><br>

**그림2**은 **예측값과 실제값의 거리**를 통하여 Cost가 최소가 되는 Model을 만들게 된다.  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/27.PNG" height="250" width="600" /></div>  
**그림2**
<br><br>
<span style ="color: red">**PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법이다.**</span>  
아래 그림은 이러한 PCA를 잘 나타내는 그림이다.  

<div><img src="http://i.imgur.com/Uv2dlsH.gif" height="100%" width="100%" /></div>  
그림 출처:<a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">stats.stackexchange.com</a>  

PCA를 잘 알기위한 알아야 하는 **고유값 분해, SVD**에 대해 먼저 알아보자  

### 고유값 분해
**고유값, 고유벡터**  
고유값 분해(Eigen Decomposition)를 알기 위해서 먼저 고유값(Eigenvalue), 고유백터(Eigenvector)가 무엇인지 알아야 한다.  
<p>$$Av = \lambda v$$</p>
위의 식에서 <span>$$\lambda$$ </span>는 행렬 A의 고유값, v는 행렬 A의 고유벡터라고 불리게 된다.  
A의 고유벡터는 A에 의해 **방향**은 보존되고 **크기**만 커진다고 말할 수 있다.  
<span style ="color: red">**선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터(eigenvector)라 하고 이 상수배 값을 고유값(eigenvalue)**</span>이다.  

<br>
**고유값분해를 이용한 대각화**  
아래와 같은 행렬이 존재한다고 가정하자.  
<p>$$\begin{bmatrix} V_11 & ... & V_1n \\  ... & ... & ... \\ V_n1 & ... & V_nn  \end{bmatrix} \begin{bmatrix} \lambda_1 &  ... & 0 \\ 0 &  \lambda_2 & ... \\ 0 &  ... & \lambda_n \end{bmatrix} = \begin{bmatrix} \lambda_1v_11 & ... & \lambda_nv_1n \\ \lambda_1v_21 & ... & \lambda_nv_2n \\ \lambda_1v_n1 & ... & \lambda_nv_nn  \end{bmatrix}$$</p>
행렬 A의 고유값, 고유벡터들을 <span>$$\lambda_i, v_i  (i=1,2, ... , n)$$ </span>이라 가정하면 아래와 같이 나타낼 수 있다.  
<p>$$Av_1 = \lambda_1v_1$$ </p>
<p>$$Av_2 = \lambda_2v_2$$ </p>
<p>$$ ... $$ </p>
<p>$$Av_n = \lambda_1v_n$$ </p>
위의 식을 한꺼번에 정리하게 되면 아래와 같은 식으로 나타낼 수 있다.  
<p>$$A\begin{bmatrix} v_1 & v_2 & ... & v_n  \end{bmatrix} = \begin{bmatrix} \lambda_1v_1 & \lambda_2v_2 & ... & \lambda_nv_n  \end{bmatrix} = \begin{bmatrix} v_1 & v_2 & ... & v_n  \end{bmatrix}\begin{bmatrix} \lambda_1 & 0 & ... & 0 \\ 0 & \lambda_2 & ... & 0 \\ ... \\ 0 & 0 & ... & \lambda_n   \end{bmatrix}$$ </p>
위에서 고유값들을 대각원소로하는 행렬을 대각행렬 <span>$$ \land $$ </span>로 정의하게 되면 아래와 같은 식으로 간단하게 표현 될 수 있다.  
<p>$$ AP = P \land $$</p>
<p>$$ A = P \land P^{-1}$$</p>
이와 같이 **행렬A**는 **자신의 고유벡터들을 열벡터로 하는 행렬**과 **고유값을 대각원소로하는 행렬의 곱**으로 분해가 가능한데 이러한 것을 **대각화 분해(eigendecomposition)**라고 한다.  

이러한 대각화 분해로 인한 det(A), A의 거듭제곱, 역행렬, 대각합(trace), 다항식 등 매우 손쉽게 계산할 수 있다.  
아래의 예시는 A의 거듭제곱을 나타낸 것 이다.  
<p>$$ A^{k} = (P \land P^{-1})^{k}$$</p>
<p>$$ = (P \land P^{-1})(P \land P^{-1}) ... (P \land P^{-1})$$</p>
<p>$$ P  \land^{k} P^{-1}$$</p>
<p>$$ P diag(\lambda_1^{k},....,\lambda_n^{k}) P^{-1}$$</p>
<br>
**고유값, 고유벡터 계산**  
고유값과 고유벡터를 구하는 방법에 대해 알아보자.  
<p>$$Av = \lambda v$$</p>
<p>$$Av - \lambda v = 0 (0: 0행렬)$$</p>
<p>$$(A - \lambda E) v = 0 (E: 단위행렬)$$</p>
위의 식에서 <span>$$ v \neq 0 $$</span>이므로 <span>$$ A - \lambda E $$</span>의 역행렬이 존재하면 안된다.  
위의 조건을 만족하기 위한 식은 아래와 같다.  
<p>$$det(A - \lambda E) v = 0$$</p>
**대칭행렬과 고유값 분해**  
SVD를 위해 사용하기 위한 가장 중요한 부분이다.  
<span style ="color: red"> **만약 A가 대칭행렬이라고 가정하면** </span>  
<p>$$Av_1 = \lambda_1 v_1, Av_2 = \lambda_2 v_2$$</p>라고 가정  
<p>$$\lambda_1 u_2^{T}u_1 = u_2^{T}(Au_1) = (u_2^{T}A)u_1$$</p>
<p>$$ = (A^{T}u_2)^{T}u_1 = (Au_2)^{T}u_1 = \lambda_2u_2^{T}u_1$$</p>
<p>$$ (\lambda_1 - \lambda_2)u_2^{T}u_1 = 0  $$</p>
<p>$$ \lambda_1 , \lambda_2 \neq 0 이므로 u_2^{T}u_1 = 0  $$</p>
<p>$$ u_2^{T}u_1 = 0  이므로 서로 직교(orthogonal)한다. $$</p>
위의 증명을 아래의 식에 대입하게 되면  
<p>$$ A = P \land P^{-1}  $$</p>
<p>$$ A = P \land P^{T}  $$</p>
<p>$$ PP^{T} = E (P^{-1} = P^{T})  $$</p>
위의 식으로 인하여 우리는 2가지 사실을 알 수 있다.  
A가 만약 대칭행렬이면  
1)<span style ="color: red"> **실원소 대칭행렬은 항상 고유값 대각화가 가능하다.** </span>  
2)<span style ="color: red"> **직교행렬(orthogonal matrix)로 대각화가 가능하다.** </span>



<span style ="color: red"> **자세한 고유값 분해에 대한 예시와 자세한 내용은 링크 참조:** </span> <a href="https://darkpgmr.tistory.com/105">다크프로그래머 blog</a>  

### SVD 
SVD(Singular Value Decomposition) 특이값 분해는 고유값 분해 처럼 행렬을 대각화하는 한 방법이다.  
SVD의 중요한 점은 **정방행렬이든 아니든 관계없이 모든 m x n 행렬에 대해 적용이 가능**하기 때문(고유값 분해는 정방행렬일때만 가능)이다.   
실수공간에서의 임의의 m x n 행렬에 대한 특이값 분해는 다음과 같이 정의된다.  
<p>$$A = U \sum V^{T}$$</p>
<p>$$A: \text{m x n 행렬}$$</p>
<p>$$U: \text{m x m 직교행렬}(AA^{T} = U(\sum \sum^{T})U^{T})$$</p>
<p>$$V: \text{n x n 직교행렬}(A^{T}A = V(\sum^{T} \sum)V^{T})$$</p>
<p>$$\sum: \text{m x n 직사각 대각행렬}$$</p>
위의 조건을 하나씩 살펴보자  
<span>$$U: \text{m x m 직교행렬}(AA^{T} = U(\sum \sum^{T})U^{T})$$ </span>  
<span>$$AA{T}$$</span>는 대칭행렬이므로 **대칭행렬과 고유값 분해** 로 인하여 orthogoanl백터로서 나타낼 수 있다.  
<p>$$A^{T} = V \sum^{T} U $$</p>
<p>$$AA^{T} = U \sum (V^{T} V) \sum^{T} U $$</p>
<p>$$AA^{T} = U \sum E \sum^{T} U $$</p>
<p>$$AA^{T} = U (\sum  \sum^{T}) U $$</p>
V도 위과 같이 증명하면 똑같이 나오게 된다.  
<span>$$\sum: \text{m x n 직사각 대각행렬}$$</span>  
<p>$$\sum = \begin{bmatrix} \sigma_1 &  & \\ ... \\ \ &  & sigma_s \\ & & 0 \end{bmatrix} (m>n)$$</p>
<p>$$\sum = \begin{bmatrix} \sigma_1 &  & \\ ... \\ \ &  sigma_s & 0 \end{bmatrix} (m 
< 
n 
)$$</p>

위와 같은 행렬은 아래와 같은 그림으로 간단하게 나타낼 수 있다.<br>
아래와 같은 행렬 m x n 행렬 A를 SVD로 분해하는 것을 full SVD라 부른다.(단 m > n )<br>
<div><img src="https://t1.daumcdn.net/cfile/tistory/2435CA425266197F02" height="200" width="500" /></div>  
<p>full SVD</p>  
위와 같은 그림들을 아래 그림들과 같이 Reduced SVD를 하는 것이 일반적이다.  
<div><img src="https://t1.daumcdn.net/cfile/tistory/2536894052661ADF36" height="200" width="500" /></div>  
<p>thin SVD</p>  
<div><img src="https://t1.daumcdn.net/cfile/tistory/23378F3352661E3336" height="200" width="500" /></div>  
<p>Compact SVD</p>  
<div><img src="https://t1.daumcdn.net/cfile/tistory/2746F23952661E501D" height="200" width="500" /></div>  
<p>Truncated SVD</p>  
<br><br>

**SVD 기하학에서의 의미**  

<p>$$A = U \sum V^{T}$$</p>
에서 A가 3차원 U가 2차원 이라고 생각하면 아래와 같은 그림으로 나타낼 수 있다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/28.PNG" height="250" width="600" /></div>  
2차원으로 줄이게 된다면 3차원의 점들은 2차원(u1, u2)를 기준으로 값을 수직으로 내려서 표현하게 된다.  
3차원인 A행렬을 U(u1, u2)의 2차원 행렬과 <span>$$\sum$$ </span>분산으로서 표현할 수 있게 되는 것이다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/29.PNG" height="250" width="600" /></div>  
<span style ="color: red"> **자세한 특이값 분해에 대한 예시와 자세한 내용은 링크 참조:** </span> <a href="https://darkpgmr.tistory.com/106?category=460967">다크프로그래머 blog</a>  

<hr>
참조: <a href="https://www.youtube.com/watch?v=85SdLVOu3GQ&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=9">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="https://ratsgo.github.io/machine%20learning/2017/04/24/PCA">ratsgo's blog</a> <br>
참조: <a href="https://darkpgmr.tistory.com/105">다크프로그래머 blog</a> <br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.